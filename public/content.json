{"meta":{"title":"coderzc的博客","subtitle":"","description":"","author":"coderzc","url":"https://coderzc.github.io","root":"/"},"pages":[{"title":"tags","date":"2021-08-17T20:33:25.000Z","updated":"2021-08-18T02:58:32.973Z","comments":true,"path":"tags/index.html","permalink":"https://coderzc.github.io/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2021-08-17T20:35:51.000Z","updated":"2021-08-18T02:58:32.973Z","comments":true,"path":"categories/index.html","permalink":"https://coderzc.github.io/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2021-08-17T20:36:02.000Z","updated":"2021-08-18T02:58:32.973Z","comments":true,"path":"about/index.html","permalink":"https://coderzc.github.io/about/index.html","excerpt":"自我介绍","text":"自我介绍"}],"posts":[{"title":"Google论文、开源与云计算","slug":"分布式/20210817-Google论文、开源与云计算","date":"2021-08-16T16:00:00.000Z","updated":"2021-08-18T02:58:32.967Z","comments":true,"path":"2021/08/17/分布式/20210817-Google论文、开源与云计算/","link":"","permalink":"https://coderzc.github.io/2021/08/17/%E5%88%86%E5%B8%83%E5%BC%8F/20210817-Google%E8%AE%BA%E6%96%87%E3%80%81%E5%BC%80%E6%BA%90%E4%B8%8E%E4%BA%91%E8%AE%A1%E7%AE%97/","excerpt":"Google论文、开源与云计算 本文搬运自：https://zhuanlan.zhihu.com/p/85808754 1.Google论文与开源 自1998年成立，至今Google已走过20个年头。在这20年里，Google不断地发表一些对于自己来说已经过时甚至不再使用的技术的论文，但是发表之后总会有类似系统被业界实现出来，也足以说明google的技术至少领先业界数年。在Amazon不断引领全球云计算浪潮开发出一系列面向普罗大众的云产品的同时；Google也在不断引领构建着满足互联网时代海量数据的存储计算和查询分析需求的软硬件基础设施。 本文对Google在这20年中发表的论文进行了一个","text":"Google论文、开源与云计算 本文搬运自：https://zhuanlan.zhihu.com/p/85808754 1.Google论文与开源自1998年成立，至今Google已走过20个年头。在这20年里，Google不断地发表一些对于自己来说已经过时甚至不再使用的技术的论文，但是发表之后总会有类似系统被业界实现出来，也足以说明google的技术至少领先业界数年。在Amazon不断引领全球云计算浪潮开发出一系列面向普罗大众的云产品的同时；Google也在不断引领构建着满足互联网时代海量数据的存储计算和查询分析需求的软硬件基础设施。 本文对Google在这20年中发表的论文进行了一个简单的总结和整理，主要选择了分布式系统和并行计算领域相关的论文，其中内容涉及数据中心/计算/存储/网络/数据库/调度/大数据处理等多个方向。通过这样的一个总结，一方面可以一窥Google强大的软硬件基础设施，另一方面也可以为不同领域的开发人员提供一个学习的参考。可以通过这些文章去了解上层应用的架构设计和实现，进而可以更好的理解和服务于上层应用。同时这些系统中所采用的架构/算法/设计/权衡，本身也可以为我们的系统设计和实现提供重要的参考。 通过Google论文可以了解到系统整体的架构，通过对应开源系统可以在代码层面进行学习。具体如下图(浅蓝色部分为Google论文/黄色为开源系统)： 2.Google论文简介下面来简要介绍下”那些年我们追过的Google论文”，由于篇幅有限主要讲下每篇论文的主要思路，另外可能还会介绍下论文作者及论文本身的一些八卦。深入阅读的话，可以直接根据下面的链接查看原文，另外很多文章网上已经有中文译文，也可以作为阅读参考。 2.1 起源 The anatomy of a large-scale hypertextual Web search engine(1998).Google创始人Sergey Brin和Larry Page于1998年发表的奠定Google搜索引擎理论基础的原始论文。在上图中我们把它放到了最底层，在这篇论文里他们描述了最初构建的Google搜索引擎基础架构，可以说所有其他文章都是以此文为起点。此文对于搜索引擎的基本架构，尤其是Google使用的PageRank算法进行了描述，可以作为了解搜索引擎的入门文章。 WEB SEARCH FOR A PLANET: THE GOOGLE CLUSTER ARCHITECTURE(IEEE Micro03).描述Google集群架构最早的一篇文章，同时也应该是最被忽略的一篇文章，此文不像GFS MapReduce Bigtable那几篇文章为人所熟知，但是其重要性丝毫不亚于那几篇。这篇文章体现了Google在硬件方面的一个革命性的选择：在数据中心中使用廉价的PC硬件取代高端服务器。这一选择的出发点主要基于性价比，实际的需求是源于互联网数据规模之大已经不能用传统方法解决，但是这个选择导致了上层的软件也要针对性地进行重新的设计和调整。由于硬件可靠性的降低及数量的上升，意味着要在软件层面实现可靠性，需要采用多个副本，需要更加自动化的集群管理和监控。也是从这个时候开始，Google开始着眼于自己设计服务器以及数据中心相关的其他硬件，逐步从托管数据中心向自建数据中心转变。而Google之后实现的各种分布式系统，都可以看做是基于这一硬件选择做出的软件层面的设计权衡。 再看下本文的作者：Luiz André Barroso/Jeff Dean/Urs Hölzle，除了Jeff Dean，其他两位也都是Google基础设施领域非常重要的人物。Urs Holzle是Google的第8号员工，最早的技术副总裁，一直在Google负责基础设施部门，Jeff Dean和Luiz Barroso等很多人都是他招进Google的，包括当前Google云平台的掌门人Diane Greene(VMWare联合创始人)据说也是在他的游说下才最终决定掌管GCP。Luiz Barroso跟Jeff Dean在加入Google以前都是在DEC工作，在DEC的时候他参与了多核处理器方面的工作，是Google最早的硬件工程师，在构建Google面向互联网时代的数据中心硬件基础设施中做了很多工作。到了2009年，Luiz André Barroso和Urs Hölzle写了一本书，书名就叫&lt;&lt;The Datacenter as a Computer&gt;&gt;，对这些工作(数据中心里的服务器/网络/供电/制冷/能效/成本/故障处理和修复等)做了更详细的介绍。 The Google File System(SOSP03).Google在分布式系统领域发表的最早的一篇论文。关于GFS相信很多人都有所了解，此处不再赘言。今天Google内部已经进化到第二代GFS：Colossus，而关于Colossus目前为止还没有相关的论文，网上只有一些零散介绍：Colossus。简要介绍下本文第一作者Sanjay Ghemawat，在加入Google之前他也是在DEC工作，主要从事Java编译器和Profiling相关工作。同时在DEC时代他与Jeff Dean就有很多合作，而他加入Google也是Jeff Dean先加入后推荐他加入的，此后的很多工作都是他和Jeff Dean一块完成的，像后来的MapReduce/BigTable/Spanner/TensorFlow，在做完Spanner之后，Jeff Dean和Sanjay开始转向构建AI领域的大规模分布式系统。2012年，Jeff Dean和Sanjay共同获得了ACM-Infosys Foundation Award。此外Google的一些开源项目像LevelDB/GPerftools/TCMalloc等，都可以看到Sanjay的身影。 MapReduce: Simplified Data Processing on Large Clusters(OSDI04).该文作者是Jeff Dean和Sanjay Ghemawat，受Lisp语言中的Map Reduce原语启发，在大规模分布式系统中提供类似的操作原语。在框架层面屏蔽底层分布式系统实现，让用户只需要关注如何编写自己的Mapper和Reducer实现，从而大大简化分布式编程。时至今日MapReduce已经成为大规模数据处理中广泛应用的一种编程模型，虽然之后有很多新的编程模型不断被实现出来，但是在很多场景MapReduce依然发挥着不可替代的作用。 而自2004年提出之后，中间也出现过很多关于MapReduce的争论，最著名的应该是2008年1月8号David J. DeWitt和Michael Stonebraker发表的一篇文章&lt;&lt; MapReduce: A major step backwards&gt;&gt;，该文发表后引起了广泛的争论。首先介绍下这两位都是数据库领域的著名科学家，David J. DeWitt，ACM Fellow，2008年以前一直在大学里搞研究，在并行数据库领域建树颇多，之后去了微软在威斯康辛的Jim Gray系统实验室。Michael Stonebraker(2014图灵奖得主)，名头要更大一些，在1992 年提出对象关系数据库。在加州伯克利分校计算机教授达25年，在此期间他创作了Ingres, Illustra, Cohera, StreamBase Systems和Vertica等系统。其中Ingres是很多现代RDBMS的基础，比如Sybase、Microsoft SQL Server、NonStop SQL、Informix 和许多其他的系统。Stonebraker曾担任过Informix的CEO，自己还经常出来创个业，每次还都成功了。关于这个争论，Jeff Dean和Sanjay Ghemawat在2010年1月份的&lt;&gt;上发表了这篇&lt;&lt;MapReduce-A Flexible Data Processing Tool&gt;&gt;进行回应，同一期上还刊了Michael Stonebraker等人的&lt;&lt;MapReduce and Parallel DBMSs-Friends or Foes &gt;&gt;。 Bigtable: A Distributed Storage System for Structured Data(OSDI06).Bigtable基于GFS构建，提供了结构化数据的可扩展分布式存储。自Bigtable论文发表之后，很快开源的HBase被实现出来，此后更是与Amazon的Dynamo一块引领了NoSQL系统的潮流，之后各种NoSQL系统如雨后春笋般出现在各大互联网公司及开源领域。此外在tablet-server中采用的LSM-Tree存储结构，使得这种在1996年就被提出的模型被重新认识，并广泛应用于各种新的存储系统实现中，成为与传统关系数据库中的B树并驾齐驱的两大模型。 如果说MapReduce代表着新的分布式计算模型的开端的话，Bigtable则代表着新的分布式存储系统的开端。自此之后在分布式计算存储领域，Google不断地推陈出新，发表了很多新的计算和存储系统，如上图中所示。在继续介绍这些新的计算存储系统之前，我们回到图的底层，关注下基础设施方面的一些系统。 2.2 基础设施 The Chubby lock service for loosely-coupled distributed systems(OSDI06).以文件系统接口形式提供的分布式锁服务，帮助开发者简化分布式系统中的同步和协调工作，比如进行Leader选举。除此之外，这篇文章一个很大的贡献应该是将Paxos应用于工业实践，并极大地促进了Paxos的流行，从这个时候开始Paxos逐渐被更多地工业界人士所熟知并应用在自己的分布式系统中。此后Google发表的其他论文中也不止一次地提到Paxos，像MegaStore/Spanner/Mesa都有提及。此文作者Mike Burrows加入Google之前也是在DEC工作，在DEC的时候他还是AltaVista搜索引擎的主要设计者。 Borg(Eurosys15) Omega(Eurosys13) Kubernetes.Borg是Google内部的集群资源管理系统，大概诞生在2003-2004年，在Borg之前Google通过两个系统Babysitter和Global Work Queue来分别管理它的在线服务和离线作业，而Borg实现了两者的统一管理。直到15年Google才公布了Borg论文，在此之前对外界来说Borg一直都是很神秘的存在。而Omega主要是几个博士生在Google做的研究型项目，最终并没有实际大规模上线，其中的一些理念被应用到Borg系统中。注：Borg这个名字源自于&lt;&lt;星际迷航&gt;&gt;里的博格人，博格人生活在银河系的德尔塔象限，是半有机物半机械的生化人。博格个体的身体上装配有大量人造器官及机械，大脑为人造的处理器。博格人是严格奉行集体意识的种族，从生理上完全剥夺了个体的自由意识。博格人的社会系统由“博格集合体”组成，每个集合体中的个体成员被称为“Drone”。集合体内的博格个体通过某种复杂的子空间通信网络相互连接。在博格集合体中，博格个体没有自我意识，而是通过一个被称为博格女皇（Borg Queen）的程序对整个集合体进行控制。 在2014年中的时候，Google启动了Kubernetes(Borg的开源版本)。2015年，Kubernetes 1.0 release，同时Google与Linux基金会共同发起了CNCF。2016年，Kubernates逐渐成为容器编排管理领域的主流。提到Kubernates，需要介绍下著名的分布式系统专家Eric Brewer，伯克利教授&amp;Google infrastructure VP，互联网服务系统早期研究者。早在1995年他就和Paul Gauthier创立了Inktomi搜索引擎(2003年被Yahoo!收购，李彦宏曾在这家公司工作)，此时距离Google创立还有3年。之后在2000年的PODC上他首次提出了CAP理论，2012年又对CAP进行了回顾。2011年他加入了Google，目前在负责推动Kubernetes的发展。 CPI2: CPU performance isolation for shared compute clusters(Eurosys13).通过监控CPI(Cycles-Per-Instruction)指标，结合历史运行数据进行分析预测找到影响系统性能的可疑程序，限制其CPU使用或进行隔离/下线，避免影响其他关键应用。本文也从一个侧面反映出，为了实现离线在线混布Google在多方面所做的努力和探索，尤其是在资源隔离方面。具体实现中，每台机器上有一个守护进程负责采集本机上运行的各个Job的CPI数据(通过采用计数模式/采样等方法降低开销，实际CPU开销小于0.1%)，然后发送到一个中央的服务器进行聚合，由于集群可能是异构的，每个Job还会根据不同的CPU类型进行单独聚合，最后把计算出来的CPI数据的平均值和标准差作为CPI spec。结合CPI历史记录建立CPI预测模型，一旦出现采样值偏离预测值的异常情况，就会记录下来，如果异常次数超过一定阈值就启动相关性分析寻找干扰源，找到之后进行相应地处理(限制批处理作业的CPU使用/调度到单独机器上等)。讲到这里，不仅让我们联想到今天大火的AIOPS概念，而很久之前Google已经在生产系统上使用类似技术。不过在论文发表时，Google只是打开了CPI2的监控功能，实际的自动化处理还未在生产系统中打开。 GOOGLE-WIDE PROFILING:A CONTINUOUS PROFILING INFRASTRUCTURE FOR DATA CENTERS(IEEE Micro10).Google的分布式Profiling基础设施，通过收集数据中心的机器上的各种硬件事件/内核事件/调用栈/锁竞争/堆内存分配/应用性能指标等信息，通过这些信息可以为程序性能优化/Job调度提供参考。为了降低开销，采样是在两个维度上进行，首先是在整个集群的机器集合上采样同一时刻只对很少一部分机器进行profiling，然后在每台机器上再进行基于事件的采样。底层通过OProfile采集系统硬件监控指标(比如CPU周期/L1 L2 Cache Miss/分支预测失败情况等)，通过GPerfTools采集应用程序进程级的运行指标(比如堆内存分配/锁竞争/CPU开销等)。收集后的原始采样信息会保存在GFS上，但是这些信息还未与源代码关联上，而部署的binary通常都是去掉了debug和符号表信息，采用的解决方法是为每个binary还会保存一个包含debug信息的未被strip的原始binary，然后通过运行MapReduce Job完成原始采样信息与源代码的关联。为了方便用户查询，历史Profiling数据还会被加载到一个分布式数据库中。通过这些Profiling数据，除了可以帮助应用理解程序的资源消耗和性能演化历史，还可以实现数据驱动的数据中心设计/构建/运维。 Dapper, a Large-Scale Distributed Systems Tracing Infrastructure(Google TR10).Google的分布式Tracing基础设施。Dapper最初是为了追踪在线服务系统的请求处理过程。比如在搜索系统中，用户的一个请求在系统中会经过多个子系统的处理，而且这些处理是发生在不同机器甚至是不同集群上的，当请求处理发生异常时，需要快速发现问题，并准确定位到是哪个环节出了问题，这是非常重要的，Dapper就是为了解决这样的问题。对系统行为进行跟踪必须是持续进行的，因为异常的发生是无法预料的，而且可能是难以重现的。同时跟踪需要是无所不在，遍布各处的，否则可能会遗漏某些重要的点。基于此Dapper有如下三个最重要的设计目标：低的额外开销，对应用的透明性，可扩展。同时产生的跟踪数据需要可以被快速分析，这样可以帮助用户实时获取在线服务状态。 B4: Experience with a Globally-Deployed Software Defined WAN(Sigcomm13).Google在全球有几十个数据中心，这些数据中心之间通常通过2-3条专线与其他数据中心进行连接。本文描述了Google如何通过SDN/OpenFlow对数据中心间的网络进行改造，通过对跨数据中心的流量进行智能调度，最大化数据中心网络链路的利用率。Google通过强大的网络基础设施，使得它的跨越全球的数据中心就像一个局域网，从而为后续很多系统实现跨数据中心的同步复制提供了网络层面的保障。 2.3 计算分析系统自MapReduce之后，Google又不断地开发出新的分布式计算系统，一方面是为了提供更易用的编程接口(比如新的DSL/SQL语言支持)，另一方面是为了适应不同场景(图计算/流计算/即席查询/内存计算/交互式报表等)的需求。 Interpreting the Data: Parallel Analysis with Sawzall(Scientific Programming05).Google为了简化MapReduce程序的编写，而提出的一种新的DSL。后来Google又推出了Tenzing/Dremel等数据分析系统，到了2010年就把Sawzall给开源了，项目主页：http://code.google.com/p/szl/。虽然与Tenzing/Dremel相比， Sawzall所能做的事情还是比较有限，但是它是最早的，同时作为一种DSL毕竟还是要比直接写MapReduce job要更易用些。 本文第一作者Rob Pike，当今世界上最著名的程序员之一，&lt;&lt;Unix编程环境&gt;&gt; &lt;&lt;程序设计实践&gt;&gt;作者。70年代就加入贝尔实验室，跟随Ken Thompson&amp;DMR(二人因为发明Unix和C语言共同获得1983年图灵奖)参与开发了Unix，后来又跟Ken一块设计了UTF-8。2002年起加入Google，之后搞了Sawzall，目前跟Ken Thompson一块在Google设计开发Go语言。 FlumeJava: Easy, Efficient Data-Parallel Pipelines(PLDI10).由于实际的数据处理中，通常都不是单个的MapReduce Job，而是多个MapReduce Job组成的Pipeline。为了简化Pipleline的管理和编程，提出了FlumeJava框架。由框架负责MapReduce Job的提交/中间数据管理，同时还会对执行过程进行优化，用户可以方便地对Pipeline进行开发/测试/运行。另外FlumeJava没有采用新的DSL，而是以Java类库的方式提供给用户，用户只需要使用Java语言编写即可。 Pregel: A System for Large-Scale Graph Processing(SIGMOD10).Google的图处理框架。Pregel这个名称是为了纪念欧拉，在他提出的格尼斯堡七桥问题中，那些桥所在的河就叫Pregel，而正是格尼斯堡七桥问题导致了图论的诞生。最初是为了解决PageRank计算问题，由于MapReduce并不适于这种场景，所以需要发展新的计算模型去完成这项计算任务，在这个过程中逐步提炼出一个通用的图计算框架，并用来解决更多的问题。核心思想源自BSP模型，这个就更早了，是在上世纪80年代由Leslie Valiant(2010年图灵奖得主)提出，之后在1990的Communications of the ACM 上，正式发表了题为A bridging model for parallel computation的文章。 Dremel: Interactive Analysis of Web-Scale Datasets(VLDB10).由于MapReduce的延迟太大，无法满足交互式查询的需求，Google开发了Dremel系统。Dremel主要做了三件事： 将嵌套记录转换为列式存储，并提供快速的反向组装 类sql的查询语言 类搜索系统的查询执行树 通过列式存储降低io，将速度提高一个数量级，这类似于诸如Vertica这样的列存式数据库，与传统行式存储不同，它们只需要读取查询语句中真正必需的那些字段数据；通过类搜索系统的查询执行系统取代mr(MapReduce)，再提高一个数量级。它类似于Hive，应该说查询层像Hive，都具有类似于SQL的查询语言，都可以用来做数据挖掘和分析；但hive是基于mr，所以实时性要差，Dremel则由于它的查询执行引擎类似于搜索服务系统，因此非常适合于交互式的数据分析方式，具有较低的延迟，但是通常数据规模要小于mr；而与传统数据库的区别是，它具有更高的可扩展性和容错性，结构相对简单，可以支持更多的底层存储方式。其中的数据转化与存储方式，巧妙地将Protobuf格式的嵌套记录转换成了列式存储，同时还能够快速的进行重组，是其比较独特的一点。 Tenzing A SQL Implementation On The MapReduce Framework(VLDB11).Tenzing是一个建立在MapReduce之上的用于Google数据的ad hoc分析的SQL查询引擎。Tenzing提供了一个具有如下关键特征的完整SQL实现(还具有几个扩展)：异构性，高性能，可扩展性，可靠性，元数据感知，低延时，支持列式存储和结构化数据，容易扩展。Tenzing的发表算是很晚的了，与之相比Facebook在VLDB09上就发表了Hive的论文。与开源系统Hive的优势在于它跟底层所依赖的MapReduce系统都是一个公司内的产品，因此它可以对MapReduce做很多改动，以满足Tenzing某些特殊性的需求，最大化Tenzing的性能。 PowerDrill：Processing a Trillion Cells per Mouse Click(VLDB12).Google推出的基于内存的列存数据库，该系统在2008年就已经在Google内部上线。与Dremel相比虽然都是面向分析场景，但是PowerDrill主要面向的是少量核心数据集上的多维分析，由于数据集相对少同时分析需求多所以可以放到内存，在把数据加载到内存分析之前会进行复杂的预处理以尽量减少内存占用。而Dremel则更加适合面向大量数据集的分析，不需要把数据加载到内存。主要采用了如下技术进行加速和内存优化： 导入时对数据进行分区，然后查询时根据分区进行过滤尽量避免进行全量扫描 底层数据采用列式存储，可以跳过不需要的列 采用全局/chunk两级字典对列值进行编码，一方面可以加速计算(chunk级的字典可以用来进行针对用户查询的chunk过滤，编码后的value变成了更短的int类型与原始值相比可以更快速的进行相关运算)，另一方面还可以达到数据压缩的目的，与通用压缩算法相比采用这种编码方式的优点是：读取时不需要进行解压这样的预处理，同时支持随机读取 编码后的数据进行压缩还可以达到1.4-2倍的压缩比，为了避免压缩带来的性能降低，采用了压缩与编码的混合策略，对数据进行分层，最热的数据是解压后的编码数据，然后稍冷的数据也还会进行压缩 对数据行根据partition key进行重排序，提高压缩比 查询分布式执行，对于同一个查询会分成多个子查询并发给多个机器执行，同时同一个子查询会发给两台机器同时执行，只要有一个返回即可，但是另一个最终也要执行完以进行数据预热 MillWheel: Fault-Tolerant Stream Processing at Internet Scale(VLDB13).Google的流计算系统，被广泛应用于构建低延迟数据处理应用的框架。用户只需要描述好关于计算的有向图，编写每个节点的应用程序代码。系统负责管理持久化状态和连续的记录流，同时将一切置于框架提供的容错性保证之下。虽然发布的比较晚，但是其中的一些机制(比如Low Watermark)被借鉴到开源的 Flink 系统中。 Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing(VLDB14).Google的跨数据中心数据仓库系统，主要是为了满足广告业务的场景需求，随着广告平台的不断发展，客户对各自的广告活动的可视化提出了更高的要求。对于更具体和更细粒度的信息需求，直接导致了数据规模的急速增长。虽然Google已经把核心广告数据迁移到了Spanner+F1上，但是对于这种广告效果实时统计需求来言，由于涉及非常多的指标这些指标可能是保存在成百上千张表中，同时这些指标与用户点击日志相关通常对应着非常大的峰值访问量，超过了Spanner+F1这样的OLTP系统的处理能力。为此Google构建了Mesa从而能处理持续增长的数据量，同时它还提供了一致性和近实时查询数据的能力。具体实现方法是：将增量更新进行batch，提交者负责为增量数据分配版本号，利用Paxos对跨数据中心的版本数据库进行更新，基于MVCC机制提供一致性访问。底层通过Bigtable存储元数据，通过Colossus来存储数据文件，此外还利用MapReduce来对连续增量数据进行合并，而为Mesa提供增量更新的上游应用通常是一个流计算系统。可以看到Mesa系统本身结合了批量处理与实时计算，还要满足OLTP+OLAP的场景需求，同时采用了分层架构实现存储计算的分离。既像一个分布式数据库，又像一个大数据准实时处理系统。 Shasta: Interactive Reporting At Scale(SIGMOD16).Google的交互式报表系统，也主要是为了满足广告业务的场景需求，与Mesa的区别在于Shasta是构建于Mesa之上的更上层封装。主要为了解决如下挑战：1)用户查询请求的低延迟要求 2)底层事务型数据库的schema与实际展现给用户的视图不友好，报表系统的开发人员需要进行复杂的转换，一个查询视图底层可能涉及多种数据源(比如F1/Mesa/Bigtable等) 3)数据实时性需求，用户修改了广告预算后希望可以在新的报表结果中可以马上体现出来。为了解决这些问题，在F1和Mesa系统之上构建了Shasta。主要从两个层面进行解决：语言层面，在SQL之上设计了一种新的语言RVL(Relational View Language)，通过该语言提供的机制(自动聚合/子句引用/视图模板/文本替换等)可以比SQL更加方便地描述用户的查询视图，RVL编译器会把RVL语句翻译成SQL，在这个过程中还会进行查询优化；系统层面，直接利用了F1的分布式查询引擎，但是进行了一些扩展比如增加单独的UDF server让UDF的执行更加安全，为了确保实时性需要直接访问F1，但是为了降低延迟在F1之上增加了一个只读的分布式Cache层。 Goods: Organizing Google’s Datasets(SIGMOD16).Google的元数据仓库Goods(Google DataSet Search)。Google内部积累了大量的数据集，而这些数据散落在各种不同的存储系统中(GFS/Bigtable/Spanner等)。面临的问题就是如何组织管理这些数据，使得公司内部工程师可以方便地找到他们需要的数据，实现数据价值的最大化。Google的做法很多方面都更像一个小型的搜索引擎，不过在这个系统里被索引的数据由网页变成了Google内部生产系统产生的各种数据，用户变成了内部的数据开发人员。整个做法看起来要费劲很多，很大程度上是因为内部系统众多但是没有一个统一的入口平台，只能采用更加自动化(不依赖人和其他系统)的做法：要爬取各个系统的日志，通过日志解析数据的元信息(这个过程中还是比较费劲的，比如为了确定数据的Schema，要把Google中央代码库里的所有protobuf定义拿过来试看哪个能匹配上)，然后把这些信息(大小/owner/访问权限/时间戳/文件格式/上下游/依赖关系/Schema/内容摘要等)保存一个中央的数据字典中(存储在Bigtable中目前已经索引了260亿条数据集信息)，提供给内部用户查询。这中间解决了如下一些问题和挑战：Schema探测/数据自动摘要/血缘分析/聚类/搜索结果ranking/过期数据管理/数据备份等。本文可以让我们一窥Google是如何管理内部数据资产的，有哪些地方可以借鉴。 2.4 存储&amp;数据库 Percalator:Large-scale Incremental Processing Using Distributed Transactions and Notifications(OSDI10).基于Bigtable的增量索引更新系统，Google新一代索引系统”咖啡因“实时性提升的关键。此前Google的索引构建是基于MapReduce，全量索引更新一次可能需要几天才能完成，为了提高索引更新的实时性Google构建了增量更新系统。Bigtable只支持单行的原子更新，但是一个网页的更新通常涉及到其他多个网页(网页间存在链接关系比如更新的这个网页上就有其他网页的锚文本)的更新。为了解决这个问题，Percolator在Bigtable之上通过两阶段提交实现了跨行事务。同时网页更新后还要触发一系列的处理流程，Percolator又实现了类似于数据库里面的触发器机制，当Percolator中的某个cell数据发生变化，就触发应用开发者指定的Observer程序。此外开源分布式数据库TiDB就参考了Percalator的事务模型。 Megastore: Providing Scalable, Highly Available Storage for Interactive Services(CIDR11).Google在2008年的SIGMOD上就介绍了Megastore，但是直到2011年才发表完整论文。Megastore本身基于Bigtable，在保留可扩展/高性能/低延迟/高可用等优点的前提下，引入了传统关系数据库中的很多概念比如关系数据模型/事务/索引，同时基于Paxos实现了全球化同步复制，可以说是最早的分布式数据库实现了。它本身也提供了分布式事务支持，但是论文中并没有描述相关实现细节，猜测应该跟Percalator类似。虽然此后被Spanner所替代，但是它的继任者Spanner很多特性都是受它影响。 Spanner: Google’s Globally-Distributed Database(OSDI12).2009年Jeff Dean的一次分享(Designs, Lessons and Advice from Building Large Distributed)中首次提到Spanner，也是过了3年到了2012年才发表完整论文。做为Megastore的继任者，它主要解决了Megastore存在的几个问题：性能、查询语言支持弱、分区不灵活。另外一个重要的创新是基于原子钟和GPS硬件实现了TrueTime API，并基于这个API实现了更强的一致性保证。除此之外其他部分则与Megastore非常类似，但是在文中对其分布式事务的实现细节进行了描述。 F1: A Distributed SQL Database That Scales(VLDB13).基于Spanner实现的分布式SQL数据库，主要实现了一个分布式并行查询引擎，支持一致性索引和非阻塞的在线Schema变更。与Spanner配合替换掉了Google核心广告系统中的MySQL数据库。F1这个名字来自生物遗传学，代指杂交一代，表示它结合了传统关系数据库和NoSQL系统两者的特性。 2.5 AI TensorFlow: A System for Large-Scale Machine Learning(OSDI16). In-Datacenter Performance Analysis of a Tensor Processing Unit(SIGARCH17).Google TPU。与往常一样，在Google公布此文的时候，新一代更强大的TPU已经开发完成。由于本文更偏重硬件，具体内容没有看。但是其中的第四作者David Patterson还是值得特别来介绍一下，因为在体系结构领域的贡献(RISC、RAID、体系结构的量化研究方法)，他和John Hennessy共同获得了2017年的图灵奖：相关新闻。2016年加入Google就是去做TPU的；2018年，与他共同获得图灵奖的John Hennessy(斯坦福第十任校长、MIPS公司创始人)被任命为Google母公司Alphabet的新任主席。 3.总结在前面两节我们对过去20年Google在分布式系统领域的经典论文进行了系统地梳理和介绍，通过这个过程我们可以看到： 每当Google发表一篇相关论文，通常都会产生一个与之对应的开源系统。比如GFS/HDFS，MapReduce/Hadoop MapReduce，BigTable/HBase，Chubby/ZooKeeper，FlumeJava/Plume，Dapper/Zipkin等等。如果把数据中心看做一台计算机的话，在数据中心之上的各种分布式系统就像当年的Unix和C语言，Hadoop及各种开源系统就像当年的Linux，而开启这个时代的人们尤其是Jeff Dean/Sanjay Ghemawat就像当年的Ken Thompson/Dennis M. Ritche，Hadoop创始人Doug Cutting就像当年的Linus Torvalds。Ken Thompson/Dennis M. Ritche因为Unix和C方面的贡献获得1983年图灵奖，或许在将来的某一天Jeff Dean/Sanjay Ghemawat也能摘得桂冠。 观察上图，我们还可以看到随着时间的推进，Google自底向上地逐步构建出一个庞大的软硬件基础设施Stack，同时每个系统内部也在不断地自我进化。而不同的系统之间，可能是互补关系，可能是继承关系，可能是替换关系。通过对这个演化过程的观察，我们也总结出一些内在的趋势和规律。论文本身固然重要，但是这些趋势和规律也很有意义。 3.1 两个维度，三个层次如题”他山之石”，人们常说不能总是低头拉车，还要注意抬头看路。那么应该如何走出去看看，看什么呢？我们可以将其划分为两个维度(时间和空间)，三个层次(架构、细节和实现)，如下图： 两个维度：时间维度上可以分为过去，现在和未来。Google的那些论文就属于未来，看看它们，那可能是未来要做的，当然慢慢地它们也会成为过去；经典的理论的东西，放到过去这个维度，它们是非常重要的，这决定了对系统理解的深度和高度；现在，就是正在做的或者符合目前实际环境可以直接借鉴的。人们有时候往往喜欢抓着未来，总是忽略了过去和现在，又或者是仅看着当前，忽略了未来和过去。空间维度上可以分为上层和底层，上层是指依赖于我们自己系统的那些应用，底层则指我们的系统本身底层所依赖的那些。有时候为了继续前进，需要跳出当前的框框，从多个维度上去学习，通过不断学习反过来进一步促进当前系统的演化。 三个层次：如果要了解其他系统，可以从三个层次去学习，先大概了解架构，然后深入到一些具体的细节问题，最后如果有时间还可以继续深入到代码级别。结合本文的第一张图来说就是：可以通过Google论文了解整体架构，然后通过开源系统相关wiki或文档可以了解到更细节的一些东西，最后结合开源系统还可以看到实际的代码实现。 3.2 合久必分，分久必合3.2.1 分 实例： 1.越来越多的计算被Offload到非CPU的计算单元：Google TPU 2.“One Size Fits All”: An Idea Whose Time Has Come and Gone：各种新的计算模型如Pregel MillWheel Dremel PowerDrill Mesa 3.计算存储分离：Mesa CFS+Spanner+F1 3.2.1 合 实例： 1.分布式数据库：从MegaStore开始到后来的Spanner F1，不断弥补着NoSQL的不足。同时Spanner自身仍在不断演化，开始具备更加丰富的SQL和OLAP支持。 2.流处理和批处理的统一：Cloud DataFlow完成了编程接口层面的统一，而Mesa则解决了数据层面的结合。 3.在线离线混部：Borg。 4.软硬件结合：整个基础设施，就是在解决一个软件(分布式系统)如何适配新硬件(面向互联网设计的数据中心)的问题。通过上层分布式系统屏蔽底层数据中心细节，实现”Datacenter As a Computer“。 3.3 理论与实践相结合3.3.1 ”新瓶装旧酒“纵观过去的20年，我们可以看到如果单纯从理论上看，Google的这些论文并没有提出新理论。它们所依赖的那些基础理论(主要来自分布式系统和关系数据库领域)，基本上都是上个世纪70/80年代就已经提出的。而Google的系统只是把这些经典理论结合自己的业务场景(互联网搜索和广告)，进行了实践并发扬广大使之成为业界潮流。看起来虽然是”新瓶装旧酒”，但是却不能小觑这一点，因为旧酒在新瓶里可能会产生新的化学反应，进而创造出新的完全不同的“酒”。如果忽略了它，当新”酒“成为新浪潮之时，就再也无法站立在浪潮之巅。 3.3.2 两个阶段如果从理论与实践的这个角度来看，我们可以把过去的20年分成两个阶段：前十年主要解决的是可扩展性问题，理论主要源自分布式系统领域；后十年在解决了可扩展问题后，开始考虑易用性问题，提供更加方便的编程接口和一致性模型，这个阶段更多地是借鉴传统关系数据库领域的一些做法。再回到当下，从AI的再度流行中我们依然可以看到其所依赖的理论基础，依然是在上个世纪就已经提出的，而今天在互联网时代大规模的数据和计算能力这个背景下，重新焕发了生命。在解决完可扩展易用性问题后，使得可以对大规模数据进行方便地存储计算和查询之后，下一个十年人们开始关注如何进一步挖掘数据，如何借助这些数据去完成以前未完成的构想，这个过程中仍在不断学习应用前人的经典理论。 3.3.3 实践联系理论从另一个方面来说，如果要真正理解这些论文，除了论文本身内容之外，也还需要去了解传统的分布式系统和关系数据库理论。比如Spanner那篇论文，如果只看论文本身，没有关系数据库和分布式系统理论基础的话估计很难看懂。有时候可能还需要多看看论文的参考文献，之后再看才会理解一些。很多研究领域的大牛们，经常会调侃做工程的家伙们，他们说”这些家伙看着就像生活在5,60年代的老家伙“，为什么呢，因为这些家伙们总是用一些很丑陋的方法去解决一个科学家们早在几十年前就给出了完美解决方案的问题，但是这些家伙看起来对此一无所知。当然了，做工程的也会挖苦下那些研究家们老是指指点点，从来不肯俯下身子来解决实际问题。但是实际上，如果你是做工程的，那就应该多看看研究家们的成果，其实很多问题的确是人家n多年前就已经提出并很好解决了的。如果是做研究的，那就多接触下工程实践，理解下现实需求，弥补下理论与实践的差距。 3.3.4 分布式理论实践具体到分布式系统领域，我们可以发现正是通过与实践相结合，理论才逐渐赢得科学界和工业界的重视。在此之前，分布式理论研究一直处于非常尴尬的状态，与实践的隔阂尤其严重，很多研究工作局限在研究领域，严重脱离现实世界。关于这一点从图灵奖的颁发上可以看出来，自1966年图灵奖首次颁发以来，直到2013年Lamport获奖之前，可以说还没有一个人因为在分布式系统领域的贡献而获得图灵奖。虽然有些获奖者的研究领域也涉及到分布式系统，但是他们获奖更多是因为在其他领域的贡献。而反观程序设计语言/算法/关系数据库等领域均有多人获奖，同时这些领域的研究成果早已被广泛应用在工业界，通过实践证明了其价值。可以说正是因为互联网的兴起，在Google等公司的分布式系统实践下，分布式理论逐渐被广泛应用到各个实际系统中，这也是 Lamport能够获得图灵奖的重要原因。 4.云计算的起源与发展本节我们将跳出Google论文的范畴，以更广泛的视角看一下今天的云计算。下面更多的是描述一些历史，进行一些”考古”，希望这个过程可以带来更多的启发和思考。 4.1 从Google论文说起4.1.1 “冰山一角”首先还是回到第一张图，我们把图缩小一下，并重点关注图的顶部。 可以看到，在Google强大的软硬件基础设施之上，在其云平台上暴露给外部用户使用的则寥寥无几。这个场景就像我们看到了一座冰山，露在水面上的只有那一角。即便是已经开放给外部用户的Cloud Bigtable是2015年才发布的，此时距离Bigtable论文发表已经过了快10年。Cloud Spanner是2017年，也已经是论文发表5年之后。虽然在2008年就推出了GAE，但是也一直不温不火。 将Google的这些系统与AWS的各种云产品对比一下，可以发现两者的出发点类似都是为了实现”Datacenter As a Computer“，但是目标用户不同。Google这些系统面向的是内部的搜索广告业务，而AWS则致力于让外部客户也能实现”Datacenter As a Computer“。就好比一个是面向大企业客户的国有大银行，一个是面向小微客户的普惠金融。从技术-&gt;产品-&gt;商品-&gt;服务的角度来看，Google在技术上做到了独步天下，但是要提供给外部客户后面的短板仍然需要补足。 早在2011年，Google员工Amazon前员工Steve Yegge在G+上发表了一篇文章对Google和Amazon进行了有趣的对比：Stevey’s Google Platforms Rant ，中文版。其中非常重要的一点就是Amazon对于服务及服务化的重视。 2015年Sundar Pichai成为Google新任CEO。进行了一系列调整，找来了VMware的联合创始人Diane Greene领导谷歌的企业及云业务，相关新闻：谷歌公有云GCP轰隆崛起？，可以看到Google正在做出很多改变，开始将云计算作为公司重要战略。同时开源了很多技术如Kubernetes和TensorFlow，试图通过容器、CloudNative和AI等新兴领域实现弯道超车。 4.1.2 为啥要发论文还有一个有趣的对比，可以看到在过去20年Google发表了非常多的论文来介绍它的内部系统，但是反观Amazon，对于它的云产品内部实现可以说介绍的非常少，相关论文只有寥寥几篇。 对于Google来说，发表论文主要是为了增加个人和公司的业界影响力，便于赢得声誉吸引人才。当然Google内部同样有非常严格的保密机制，禁止员工向外界透露内部系统信息，除非获得了授权。通过前面的一些论文也可以看到，从系统做出来上线算，真正论文发表通常是5年之后的事情了，而发表的时候内部已经有下一代系统了。按照中国古话说”富贵不还乡，如锦衣夜行“，内部再牛逼别人看不到就没有存在感。 反观Amazon，则没有这个苦恼，因为它云平台上的所有系统都是对外开放的，外面的人可以切实地感受到它的存在，大部分情况下都不需要通过论文来提升存在感。 4.2 “5朵云”的起源IBM的CEO Thomas J. Watson在1943年说过这样一段话：”I think there is a world market for maybe five computers,” 后来在Cloud Computing概念提出后，逐步演变成5朵云的说法。 4.3 AWS关于售卖计算能力给外部客户的想法最早源自2003年Benjamin Black和Chris Pinkham写的一篇报告中，这个想法引起了Jeff Bezos的兴趣。之后2004年就开干了，当时大家一致觉得Pinkham最适合去干这件事，但是他那个时候正想着回到他的家乡南非，于是Amazon就让他在南非开了新的办公室，在那里他们创建了EC2团队并开发出了EC2。Benjamin Black 在一篇文章(EC2 Origins)中介绍了这段有趣的历史。 2006年AWS正式上线了EC2和S3，自此拉开了云计算的序幕。其后续整个发展的详细历程可以参考：Timeline of Amazon Web Services。 此外还有一个比较有意思的问题：为什么 AWS 云计算服务是亚马逊先做出来，而不是 Google ？其中有偶然也有必然，简要总结一下就是”天时、地利、人和“。 参考文献https://www.gcppodcast.com/post/episode-46-borg-and-k8s-with-john-wilkes/ https://blog.risingstack.com/the-history-of-kubernetes/ Borg, Omega, and Kubernetes http://www.wired.com/2015/09/google-2-billion-lines-codeand-one-place/ https://en.wikipedia.org/wiki/Eric_Brewer_(scientist) 如何看待谷歌工程师透露谷歌有20亿行代码，相当于写40遍Windows？ Return of the Borg: How Twitter Rebuilt Google’s Secret Weapon http://www.infoq.com/cn/news/2014/08/google-data-warehouse-mesa https://en.wikipedia.org/wiki/Amazon_Web_Services https://en.wikipedia.org/wiki/Thomas_J._Watson","categories":[{"name":"分布式","slug":"分布式","permalink":"https://coderzc.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[]},{"title":"kubernetes","slug":"容器化/20210801-kubernetes","date":"2021-07-31T16:00:00.000Z","updated":"2021-08-18T02:58:32.968Z","comments":true,"path":"2021/08/01/容器化/20210801-kubernetes/","link":"","permalink":"https://coderzc.github.io/2021/08/01/%E5%AE%B9%E5%99%A8%E5%8C%96/20210801-kubernetes/","excerpt":"1. kubernetes 起源与发展 1.1 kubernetes 起源 * Kubernetes最初源于谷歌内部的Borg，Kubernetes 的最初目标是为应用的容器化编排部署提供一个最小化的平台，包含几个基本功能： 1. 将应用水平扩容到多个集群 2. 为扩容的实例提供负载均衡的策略 3. 提供基本的健康检查和自愈能力 4. 实现任务的统一调度 1.2 kubernetes 发展 * 2014年6月 谷歌云计算专家Eric Brewer在旧金山的发布会为这款新的开源工具揭牌。 * 2015年7月22日K8S迭代到 v1.0并在OSC","text":"1. kubernetes 起源与发展 1.1 kubernetes 起源 Kubernetes最初源于谷歌内部的Borg，Kubernetes 的最初目标是为应用的容器化编排部署提供一个最小化的平台，包含几个基本功能： 将应用水平扩容到多个集群 为扩容的实例提供负载均衡的策略 提供基本的健康检查和自愈能力 实现任务的统一调度 1.2 kubernetes 发展 2014年6月 谷歌云计算专家Eric Brewer在旧金山的发布会为这款新的开源工具揭牌。 2015年7月22日K8S迭代到 v1.0并在OSCON大会上正式对外公布。 为了建立容器编排领域的标准和规范，Google、RedHat 等开源基础设施领域玩家们，在 2015 年共同牵头发起了名为 CNCF（Cloud Native Computing Foundation）的基金会。Kubernetes 成为 CNCF 最核心的项目。发起成员：AT&amp;T, Box, Cisco, Cloud Foundry Foundation, CoreOS, Cycle Computing, Docker, eBay, Goldman Sachs, Google, Huawei, IBM, Intel, Joyent, Kismatic, Mesosphere, Red Hat, Switch SUPERNAP, Twitter, Univa, VMware and Weaveworks。 2018年，超过 1700 开发者成为 Kubernetes 项目社区贡献者，全球有 500 多场沙龙。国内出现大量基于 Kubernetes 的创业公司。 2020 年，Kubernetes 项目已经成为贡献者仅次于 Linux 项目的第二大开源项目。成为了业界容器编排的事实标准，各大厂商纷纷宣布支持 Kubernetes 作为容器编排的方案。 2. 为什么需要kubernetes2.1 传统的容器编排痛点容器技术虽然解决了应用和基础设施异构的问题，让应用可以做到一次构建，多次部署，但在复杂的微服务场景，单靠 Docker 技术还不够，它仍然有以下问题没有解决： 集成和编排微服务模块 提供按需自动扩容，缩容能力 故障自愈 集群内的通信 2.2 Kubernetes 能解决的问题 按需的垂直扩容，新的服务器(node)能够轻易的增加或删除 按需的水平扩容，容器实例能够轻松扩容，缩容 副本控制器，你不用担心副本的状态 服务发现和路由 自动部署和回滚，如果应用状态错误，可以实现自动回滚 2.3 什么时候使用 Kubernetes？ 当你的应用是微服务架构 开发者需要快速部署自己的新功能到测试环境进行验证 降低硬件资源成本，提高使用率 2.4 什么时候不适合使用 Kubernetes 应用是轻量级的单体应用，没有高并发的需求 团队文化不适应变革 3. Kubernetes 架构与核心概念 3.1 主控制节点组件主控制节点组件对集群做出全局决策(比如调度)，以及检测和响应集群事件（例如，当不满足部署的 replicas 字段时，启动新的 pod）。 主控制节点组件可以在集群中的任何节点上运行。 然而，为了简单起见，设置脚本通常会在同一个计算机上启动所有主控制节点组件，并且不会在此计算机上运行用户容器。 apiserver主节点上负责提供 Kubernetes API 服务的组件；它是 Kubernetes 控制面的前端组件。 etcdetcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。 kube-scheduler主节点上的组件，该组件监视那些新创建的未指定运行节点的 Pod，并选择节点让 Pod 在上面运行。调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。 kube-controller-manager在主节点上运行控制器的组件。从逻辑上讲，每个控制器都是一个单独的进程，但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。这些控制器包括: 1. 节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应。 2. 副本控制器（Replication Controller）: 负责为系统中的每个副本控制器对象维护正确数量的 Pod。 3. 终端控制器（Endpoints Controller）: 填充终端(Endpoints)对象(即加入 Service 与 Pod)。 4. 服务帐户和令牌控制器（Service Account &amp; Token Controllers），为新的命名空间创建默认帐户和 API 访问令牌. 3.2 从节点组件节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。 kubelet一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。kubelet 不会管理不是由 Kubernetes 创建的容器。 kube-proxykube-proxy 是集群中每个节点上运行的网络代理,实现 Kubernetes Service 概念的一部分。kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。 容器运行时（Container Runtime）容器运行环境是负责运行容器的软件。Kubernetes 支持多个容器运行环境: Docker、 containerd、cri-o、 rktlet 以及任何实现 Kubernetes CRI (容器运行环境接口)。 3.3 插件（Addons） Kubeadm Kubeadm 是Kubernetes的自动化部署工具，降低了部署难度，提高效率。 Kubectl Kubectl 是Kubernetes集群管理工具相当于客户端。 DNS尽管其他插件都并非严格意义上的必需组件，但几乎所有 Kubernetes 集群都应该有集群 DNS， 因为很多示例都需要 DNS 服务。 Web 界面（仪表盘）Dashboard 是K ubernetes 集群的通用的、基于 Web 的用户界面。 它使用户可以管理集群中运行的应用程序以及集群本身并进行故障排除。 容器资源监控容器资源监控 将关于容器的一些常见的时间序列度量值保存到一个集中的数据库中，并提供用于浏览这些数据的界面。 集群层面日志集群层面日志 机制负责将容器的日志数据 保存到一个集中的日志存储中，该存储能够提供搜索和浏览接口。 4. 初始化基础环境123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081# 下载yum 源wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo# 清除缓存yum clean all# 安装基本软件包yum install wget net‐tools vim bash‐comp* ‐y#配置 K8S的阿里云yum源cat &gt;&gt;/etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 配置 Docker yum源wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo# 清除缓存yum clean all# 设置hostsvim /etc/hosts 192.168.99.101 master 192.168.99.102 node1 192.168.99.103 node2# 关闭防火墙systemctl stop firewalldsystemctl disable firewalld# 关闭 SeLinuxsetenforce 0sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config# 关闭 swapswapoff -ayes | cp /etc/fstab /etc/fstab_bakcat /etc/fstab_bak |grep -v swap &gt; /etc/fstab# 将桥接的IPv4流量传递到iptables的链modprobe br_netfilterecho &quot;1&quot; &gt;/proc/sys/net/bridge/bridge-nf-call-iptablesvi /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1# 安装并启动 dockeryum install -y docker-ce.x86_64 docker-ce-cli.x86_64 containerd.io.x86_64# 更换 docker 镜像源mkdir /etc/dockercat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; &quot;registry-mirrors&quot;: [&quot;https://registry.cn-hangzhou.aliyuncs.com&quot;], &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;EOF# 重启dockersystemctl daemon-reload# 开机自启systemctl enable docker systemctl restart docker# 如果报网络错误可能是没有成功建立docker网卡，可以手动创建`failed to start daemon: Error initializing network controller: list bridge addresses failed: PredefinedLocalScopeDefaultNetworks List:`ip link add name docker0 type bridgeip addr add dev docker0 172.1.0.1/16# 安装kubelet、kubeadm、kubectlyum install -y kubelet-1.15.10 kubeadm-1.15.10 kubectl-1.15.10# 启动 kubeletsystemctl enable kubelet &amp;&amp; systemctl start kubelet 5. 初始化 Master123456789101112131415161718192021222324252627282930313233343536# 初始化K8s主节点kubeadm init --kubernetes-version=1.15.10 \\--apiserver-advertise-address=$&#123;master_ip&#125; \\--image-repository registry.aliyuncs.com/google_containers \\--service-cidr=10.1.0.0/16 \\--pod-network-cidr=10.244.0.0/16mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 安装网络插件 Flannelkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 查看K8s状态kubectl get node NAME STATUS ROLES AGE VERSION master Ready master 14m v1.15.10 # 故障排查 ！！！# kubelet 启动失败 报错：Failed to start ContainerManager failed to get rootfs info: unable to find data in memory cachevim /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf 添加下面环境变量Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml --feature-gates=\\&quot;LocalStorageCapacityIsolation=false,SupportNodePidsLimit=false,SupportPodPidsLimit=false\\&quot;&quot;# Failed to update stats for container &quot;/kubepods.slice/kubepods-burstable.slice&quot;: failure - /sys/fs/cgroup/cpuacct/kubepods.slice/kubepods-burstable.slice/cpuacct.statcat &gt;/etc/systemd/system/kubelet.service.d/11-cgroups.conf&lt;&lt;EOF[Service]CPUAccounting=trueMemoryAccounting=trueEOF并在刚才那个环境变量上追加参数：Environment=&quot;KUBELET_CONFIG_ARGS=--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice&quot;# 然后重启 kubeletsystemctl daemon-reload &amp;&amp; systemctl restart kubelet 6. 初始化 WorkerNode123456789101112131415161718192021222324252627282930313233343536373839404142# 拷贝 admin.conf 到 wokerNodescp /etc/kubernetes/admin.conf root@node1:/etc/kubernetes/# 配置 Kubeconfig 环境变量echo &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; ~/.bash_profilesource ~/.bash_profile# 如果是克隆的 master 机器, 需要清理 master 环境网络kubeadm resetsystemctl stop kubeletsystemctl stop dockerrm -rf /var/lib/cni/rm -rf /var/lib/kubelet/*rm -rf /var/cni/ifconfig cni0 downifconfig flannel.1 downifconfig docker0 downip link delete cni0ip link delete flannel.1systemctl start dockersystemctl start kubelet# 否则安装 Flannel 网络插件kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 将master节点下面 /etc/cni/net.d/下面的所有文件拷贝到node节点上mkdir -p /etc/cni/net.d/# 在 masterscp /etc/cni/net.d/* root@$&#123;node_ip&#125;:/etc/cni/net.d/# 在 master 生成tokenkubeadm token create --print-join-command&gt; kubeadm join $&#123;master_ip&#125;:6443 --token 56peeh.gx8l6z1vwo04usrb --discovery-token-ca-cert-hash sha256:3ba111312aee9e77ca7939f8336db665b01ea3f457bee501117810b2d8ccfe3c# 在wokerNode上执行这个joinkubeadm join $&#123;master_ip&#125;:6443 --token 56peeh.gx8l6z1vwo04usrb --discovery-token-ca-cert-hash sha256:3ba111312aee9e77ca7939f8336db665b01ea3f457bee501117810b2d8ccfe3c# 查看集群状态kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready master 15m v1.15.10 node1 Ready &lt;none&gt; 9m41s v1.15.10 7. 安装 Kubernetes Dashboard12345# 安装 Kubernetes Dashboard 根据 CRD，使用http免密登录kubectl apply -f https://github.com/coderzc/coderzc.github.io/tree/master/blog/%E5%AE%B9%E5%99%A8%E5%8C%96/recommended.yaml# 显示 admin 的 tokenkubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token 8. PodPod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。 Pod （就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个） 容器； 这些容器共享存储、网络、以及怎样运行这些容器的声明。 Pod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。 Pod 所建模的是特定于应用的“逻辑主机”，其中包含一个或多个应用容器， 这些容器是相对紧密的耦合在一起的。 在非云环境中，在相同的物理机或虚拟机上运行的应用类似于 在同一逻辑主机上运行的云应用。 除了应用容器，Pod 还可以包含在 Pod 启动期间运行的 Init 容器。 你也可以在集群中支持临时性容器 的情况下，为调试的目的注入临时性容器。 详细介绍：https://kubernetes.io/zh/docs/concepts/workloads/pods","categories":[{"name":"容器化","slug":"容器化","permalink":"https://coderzc.github.io/categories/%E5%AE%B9%E5%99%A8%E5%8C%96/"}],"tags":[]},{"title":"docker","slug":"容器化/20210718-docker","date":"2021-07-17T16:00:00.000Z","updated":"2021-08-18T02:58:32.968Z","comments":true,"path":"2021/07/18/容器化/20210718-docker/","link":"","permalink":"https://coderzc.github.io/2021/07/18/%E5%AE%B9%E5%99%A8%E5%8C%96/20210718-docker/","excerpt":"1. 什么是Docker 官方解释：Package Software into Standardized Units for Development, Shipment and Deployment. 2. Docker namespace 隔离原理 Docker通过 Namespace 实现进程隔离 1 int clone(int (*child_func)(void *), void *child_stack, int flags, void *arg); 模拟隔离 1 2 # 创建一个隔离环境 unshare --fork --pid --mount-proc ba","text":"1. 什么是Docker官方解释：Package Software into Standardized Units for Development, Shipment and Deployment. 2. Docker namespace 隔离原理Docker通过 Namespace 实现进程隔离 1int clone(int (*child_func)(void *), void *child_stack, int flags, void *arg); 模拟隔离 12# 创建一个隔离环境unshare --fork --pid --mount-proc bash 3. Docker 的资源配额 CGroups3.1 Docker 使用CGroups实现资源的配额管理 Cgroups (control groups) 2007年由谷歌工程师研发 2008年并入 Linux Kernel 2.6.24 C语言实现 3.2 CGroups 限制进程的 CPU使用时间Docker中的 CPU，内存，网络的限制均通过 cgroups 实现 3.3 实践123456789101112131415161718# 在宿主机上创建一个让 CPU 飙升到100%的进程： （此操作有风险，慎用）while : ; do : ; done &amp;# 记录下 PID = 5004cd /sys/fs/cgroup/cpumkdir cgroups_test# 查看配额cat cpu.cfs_quota_us# 设定20%cpu时间的上限echo 20000 &gt; cpu.cfs_quota_us# 绑定进程号echo 27358 &gt; /sys/fs/cgroup/cpu/cgroups_test/tasks# 清理该进程kill -9 5004 3.4 docker 里如何加参数进行资源配额123456# 分配一个50%的cpu时间配额docker run -it --cpus=&quot;.5&quot; nginx /bin/sh# 查看是否有对应的cgroupcat /sys/fs/cgroup/cpu/cpu.cfs_quota_us# 配置显示 500000，证明--cpus=&quot;.5&quot;的参数已经生效 Docker 镜像4.1 Docker 镜像由来与特性 虽然 Docker 实现了运行环境的隔离，但如何将一个运行的容器快速进行启动，复制，迁移到其他的主机上运行？ 如果容器无法快速进行复制，迁移，那么和以 VMware 为代表的虚拟化技术相比并没有太多优势 Docker 镜像具备了应用运行所需要的所有依赖 一次构建，处处运行 Docker 镜像的存储是基于 checksum 的去重存储，大大降低存储空间 4.2 编写Dockerfile12345678910111213FROM openjdk:8-jdk-alpineLABEL maintainer=&quot;coderzc async-nio-concurrent&quot;VOLUME /tmpADD async-nio-concurrent-0.0.1-SNAPSHOT.jar async-nio-concurrent.jarENV mysql_hostname=mysql57# 开放容器的端口EXPOSE 8088 8081ENTRYPOINT [&quot;JAVA&quot;, &quot;-jar&quot;, &quot;async-nio-concurrent.jar&quot;] 4.3 构建并上传 Docker 镜像1234567891011121314# 把 async-nio-concurrent-0.0.1-SNAPSHOT.jar 放到与 Dockerfile 同级目录&gt; async-nio-concurrent-0.0.1-SNAPSHOT.jar Dockerfile# 构建根据 Dockerfile 构建镜像docker build -t async-nio-concurrent:1.0 .# 登陆 docker hubdocker login# 把本地镜像归入仓库docker tag async-nio-concurrent:1.0 czcoder/async-nio-concurrent:1.0# pushdocker push czcoder/async-nio-concurrent:1.0 4.4 Docker run –link 运行12345678# 把 mysql 与 async-nio-concurrent 网络打通, 并增加一条 mysql57 的 hosts 记录docker run --name async-nio-concurrent -d -p 18081:8081 -p 8088:8088 --link mysql57 async-nio-concurrent:1.0# 查看日志docker logs async-nio-concurrent# 进入容器docker exec -it async-nio-concurrent /bin/bash 4.5 外网访问http://${宿主机ip}:18081/index.html 5. Docker 常见命令1234567891011121314# 查看 docker 基本信息docker info# 查看 docker 镜像docker images# 删除 docker 镜像docker rmi $image_name# 查看 docker 容器docker ps -a# 启动/停止/重启容器docker start/stop/restart $CONTAINER","categories":[{"name":"容器化","slug":"容器化","permalink":"https://coderzc.github.io/categories/%E5%AE%B9%E5%99%A8%E5%8C%96/"}],"tags":[]},{"title":"TCP RST 相关理解","slug":"NIO与网络编程/20200601-TCP RST 相关理解","date":"2020-05-31T16:00:00.000Z","updated":"2021-08-18T02:58:32.965Z","comments":true,"path":"2020/06/01/NIO与网络编程/20200601-TCP RST 相关理解/","link":"","permalink":"https://coderzc.github.io/2020/06/01/NIO%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/20200601-TCP%20RST%20%E7%9B%B8%E5%85%B3%E7%90%86%E8%A7%A3/","excerpt":"一、出现RST包的情况 1、连接请求到达时，目的端口不存在情况 2、向一个已经关闭的连接发送数据 3、向一个已经崩溃的对端发送数据（连接之前已经被建立） 4、请求超时。 使用setsockopt的SO_RCVTIMEO选项设置recv的超时时间。接收数据超时时，会发送RST包 5、close(sockfd)时，直接丢弃接收缓冲区未读取的数据，并给对方发一个RST。这个是由SO_LINGER选项来控制的 6、TCP收到了一个根本不存在的连接上的分节 7、处理半打开连接， 一方关闭了连接，另一方却没有收到结束报文（如网络故障），此时另一方还维持着原来的连接。而一方即使重启，也没有该连接","text":"一、出现RST包的情况1、连接请求到达时，目的端口不存在情况 2、向一个已经关闭的连接发送数据 3、向一个已经崩溃的对端发送数据（连接之前已经被建立） 4、请求超时。 使用setsockopt的SO_RCVTIMEO选项设置recv的超时时间。接收数据超时时，会发送RST包 5、close(sockfd)时，直接丢弃接收缓冲区未读取的数据，并给对方发一个RST。这个是由SO_LINGER选项来控制的 6、TCP收到了一个根本不存在的连接上的分节 7、处理半打开连接， 一方关闭了连接，另一方却没有收到结束报文（如网络故障），此时另一方还维持着原来的连接。而一方即使重启，也没有该连接的任何信息。这种状态 就叫做半打开连接。而此时另一方往处于半打开状态的连接写数据，则对方回应RST复位报文 8、应用层可以通过 设置SO_LINGER 来发送RST 12345#include &lt;sys/socket.h&gt;struct linger &#123;int l_onoff //0=off, nonzero=on(开关)int l_linger //linger time(延迟时间)&#125; 二、收到RST包后的表现1、TCP socket在任何状态下，只要收到RST包，即可进入CLOSED初始状态 2、值得注意的是RST报文段不会导致另一端产生任何响应，另一端根本不进行确认。收到RST的一方将终止该连接 三、程序中表现1、阻塞模型下，内核无法主动通知应用层出错，只有应用层主动调用read()或者write()这样的IO系统调用时，内核才会利用出错来通知应用层对端RST 2、非阻塞模型下，select或者epoll会返回sockfd可读,应用层对其进行读取时，read()会报错RST 这点很重要，这也是大多数Connection reset by peer的原因，我们需要捕获这个异常并关闭连接","categories":[{"name":"NIO与网络编程","slug":"NIO与网络编程","permalink":"https://coderzc.github.io/categories/NIO%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[]},{"title":"epoll","slug":"NIO与网络编程/20200601-epoll","date":"2020-05-31T16:00:00.000Z","updated":"2021-08-18T02:58:32.966Z","comments":true,"path":"2020/06/01/NIO与网络编程/20200601-epoll/","link":"","permalink":"https://coderzc.github.io/2020/06/01/NIO%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/20200601-epoll/","excerpt":"函数主要功能 1、epoll_create 从slab缓存中创建一个eventpoll对象,并且创建一个匿名的fd跟fd对应的file对象,而eventpoll对象保存在struct file结构的private指针中,并且返回, 该fd对应的file operations只是实现了poll跟release操作，创建eventpoll对象的初始化操作 获取当前用户信息,是不是root,最大监听fd数目等并且保存到eventpoll对象中 初始化等待队列,初始化就绪链表,初始化红黑树的头结点 2、epoll_ctl 将epoll_event结构拷贝到内核空间中，并且判断加入的fd是否支持","text":"函数主要功能 1、epoll_create从slab缓存中创建一个eventpoll对象,并且创建一个匿名的fd跟fd对应的file对象,而eventpoll对象保存在struct file结构的private指针中,并且返回, 该fd对应的file operations只是实现了poll跟release操作，创建eventpoll对象的初始化操作获取当前用户信息,是不是root,最大监听fd数目等并且保存到eventpoll对象中 初始化等待队列,初始化就绪链表,初始化红黑树的头结点 2、epoll_ctl将epoll_event结构拷贝到内核空间中，并且判断加入的fd是否支持poll结(epoll,poll,selectI/O多路复用必须支持poll操作). 从epfd-&gt;file-&gt;privatedata获取event_poll对象,根据op区分是添加删除还是修改, 首先在eventpoll结构中的红黑树查找是否已经存在了相对应的fd,没找到就支持插入操作,否则报重复的错误，还有修改,删除操作。 插入操作时,会创建一个与fd对应的epitem结构,并且初始化相关成员，并指定调用poll_wait时的回调函数用于数据就绪时唤醒进程,(其内部,初始化设备的等待队列,将该进程注册到等待队列)完成这一步, epitem就跟这个socket关联起来了, 当它有状态变化时,会通过ep_poll_callback()来通知. 最后调用加入的fd的fileoperation-&gt;poll函数(最后会调用poll_wait操作)用于完注册操作，将epitem结构添加到红黑树中。 3、epoll_wait计算睡眠时间(如果有),判断eventpoll对象的链表是否为空,不为空那就干活不睡明.并且初始化一个等待队列,把自己挂上去,设置自己的进程状态 若是可睡眠状态.判断是否有信号到来(有的话直接被中断醒来,),如果没有那就调用schedule_timeout进行睡眠, 如果超时或者被唤醒,首先从自己初始化的等待队列删除,然后通过调用ffd.file-&gt;f_op-&gt;poll()再次检查每个文件描述符是否真的准备好了,然后开始拷贝资源给用户空间了 拷贝资源则是先把就绪事件链表转移到中间链表,然后挨个遍历拷贝到用户空间,并且挨个判断其是否为水平触发,是的话再次插入到就绪链表 epoll 剖析epoll源码剖析","categories":[{"name":"NIO与网络编程","slug":"NIO与网络编程","permalink":"https://coderzc.github.io/categories/NIO%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[]},{"title":"二进制、计算机编码、位运算","slug":"计算机基础/20200420-二进制、计算机编码、位运算","date":"2020-04-19T16:00:00.000Z","updated":"2021-08-18T02:58:32.973Z","comments":true,"path":"2020/04/20/计算机基础/20200420-二进制、计算机编码、位运算/","link":"","permalink":"https://coderzc.github.io/2020/04/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/20200420-%E4%BA%8C%E8%BF%9B%E5%88%B6%E3%80%81%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BC%96%E7%A0%81%E3%80%81%E4%BD%8D%E8%BF%90%E7%AE%97/","excerpt":"整理一下二进制、计算机编码、位运算相关知识，拒绝含糊不清 主要涉及三个基本数据类型 byte、int、char 以16进制表示查看二进制文件 1 xxd file 反码、补码： https://www.cnblogs.com/zhangziqiu/archive/2011/03/30/ComputerCode.html 上面虽然解释了 反码，补码，但是还是有一些问题没有解释清楚：为什么负数的补码是反码+1，为什么正数的补码还是本身，-128的补码为什么是10000000等等，请看下面👇 https://www.cnblogs.com/esmusssein/p/11182321.h","text":"整理一下二进制、计算机编码、位运算相关知识，拒绝含糊不清 主要涉及三个基本数据类型 byte、int、char 以16进制表示查看二进制文件1xxd file 反码、补码：https://www.cnblogs.com/zhangziqiu/archive/2011/03/30/ComputerCode.html 上面虽然解释了 反码，补码，但是还是有一些问题没有解释清楚：为什么负数的补码是反码+1，为什么正数的补码还是本身，-128的补码为什么是10000000等等，请看下面👇https://www.cnblogs.com/esmusssein/p/11182321.html 字符编码：https://www.ibm.com/developerworks/cn/java/j-lo-chinesecoding/ Java 判断本地主机字节序工具：ByteOrder.nativeOrder();大端小端在一台机器上没感知，因为高位地位是确定的，只不过在有的机器上内存的高地址对应高位叫“小端字节序” 像x86架构，而有的是内存的高地址对应低位叫“大端字节序”。 Unicode 编码查询工具：https://www.qqxiuzi.cn/bianma/Unicode-UTF.php Java内码是utf-16吗？https://blog.csdn.net/u014631304/article/details/77509380 关于Unicode更多理解：https://www.jianshu.com/p/ad4bff4d9fa3 Java编码扩展知识：https://www.jianshu.com/p/1b00ca07b003 理解byte负数：https://blog.csdn.net/csdn_ds/article/details/79106006 &amp; 0xff 获取真值原理：https://blog.csdn.net/i6223671/article/details/88924481 常用工具代码： byte 转 二进制表示 123456789101112/** * byte 转 二进制表示 * @param b * @return */public static String byteToBit(byte b) &#123; return &quot;&quot; + (byte) ((b &gt;&gt; 7) &amp; 0x1) + (byte) ((b &gt;&gt; 6) &amp; 0x1) + (byte) ((b &gt;&gt; 5) &amp; 0x1) + (byte) ((b &gt;&gt; 4) &amp; 0x1) + (byte) ((b &gt;&gt; 3) &amp; 0x1) + (byte) ((b &gt;&gt; 2) &amp; 0x1) + (byte) ((b &gt;&gt; 1) &amp; 0x1) + (byte) ((b &gt;&gt; 0) &amp; 0x1);&#125; 获取byte指定位的值 123456789/** * 获取byte指定位的值 * @param b * @param i (i&gt;=1) * @return */public static byte getBitArray(byte b,int i) &#123; return (byte) ((b &gt;&gt; (8-i)) &amp; 0x1);&#125; 字节数组转16进制 12345678910111213141516/** * 字节数组转16进制 * @param bytes 需要转换的byte数组 * @return 转换后的Hex字符串 */public static String bytesToHex(byte[] bytes) &#123; StringBuffer sb = new StringBuffer(); for(int i = 0; i &lt; bytes.length; i++) &#123; String hex = Integer.toHexString(bytes[i] &amp; 0xFF); if(hex.length() &lt; 2)&#123; sb.append(0); &#125; sb.append(hex); &#125; return sb.toString();&#125; char 转 byte数组(JVM大端转化的结果，非真实内存存放顺序) 1234567891011/** * char 转 byte数组 * @param c * @return */public static byte[] charToByte(char c) &#123; byte[] b = new byte[2]; b[0] = (byte) ((c &amp; 0xFF00) &gt;&gt; 8); b[1] = (byte) (c &amp; 0xFF); return b;&#125; 16进制 转 字节数组 1234567891011121314151617181920/** * 16进制 转 字节数组 * @param hexString * @return */ public static byte[] hexString2Bytes(String hexString) &#123; BiFunction&lt;Byte, Byte, Byte&gt; uniteBytes = (src0, src1) -&gt; &#123; char b0 = (char) Byte.decode(&quot;0x&quot; + new String(new byte[] &#123;src0&#125;)).byteValue(); b0 = (char) (b0 &lt;&lt; 4); char b1 = (char) Byte.decode(&quot;0x&quot; + new String(new byte[] &#123;src1&#125;)).byteValue(); return (byte) (b0 ^ b1); &#125;; int size = hexString.length(); byte[] ret = new byte[size / 2]; byte[] tmp = hexString.getBytes(); for (int i = 0; i &lt; size / 2; i++) &#123; ret[i] = uniteBytes.apply(tmp[i * 2], tmp[i * 2 + 1]); &#125; return ret; &#125; byte数组 转 int 1234567891011121314/** * byte数组 转 int * @param bytes * @return */public static int bytes2Int(byte[] bytes) &#123; int result = 0; //将每个byte依次搬运到int相应的位置 result = bytes[0] &amp; 0xff; result = result &lt;&lt; 8 | bytes[1] &amp; 0xff; result = result &lt;&lt; 8 | bytes[2] &amp; 0xff; result = result &lt;&lt; 8 | bytes[3] &amp; 0xff; return result;&#125; int 转 byte数组 1234567891011121314/** * int 转 byte数组 * @param num * @return */public static byte[] int2Bytes(int num) &#123; byte[] bytes = new byte[4]; //通过移位运算，截取低8位的方式，将int保存到byte数组 bytes[0] = (byte) (num &gt;&gt;&gt; 24); bytes[1] = (byte) (num &gt;&gt;&gt; 16); bytes[2] = (byte) (num &gt;&gt;&gt; 8); bytes[3] = (byte) num; return bytes;&#125; 获取String内存中对应的 UTF-16 编码的byte数组 (JVM大端转化的结果，非真实内存存放顺序) 1234567891011121314151617/** * 获取String内存中对应的 UTF-16 编码的byte数组 * @param str * @return */public static byte[] string2bytes(String str) &#123; ByteBuffer heapByteBuffer = ByteBuffer.allocate (str.length() * 2); for (char c : str.toCharArray()) &#123; heapByteBuffer.putChar(c); &#125; heapByteBuffer.flip(); int len = heapByteBuffer.limit() - heapByteBuffer.position(); byte[] bytes = new byte[len]; heapByteBuffer.get(bytes); return bytes;&#125; 通过Unsafe查看字符在内存中的实际顺序(非JVM优化过的,我认为这才是内存真实存放的顺序，取决于具体的CPU架构) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@SuppressWarnings(&quot;restriction&quot;)static private sun.misc.Unsafe getUnsafe() throws IllegalArgumentException, IllegalAccessException &#123; Class&lt;?&gt; cls = sun.misc.Unsafe.class; Field[] fields = cls.getDeclaredFields(); for (Field f : fields) &#123; if (&quot;theUnsafe&quot;.equals(f.getName())) &#123; f.setAccessible(true); return (sun.misc.Unsafe) f.get(null); &#125; &#125; throw new IllegalAccessException(&quot;no declared field: theUnsafe&quot;);&#125;/** * 查看字符在内存中真实的顺序，非JVM优化的，取决于CPU架构 * @param ch * @return * @throws IllegalAccessException */public static byte[] char2bytesNative(char ch) throws IllegalAccessException &#123; byte[] bytes = new byte[2]; Unsafe unsafe = getUnsafe(); long address = unsafe.allocateMemory(2); unsafe.putChar(address, ch); byte b = unsafe.getByte(address); bytes[0] = b; b = unsafe.getByte(address + 1); bytes[1] = b; return bytes;&#125;public static void main(String[] args) throws Throwable &#123; System.out.println(bytes2HexString(charToByte(&#x27;国&#x27;))); System.out.println(bytes2HexString(char2bytesNative(&#x27;国&#x27;)));&#125;/** * 还有一种更直接的方法 * @param ch * @return * @throws IllegalAccessException */public static byte[] char2bytesNative2(char ch) throws IllegalAccessException &#123; Unsafe unsafe = getUnsafe(); char[] charArr = new char[1]; charArr[0] = ch; byte b = unsafe.getByte(charArr, (long) (unsafe.arrayBaseOffset(charArr.getClass()))); byte[] bytes = new byte[2]; bytes[0] = b; b = unsafe.getByte(charArr, (long) (unsafe.arrayBaseOffset(charArr.getClass()) + 1)); bytes[1] = b; return bytes;&#125;","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://coderzc.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"实现 NIO Socket","slug":"NIO与网络编程/20190624-实现 NIO Socket","date":"2019-06-23T16:00:00.000Z","updated":"2021-08-18T02:58:32.965Z","comments":true,"path":"2019/06/24/NIO与网络编程/20190624-实现 NIO Socket/","link":"","permalink":"https://coderzc.github.io/2019/06/24/NIO%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/20190624-%E5%AE%9E%E7%8E%B0%20NIO%20Socket/","excerpt":"服务端 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101","text":"服务端123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.nio.charset.Charset;import java.text.SimpleDateFormat;import java.util.Date;import java.util.Set;/** * @author: coderzc */public class NioSocketServer &#123; private static Selector selector = null; private static ServerSocketChannel serverSocketChannel = null; private static ByteBuffer writeBuffer = ByteBuffer.allocateDirect(1024); private static ByteBuffer readBuffer = ByteBuffer.allocateDirect(1024); private static StringBuffer message = new StringBuffer(); private static SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss:SSS&quot;); static &#123; // 初始化 try &#123; // socket() serverSocketChannel = ServerSocketChannel.open(); // 把serverSocketChannel 变成非阻塞模式（accept非阻塞） serverSocketChannel.configureBlocking(false); // bind()、listen() serverSocketChannel.bind(new InetSocketAddress(8888)); System.out.println(&quot;listening on port 8888&quot;); // 创建多路io复用器【select/poll/epoll】 // 相当于 epoll_create() 创建epoll的根结点 selector selector = Selector.open(); // 相当于 epoll_ctl(,EPOLL_CTL_ADD,) 把serverSocket注册到selector这个多路复用器上（上epoll树），检测类型为accept事件 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) throws IOException &#123; // 相当于 epoll_wait(,,,-1) while (selector.select(0L) &gt; 0) &#123; //参数：0L永远阻塞 ；返回值: 不会等于0 // 遍历io就绪队列 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); System.out.println(&quot;\\033[31;m&quot;+&quot;selector is activity \\033[0m，keySize：&quot; + selectionKeys.size()); for (SelectionKey selectionKey : selectionKeys) &#123; // isAcceptable() 返回true 代表该channel是accepted的serverSocketChannel if (selectionKey.isAcceptable()) &#123; System.out.println(&quot;\\033[33;4m&quot;+simpleDateFormat.format(new Date())+&quot;\\033[0m \\033[31;4misAcceptable&quot;+&quot;\\033[0m&quot;); ServerSocketChannel serverChannel = (ServerSocketChannel) selectionKey.channel(); SocketChannel socketChannel = serverChannel.accept(); // 防止客户端传来RST，socket被移除就绪队列,判断一下（之前已经把serverSocketChannel设置为非阻塞，所有就绪队列没有会立刻返回null） if (socketChannel == null) &#123; continue; &#125; // 把socketChannel变成非阻塞模式（读、写非阻塞） socketChannel.configureBlocking(false); // 把socketChannel注册到多路复用器上（上epoll树） SelectionKey newSocketKey = socketChannel.register(selector, SelectionKey.OP_READ); // 打印客户端ip：port String ip = socketChannel.socket().getInetAddress().getHostAddress(); int port = socketChannel.socket().getPort(); String format = String.format(&quot;hi new client ip:%s,port:%s\\n&quot;, ip, port); System.out.println(format); // 向客户端输出hi，ip:port // 把读到的数据绑定到key中 newSocketKey.attach(&quot;hi～，&quot;+format); // 注册写事件 newSocketKey.interestOps(newSocketKey.interestOps() | SelectionKey.OP_WRITE); &#125; // isReadable() 返回true 代表channel是readable的socketChannel else if (selectionKey.isReadable()) &#123; System.out.println(&quot;\\033[33;4m&quot;+simpleDateFormat.format(new Date())+&quot;\\033[0m \\033[31;4misReadable\\033[0m&quot;); handleReceive(selectionKey); &#125; else if (selectionKey.isWritable()) &#123; System.out.println(&quot;\\033[33;4m&quot;+simpleDateFormat.format(new Date())+&quot;\\033[0m \\033[31;4misWritable&quot;+&quot;\\033[0m&quot;); handleSend(selectionKey); &#125; &#125; // 清除处理过的事件,防止下次循环时重复处理 // 感觉想java的bug，你每次执行select() 时候java从调用内核函数epoll_wait()获得到最近的就绪队列后直接addAll到selectionKeys，不会帮你清空selectionKeys selectionKeys.clear(); &#125; &#125; private static void handleReceive(SelectionKey selectionKey) throws IOException &#123; SocketChannel socketChannel = (SocketChannel) selectionKey.channel(); // 读取channel中数据 readBuffer.clear(); int read = 0; try &#123; while ((read = socketChannel.read(readBuffer)) &gt; 0) &#123;// 循环读取缓冲区数据，直到把缓冲区读空，防止频繁调用select()，要配合非阻塞channel才会发挥最大性能// System.out.println(&quot;readSize：&quot; + read); readBuffer.flip(); String chunk = Charset.forName(&quot;UTF-8&quot;).decode(readBuffer).toString();// System.out.println(&quot;chunk：&quot;+chunk); message.append(chunk); // 检测消息行尾 if (message.indexOf(&quot;\\n&quot;) &gt;= 0) &#123; System.out.print(&quot;receiveData ---&gt;&quot; + &quot;\\033[36;4m&quot;+message+&quot;\\033[0m&quot;); // 把要写的的数据绑定到key中 selectionKey.attach(&quot;server message echo:&quot; + &quot;已经收到\\n&quot;); // 注册写事件 selectionKey.interestOps(selectionKey.interestOps() | SelectionKey.OP_WRITE); // 清除客户端消息行缓存 message.setLength(0); &#125; readBuffer.clear(); &#125; // read == -1 代表客户端已经断开 if (read == -1) &#123; System.out.println(&quot;disconnect a client..&quot;); // 反注册channel selectionKey.cancel(); // 关闭socket socketChannel.close(); &#125; //非阻塞模式下，read==0代表当前系统缓冲区已经空了 if (read == 0) &#123; System.out.println(&quot;000,接受缓冲区已经读完了\\n&quot;); &#125; &#125;catch (Exception e)&#123; System.err.println(&quot;读取数据发生异常&quot;); // 反注册channel selectionKey.cancel(); // 关闭socket socketChannel.close(); &#125; &#125; private static void handleSend(SelectionKey selectionKey) throws IOException &#123; SocketChannel socketChannel = (SocketChannel) selectionKey.channel(); String message = (String) selectionKey.attachment(); if (message == null) &#123; return; &#125; selectionKey.attach(null); writeBuffer.clear(); writeBuffer.put(message.getBytes(Charset.forName(&quot;UTF-8&quot;))); writeBuffer.flip(); while (writeBuffer.hasRemaining()) &#123; socketChannel.write(writeBuffer); &#125; System.out.println(&quot;写出数据 ---&gt; &quot;+message); // 取消读事件,防止isWritable一直激活，cpu空转 selectionKey.interestOps(selectionKey.interestOps() &amp; ~SelectionKey.OP_WRITE); &#125; 客户端 (支持断线自动重连)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.SocketChannel;import java.nio.charset.Charset;import java.util.Set;/** * @author: coderzc */public class NioSocketClient &#123; private static final ByteBuffer sendBuffer = ByteBuffer.allocateDirect(8196); private static final ByteBuffer receiveBuffer = ByteBuffer.allocateDirect(1024); private static Selector selector = null; private static SocketChannel socketChannel = null; private static SelectionKey registerKey = null; private static Thread reactor = null; private static boolean isConnected() &#123; return socketChannel != null &amp;&amp; socketChannel.isConnected(); &#125; public static void main(String[] args) throws IOException &#123; // 打开复用器 selector = Selector.open(); // 开始连接 initConnect(); // 启动reactor线程处理 这里和reactor和netty不太一样，就是我自己起的名，用来处理事件的select循环而已 reactor = new Thread(NioSocketClient::reactor); reactor.start(); // 监听键盘输入 Thread receiver = new Thread(NioSocketClient::listenUserInput); receiver.start(); &#125; /** * 初始化连接，注册OP_CONNECT事件 */ private static void initConnect() &#123; try &#123; // 打开通道 socketChannel = SocketChannel.open(); //设置通道为非阻塞 socketChannel.configureBlocking(false); //连接主机 boolean connect = socketChannel.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 8888)); //注册事件 /* TODO 唤醒select * 因为主线程select() 阻塞，将会导致下面的方法中，当前线程调用socketChannel.register() 由于获取不到锁也阻塞 * 所以要在调register之前把select()唤醒 * * selector.wakeup(); * registerKey = socketChannel.register(selector, 0); * * TODO selector.wakeup() 不好控制，所以改成在前面调用interrupt(),中断reactor线程，在注册连接事件后，重启一个新的reactor线程 */ if (connect) &#123; registerKey = socketChannel.register(selector, SelectionKey.OP_READ); &#125; else &#123; registerKey = socketChannel.register(selector, SelectionKey.OP_CONNECT); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; private static void reconnection() &#123; //清除旧的连接 try &#123; registerKey.cancel(); socketChannel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; // TODO 中断reactor线程 // (其实是中断线程阻塞提前返回,当thread.sleep、thread.join、thread.wait、object.wait 遇到interrupt时，会抛出异常而使线程提前终止) // ，但这些方法会在抛出异常的同时通过Thread.interrupted()将中断标识清除 reactor.interrupt(); // 注册新的OP_CONNECT事件 initConnect(); // 重启reactor线程处理 reactor = new Thread(NioSocketClient::reactor); reactor.start(); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private static void reactor() &#123; try &#123; // handler socket data while (!Thread.interrupted()) &#123; int count = -2; while ((count = selector.select(0L)) &gt; 0) &#123;// System.out.println(count+&quot;---&quot;); Set&lt;SelectionKey&gt; keys = selector.selectedKeys();// if (keys == null || keys.size() == 0) &#123;// continue;// &#125; System.out.println(&quot;selector is activity，keySize：&quot; + keys.size()); for (SelectionKey key : keys) &#123; // 判断连接状态 if (key.isValid() &amp;&amp; key.isConnectable()) &#123; System.out.println(&quot;isConnectable&quot;); //TODO isConnectable后，应该移除 OP_CONNECT 事件， // 否则在finishConnect成功后，select() 将一直返回0 ，CPU空转 // 在socketChannel.register(selector, SelectionKey.OP_READ)这里就回会把OP_CONNECT事件去掉，所以这里暂时不需要// key.interestOps(key.interestOps() &amp; ~SelectionKey.OP_CONNECT); SocketChannel socketChannel = (SocketChannel) key.channel(); boolean finishConnect = false; try &#123; finishConnect = socketChannel.finishConnect(); &#125; catch (Exception e) &#123; finishConnect = false; &#125; // 完成连接 if (finishConnect) &#123; System.out.println(&quot;已经连接到服务器&quot;); // 注册读事件 socketChannel.register(selector, SelectionKey.OP_READ); // 将断网前的数据发出 synchronized (sendBuffer)&#123; if(sendBuffer.hasRemaining() &amp;&amp; (sendBuffer.remaining()!=sendBuffer.capacity()))&#123;// 缓冲区有数据 key.interestOps(SelectionKey.OP_READ | SelectionKey.OP_WRITE);//添加写事件 &#125; &#125; &#125; else &#123; System.out.println(&quot;连接失败尝试重连。。。&quot;); // 清除旧的连接 key.cancel(); socketChannel.close(); // 等待连接安全关闭 Thread.sleep(2000); /** * 重新连接 * 因为在同一个线程中，所以仅重连，不用销毁reactor线程 */ initConnect(); &#125; &#125; // 接受来自服务器的响应 else if (key.isReadable()) &#123; System.out.println(&quot;isReadable&quot;); receive(key); &#125; // 实际上只要注册了关心写操作，这个操作就一直被激活 else if (key.isWritable()) &#123; System.out.println(&quot;isWritable&quot;); send(key); &#125; &#125; keys.clear(); &#125; &#125; &#125; catch (InterruptedException e) &#123; System.out.println(&quot;reactor线程被提前中断，正常退出&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; private static void receive(SelectionKey key) throws IOException &#123; SocketChannel socketChannel = (SocketChannel) key.channel(); int read = socketChannel.read(receiveBuffer); if (read &gt; 0) &#123; receiveBuffer.flip(); String receiveData = Charset.forName(&quot;UTF-8&quot;).decode(receiveBuffer).toString(); System.out.println(&quot;receive server message---&gt;&quot; + receiveData); receiveBuffer.clear(); &#125; // read == -1 代表服务端已经断开 if (read == -1) &#123; System.out.println(&quot;server already close.\\n&quot;); // 反注册channel key.cancel(); // 关闭socket socketChannel.close(); &#125; &#125; private static void send(SelectionKey key) throws IOException &#123; SocketChannel socketChannel = (SocketChannel) key.channel(); boolean sysFull = false; synchronized (sendBuffer)&#123; //写模式切换到读模式 ---&gt; position归零、limit为之前的position的值 sendBuffer.flip(); while (sendBuffer.hasRemaining()) &#123; int writed = socketChannel.write(sendBuffer); System.out.println(&quot;writed byte is：&quot; + writed); if(writed&lt;=0)&#123; sysFull = true; // 系统发送缓存已经满了 System.out.println(&quot;系统发送缓存已经满了&quot;); break; &#125; &#125; // 将数据移到开头 ----&gt; position=limit-position、limit为capacity sendBuffer.compact(); &#125; if(!sysFull)&#123; key.interestOps(key.interestOps() &amp; ~SelectionKey.OP_WRITE);//取消写事件 &#125; &#125; private static void listenUserInput() &#123; BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(System.in)); try &#123; String msg; while ((msg = bufferedReader.readLine()) != null) &#123; System.out.println(&quot;Thread.getAllStackTraces size:&quot; + Thread.getAllStackTraces().keySet().size()); if (!isConnected()) &#123; System.out.println(&quot;发送失败，连接已经断开，尝试重连。。。&quot;); // 把无法发送的消息进行留言，等下次连接成功时一并发出 synchronized (sendBuffer) &#123; sendBuffer.put((msg + &quot;\\n&quot;).getBytes()); &#125; if (socketChannel != null &amp;&amp; !socketChannel.isConnectionPending()) &#123;// 当前没有正在尝试连接，则主动触发重连 // 启动重连 reconnection(); &#125; else &#123; System.out.println(&quot;已经尝试重连，请勿重复触发。。。&quot;); &#125; continue; &#125; // 向发送缓冲区写入消息 synchronized (sendBuffer) &#123; sendBuffer.put((msg + &quot;\\n&quot;).getBytes()); &#125; registerKey.interestOps(SelectionKey.OP_READ | SelectionKey.OP_WRITE);//添加写事件 /* TODO 1、selector所在线程可能正阻塞在select()上， interestOps 的改变不会立即被selector感知，需要手动唤醒selector所在线程 2、另外 key.readyOps() 并不能输出实时的状态，只有select()函数返回时key.readyOps()里的值才会被更新 即：jdk不会你帮你把key.readyOps()归零，直到select() 返回 [readyOps()、selectedKeys()] 都是这样 */ selector.wakeup(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;","categories":[{"name":"NIO与网络编程","slug":"NIO与网络编程","permalink":"https://coderzc.github.io/categories/NIO%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[]},{"title":"使用Logistic 增长模型拟合感染人数 (2","slug":"机器学习/20181204-使用Logistic 增长模型拟合感染人数 (2.11 日更新)","date":"2018-12-03T16:00:00.000Z","updated":"2021-08-18T02:58:32.969Z","comments":true,"path":"2018/12/04/机器学习/20181204-使用Logistic 增长模型拟合感染人数 (2.11 日更新)/","link":"","permalink":"https://coderzc.github.io/2018/12/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/20181204-%E4%BD%BF%E7%94%A8Logistic%20%E5%A2%9E%E9%95%BF%E6%A8%A1%E5%9E%8B%E6%8B%9F%E5%90%88%E6%84%9F%E6%9F%93%E4%BA%BA%E6%95%B0%20(2.11%20%E6%97%A5%E6%9B%B4%E6%96%B0)/","excerpt":"最近武汉肺炎，牵动着全国人民的心，大家可能都想知道疫情什么时候才能结束，今天我使用2.2之前卫健委发布的确诊人数，按照logistic增长模型拟合了一条曲线并大概预测一下之后的疫情情况，模型的结果仅供参考。 本文基于邢翔瑞博主的文章编写，在此感谢翔瑞作者。原文地址：https://blog.csdn.net/weixin_36474809/article/details/104101055 什么是Logistic增长曲线： Logistic函数或Logistic曲线是一种常见的S形函数，它是皮埃尔·弗朗索瓦·韦吕勒在1844或1845年在研究它与人口增长的关系时命名的。广义Logistic","text":"最近武汉肺炎，牵动着全国人民的心，大家可能都想知道疫情什么时候才能结束，今天我使用2.2之前卫健委发布的确诊人数，按照logistic增长模型拟合了一条曲线并大概预测一下之后的疫情情况，模型的结果仅供参考。 本文基于邢翔瑞博主的文章编写，在此感谢翔瑞作者。原文地址：https://blog.csdn.net/weixin_36474809/article/details/104101055 什么是Logistic增长曲线：Logistic函数或Logistic曲线是一种常见的S形函数，它是皮埃尔·弗朗索瓦·韦吕勒在1844或1845年在研究它与人口增长的关系时命名的。广义Logistic曲线可以模仿一些情况人口增长（P）的S形曲线。起初阶段大致是指数增长；然后随着开始变得饱和，增加变慢；最后，达到成熟时增加停止。当一个物种迁入到一个新生态系统中后，其数量会发生变化。假设该物种的起始数量小于环境的最大容纳量，则数量会增长。该物种在此生态系统中有天敌、食物、空间等资源也不足（非理想环境），则增长函数满足逻辑斯谛方程，图像呈S形，此方程是描述在资源有限的条件下种群增长规律的一个最佳数学模型。在以下内容中将具体介绍逻辑斯谛方程的原理、生态学意义及其应用。 Logistic方程,即常微分方程: 而将上面的方程解出来，可以得到logistic函数：其中为P0初始值，K为终值，r衡量曲线变化快慢，t为时间。 编程实现：接下来我们就用python来拟合这个曲线。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158#!/usr/bin/python# -*- coding: UTF-8 -*-&quot;&quot;&quot;拟合2019-nCov肺炎感染确诊人数https://blog.csdn.net/z_ccsdn/article/details/104134358&quot;&quot;&quot;import datetimeimport matplotlib.pyplot as pltimport numpy as npimport requestsfrom matplotlib.font_manager import FontPropertiesfrom scipy.optimize import curve_fitfrom sklearn.metrics import mean_squared_error# 引入中文字体库font = FontProperties(fname=r&quot;./simsun.ttc&quot;, size=14)sdate = Nonehyperparameters_r = Nonehyperparameters_K = None&#x27;&#x27;&#x27;从csv获取数据csv 样例：01.11 41 001.12 41 0&#x27;&#x27;&#x27;def load_data_fromcsv(file_path): data = [] with open(file_path, &#x27;r&#x27;) as file: lines = file.readlines() for line in lines: line = line.strip() if line: line_array = line.split(&#x27; &#x27;) if len(line_array) == 3: data.append( &#123;&#x27;date&#x27;: line_array[0], &#x27;confirm&#x27;: line_array[1], &#x27;suspect&#x27;: line_array[2]&#125;) data.sort(key=lambda x: x[&quot;date&quot;]) date_temple = &#x27;%Y.%m.%d&#x27; # 获取首次出现感染人数的日期 global sdate sdate = datetime.datetime.strptime( &#x27;2020.&#x27; + data[0][&#x27;date&#x27;], f&#x27;&#123;date_temple&#125;&#x27;).date() x_data_history = [datetime.datetime.strptime(&#x27;2020.&#x27; + dd[&#x27;date&#x27;], f&#x27;&#123;date_temple&#125;&#x27;).date().strftime(&quot;%m-%d&quot;) for dd in data] t = [datetime.datetime.strptime( &#x27;2020.&#x27; + dd[&#x27;date&#x27;], f&#x27;&#123;date_temple&#125;&#x27;).date() for dd in data] P_confirm = [int(dd[&#x27;confirm&#x27;]) for dd in data] P_suspect = [int(dd[&#x27;suspect&#x27;]) for dd in data] return np.array(t, dtype=np.datetime64), np.array(P_confirm), np.array(P_suspect), x_data_historydef load_data(): # 拉取腾讯新闻数据 res = requests.get(&#x27;https://service-n9zsbooc-1252957949.gz.apigw.tencentcs.com/release/qq&#x27;) res_json = res.json() data = res_json[&#x27;data&#x27;][&#x27;wuwei_ww_cn_day_counts&#x27;] # 补充更早些的数据： data.append(&#123;&#x27;date&#x27;: &#x27;01.11&#x27;, &#x27;confirm&#x27;: &#x27;41&#x27;, &#x27;suspect&#x27;: &#x27;0&#x27;&#125;) data.append(&#123;&#x27;date&#x27;: &#x27;01.12&#x27;, &#x27;confirm&#x27;: &#x27;41&#x27;, &#x27;suspect&#x27;: &#x27;0&#x27;&#125;) data.sort(key=lambda x: x[&quot;date&quot;]) # 因为21号以前并非是全国数据，数据不好要去掉 data = data[10:] print(data) # 获取首次出现感染人数的日期 global sdate sdate = datetime.datetime.strptime(&#x27;2020.&#x27; + data[0][&#x27;date&#x27;], &#x27;%Y.%m/%d&#x27;).date() x_data_history = [datetime.datetime.strptime(&#x27;2020.&#x27; + dd[&#x27;date&#x27;], &#x27;%Y.%m/%d&#x27;).date().strftime(&quot;%m-%d&quot;) for dd in data] t = [datetime.datetime.strptime(&#x27;2020.&#x27; + dd[&#x27;date&#x27;], &#x27;%Y.%m/%d&#x27;).date() for dd in data] P_confirm = [int(dd[&#x27;confirm&#x27;]) for dd in data] P_suspect = [int(dd[&#x27;suspect&#x27;]) for dd in data] return np.array(t, dtype=np.datetime64), np.array(P_confirm), np.array(P_suspect), x_data_history# 计算相隔天数def day_delay(t): t0_date = np.datetime64(sdate, &#x27;D&#x27;) t_ = (t - t0_date) days = (t_ / np.timedelta64(1, &#x27;D&#x27;)).astype(int) return daysdef logistic_increase_function(t,P0): r = hyperparameters_r K = hyperparameters_K # t:time t0:initial time P0:initial_value K:capacity r:increase_rate exp_value = np.exp(r * (t)) return (K * exp_value * P0) / (K + (exp_value - 1) * P0)if __name__ == &#x27;__main__&#x27;: # 日期及感染人数 t, P_confirm, P_suspect, x_show_data = load_data() # t, P_confirm, P_suspect, x_show_data = load_data_fromcsv(&#x27;~/data.csv&#x27;) x_data, y_data = day_delay(t), P_confirm # 分隔训练测试集,将最后的30%数据作为测试集 x_train, x_test, y_train, y_test = x_data[:-1 * int(len(x_data) * 0.3)], x_data[-1 * int(len(x_data) * 0.3):], y_data[:-1 * int(len(x_data) * 0.3)],y_data[-1 * int(len(x_data) * 0.3):] print(x_train) print(x_test) popt = None mse = float(&quot;inf&quot;) r = None k = None # 网格搜索来优化r和K参数 max_k = 50000 # 限定的最大感染人数 for k_ in np.arange(20000, max_k, 1): hyperparameters_K = k_ for r_ in np.arange(0, 1, 0.01): # 用最小二乘法估计拟合 hyperparameters_r = r_ popt_, pcov_ = curve_fit(logistic_increase_function, x_train, y_train) # # 获取popt里面是拟合系数 print(&quot;K:capacity P0:initial_value r:increase_rate&quot;) print(k_, popt_, r_) # 计算均方误差对测试集进行验证 mse_ = mean_squared_error(y_test, logistic_increase_function(x_test, *popt_)) print(&quot;mse:&quot;, mse_) if mse_ &lt;= mse: mse = mse_ popt = popt_ r = r_ k = k_ hyperparameters_K = k hyperparameters_r = r print(&quot;----------------&quot;) print(&quot;hyperparameters_K:&quot;, hyperparameters_K) print(&quot;hyperparameters_r:&quot;, hyperparameters_r) print(&quot;----------------&quot;) popt, pcov = curve_fit(logistic_increase_function, x_data, y_data) print(&quot;K:capacity P0:initial_value r:increase_rate&quot;) print(hyperparameters_K, popt, hyperparameters_r) # 未来预测 date_nums = 32 #需要预测的总天数，从第一天开始算起 future = np.linspace(0, date_nums, date_nums) future = np.array(future) future_predict = logistic_increase_function(future, *popt) # 绘图 x_show_data_all = [(sdate + (datetime.timedelta(days=fu))).strftime(&quot;%m-%d&quot;) for fu in future] plt.scatter(x_show_data, P_confirm, s=35, c=&#x27;green&#x27;, marker=&#x27;.&#x27;, label=&quot;确诊人数&quot;) plt.plot(x_show_data_all, future_predict, &#x27;r-s&#x27;, marker=&#x27;+&#x27;, linewidth=1.5, label=&#x27;预测曲线&#x27;) plt.tick_params(labelsize=5) plt.xlabel(&#x27;时间&#x27;, FontProperties=font) plt.ylabel(&#x27;感染人数&#x27;, FontProperties=font) plt.xticks(x_show_data_all) plt.grid() # 显示网格 plt.legend(prop=font) # 指定legend的位置右下角 plt.show() 拟合结果：2.3日更新拟合结果：2.4 日更新拟合结果：2.5 日更新拟合结果：2.6 日更新拟合结果：2.7 日更新拟合结果：2.8 日更新拟合结果：2.11更新拟合结果：","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://coderzc.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"机器学习(ML)-入门知识","slug":"机器学习/20181204-机器学习(ML)-入门知识","date":"2018-12-03T16:00:00.000Z","updated":"2021-08-18T02:58:32.970Z","comments":true,"path":"2018/12/04/机器学习/20181204-机器学习(ML)-入门知识/","link":"","permalink":"https://coderzc.github.io/2018/12/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/20181204-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(ML)-%E5%85%A5%E9%97%A8%E7%9F%A5%E8%AF%86/","excerpt":"ML知识概括： ML常用公式 推荐学习博客：莫烦Python(有视频和图文，很大神非常推荐) https://morvanzhou.github.io/ 基本术语 * ***特征向量(feature vector)/样本(sample)***：用多个维度来描述一件事物有叫做“样本”也叫做“特征向量” * *训练(training)：从数据中学得模型得过程 * 训练样本：训练过程中使用的数据称为“训练数据”，其中每个样本称为一个“训练样本”，训练样本组成的集合称为“训练集”。 * ***模型/学习器(learner)***：可看作学习算法在给定数据和参数空间上的实例化也叫做","text":"ML知识概括： ML常用公式 推荐学习博客：莫烦Python(有视频和图文，很大神非常推荐)https://morvanzhou.github.io/ 基本术语 ***特征向量(feature vector)/样本(sample)***：用多个维度来描述一件事物有叫做“样本”也叫做“特征向量” *训练(training)：从数据中学得模型得过程 训练样本：训练过程中使用的数据称为“训练数据”，其中每个样本称为一个“训练样本”，训练样本组成的集合称为“训练集”。 ***模型/学习器(learner)***：可看作学习算法在给定数据和参数空间上的实例化也叫做 ***标记(label)***：关于样本预测结果的信息，例如：“好瓜”、“坏瓜” 称为标记，而，拥有了标记信息的样本称为样例，一般的用(xi,yi)表示第i个样例，xi为第i个特征向量，yi是xi的标记。 ***分类(classification)&amp;回归(regression)***：对于欲预测的是离散值,此类学习任务称为“分类”；若欲预测的是连续值，例如西瓜的成熟度0.95、0.37，此类学习任务称为“回归” ***二分类(binary classification)&amp;多分类(multi-class classification)***：对只涉及两个类别的的学习任务称为“二分类”任务，通常称为一个类为“正类”，另一个类为“反类”；涉及多个类别时，则称“多分类”任务。 ***测试/评估(testing)***：学得模型后，使用其进行预测的过程称为“测试” ***聚类(clustering)***：将训练集的样本分成若干组，每一组称为一簇(cluster),这些自动形成的簇可能对应一些潜在的概念划分，而且这样的学习过程中使用的训练样本通常不用于标记信息。 监督学习&amp;无监督学习 泛化(generalization) ：学得模型适用于新样本的能力，称为泛化能力 假设空间scikit-learn 中文文档http://sklearn.apachecn.org/cn/0.19.0/","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://coderzc.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"机器学习-线性模型","slug":"机器学习/20181204-机器学习-线性模型","date":"2018-12-03T16:00:00.000Z","updated":"2021-08-18T02:58:32.970Z","comments":true,"path":"2018/12/04/机器学习/20181204-机器学习-线性模型/","link":"","permalink":"https://coderzc.github.io/2018/12/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/20181204-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/","excerpt":"1. 基本形式 * 线性模型：试图学得一个通过属性的线性组合来进行预测的函数，即 用向量形式写成： 2.线性回归 对于给定数据集D={(x1,y1),(x2,y2),(x3,y3),…..,(xm,ym)},其中 x1=(x1;x2;…;xd),yi ∈ R,”线性回归”试图学得一个线性模型以尽可能准确的预测实值输出标记。 我们先考虑一种最简单的情形：输入属性数目只有一个即 线性回归试图学得： 如何确定w和b呢？显然，关键在于如何衡量f(x)与y之间的差别，之前介绍过，均方误差是回归任务常用的性能度量,因此我们试图让均方误差最小化，即","text":"1. 基本形式 线性模型：试图学得一个通过属性的线性组合来进行预测的函数，即用向量形式写成： 2.线性回归对于给定数据集D={(x1,y1),(x2,y2),(x3,y3),…..,(xm,ym)},其中 x1=(x1;x2;…;xd),yi ∈ R,”线性回归”试图学得一个线性模型以尽可能准确的预测实值输出标记。我们先考虑一种最简单的情形：输入属性数目只有一个即线性回归试图学得：如何确定w和b呢？显然，关键在于如何衡量f(x)与y之间的差别，之前介绍过，均方误差是回归任务常用的性能度量,因此我们试图让均方误差最小化，即","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://coderzc.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"机器学习利器之Numpy","slug":"机器学习/20181204-机器学习利器之Numpy","date":"2018-12-03T16:00:00.000Z","updated":"2021-08-18T02:58:32.970Z","comments":true,"path":"2018/12/04/机器学习/20181204-机器学习利器之Numpy/","link":"","permalink":"https://coderzc.github.io/2018/12/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/20181204-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%A9%E5%99%A8%E4%B9%8BNumpy/","excerpt":"PS：机器学习相关代码：https://nbviewer.jupyter.org/github/coderzc/machine_learning/tree/master/jupyter GitHub源码：https://github.com/coderzc/machine_learning 等有时间整理一下，发出来 Numpy 创建N维数组 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 4","text":"PS：机器学习相关代码：https://nbviewer.jupyter.org/github/coderzc/machine_learning/tree/master/jupyterGitHub源码：https://github.com/coderzc/machine_learning等有时间整理一下，发出来 Numpy 创建N维数组1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import numpy as np&#x27;&#x27;&#x27; 创建10行10列的数值为浮点0的矩阵 &#x27;&#x27;&#x27;&gt;&gt;&gt; print(&quot;np.zeros\\n&quot;, np.zeros([10, 10]))np.zeros [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]&#x27;&#x27;&#x27; 创建10行10列的数值为浮点1的矩阵 &#x27;&#x27;&#x27;&gt;&gt;&gt; print(&quot;np.ones\\n&quot;, np.ones([10, 10]))np.ones [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]&#x27;&#x27;&#x27; 创建10行10列的数值为浮点1的对角矩阵 &#x27;&#x27;&#x27;&gt;&gt;&gt; print(&quot;np.eye\\n&quot;, np.eye(10, 10))np.eye [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]&#x27;&#x27;&#x27; 从数值范围创建数组 开始，结束，步长，输出元素类型 &#x27;&#x27;&#x27;&gt;&gt;&gt; print(&quot;np.arange\\n&quot;, np.arange(0, 100, 2, float))np.arange [ 0. 2. 4. 6. 8. 10. 12. 14. 16. 18. 20. 22. 24. 26. 28. 30. 32. 34. 36. 38. 40. 42. 44. 46. 48. 50. 52. 54. 56. 58. 60. 62. 64. 66. 68. 70. 72. 74. 76. 78. 80. 82. 84. 86. 88. 90. 92. 94. 96. 98.]&#x27;&#x27;&#x27;生产随机数组 5行5列 范围0～1&#x27;&#x27;&#x27;&gt;&gt;&gt; np.random.rand(5, 5)array([[0.79909192, 0.40687012, 0.05833267, 0.90631693, 0.85774438], [0.65685319, 0.99620959, 0.64195711, 0.28694344, 0.54805126], [0.87347445, 0.20443748, 0.45883044, 0.90017425, 0.17487183], [0.4833086 , 0.59498315, 0.75053456, 0.93725983, 0.79870607], [0.8908418 , 0.49860926, 0.44097606, 0.53744394, 0.21089092]])&#x27;&#x27;&#x27; 生成在半开半闭区间 [low,high)上离散均匀分布的整数值;若high=None，则取值区间变为[0,low) ; size维度 &#x27;&#x27;&#x27;&gt;&gt;&gt; np.random.randint(4,10,size=(5, 5))array([[6, 6, 6, 7, 9], [4, 8, 6, 7, 7], [6, 8, 6, 5, 7], [7, 8, 8, 4, 5], [7, 6, 5, 5, 7]])&#x27;&#x27;&#x27; 给定均值/标准差/维度的正态分布 &#x27;&#x27;&#x27;&gt;&gt;&gt; np.random.normal(1.75, 0.1, (3, 4))array([[1.83246388, 1.73186179, 1.78198763, 1.76844117], [1.69089184, 1.69620751, 1.78018062, 1.68086896], [1.86462936, 1.61972878, 1.95645574, 1.66104741]])&#x27;&#x27;&#x27; 将列表转换为np数组 &#x27;&#x27;&#x27;&gt;&gt;&gt; array = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]&gt;&gt;&gt; np_array = np.array(array, dtype=float) # copy,新数组&gt;&gt;&gt; print(&quot;np.array:\\n&quot;, np_array)np.array: [[ 1. 2. 3. 4.] [ 5. 6. 7. 8.] [ 9. 10. 11. 12.]]&gt;&gt;&gt; np_array2 = np.asarray(array, dtype=float) # view,会改变原数组&gt;&gt;&gt; print(&quot;np.asarray:\\n&quot;, np_array2)np.asarray: [[ 1. 2. 3. 4.] [ 5. 6. 7. 8.] [ 9. 10. 11. 12.]] 查看数组属性123456789101112131415# 数组元素个数&gt;&gt;&gt; print(&quot;数组元素个数 size:&quot;, np_array.size)数组元素个数 size: 12# 数组形状&gt;&gt;&gt; print(&quot;数组形状 shape:&quot;, np_array.shape)数组形状 shape: (3, 4)# 数组维度&gt;&gt;&gt; print(&quot;数组维度 ndim:&quot;, np_array.ndim)数组维度 ndim: 2# 数组元素类型&gt;&gt;&gt; print(&quot;数组元素类型 dtype:&quot;, np_array.dtype)数组元素类型 dtype: float64# 数组中每个元素的字节大小&gt;&gt;&gt; print(&quot;数组元素类型 itemsize:&quot;, np_array.itemsize)数组元素类型 itemsize: 8 shape操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&gt;&gt;&gt; array = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]&gt;&gt;&gt; n1 = np.asarray(array)# 改变数组的格式&gt;&gt;&gt; n2 = n1.reshape(6, 2)&gt;&gt;&gt; print(n1)[[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]]&gt;&gt;&gt; print(n2)[[ 1 2] [ 3 4] [ 5 6] [ 7 8] [ 9 10] [11 12]]# 将多维降到1维展开&gt;&gt;&gt; print(&quot;flatten():&quot;, n2.flatten()) # copy,新数组flatten(): [ 1 2 3 4 5 6 7 8 9 10 11 12]&gt;&gt;&gt; print(&quot;ravel():&quot;, n2.ravel()) # view，会改变原数组,却不会改变shaperavel(): [ 1 2 3 4 5 6 7 8 9 10 11 12]# 转置&gt;&gt;&gt; n3 = np.arange(12)&gt;&gt;&gt; n3 = n3.reshape(3, 4)&gt;&gt;&gt; print(&quot;n3:&quot;, n3)n3: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]&gt;&gt;&gt; print(&quot;n3.T:&quot;, n3.T)n3.T: [[ 0 4 8] [ 1 5 9] [ 2 6 10] [ 3 7 11]]# reshape一些特殊值&gt;&gt;&gt; n4 = np.arange(10, 130, 10)&gt;&gt;&gt; print(&quot;n4:&quot;, n4.reshape(4, 3))n4: [[ 10 20 30] [ 40 50 60] [ 70 80 90] [100 110 120]]# -1 一维展开 与 ravel()作用相似&gt;&gt;&gt; print(n4.reshape(-1))[ 10 20 30 40 50 60 70 80 90 100 110 120]# (-1,1) n行，1列&gt;&gt;&gt; print(n4.reshape(-1, 1))[[ 10] [ 20] [ 30] [ 40] [ 50] [ 60] [ 70] [ 80] [ 90] [100] [110] [120]]# (1,-1) 1行，n列但任然是二维矩阵&gt;&gt;&gt; print(n4.reshape(1, -1))[[ 10 20 30 40 50 60 70 80 90 100 110 120]] 数组索引和迭代12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&gt;&gt;&gt; print(&#x27;\\n\\n&#x27;)&gt;&gt;&gt; n5 = np.arange(30)&gt;&gt;&gt; print(&#x27;n5:&#x27;, n5)n5: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]# 获取第一个元素&gt;&gt;&gt; print(n5[0])0# 获取倒数第一个元素&gt;&gt;&gt; print(n5[-1])29# 取前十个数&gt;&gt;&gt; print(n5[:10])[0 1 2 3 4 5 6 7 8 9]# 取后十个数&gt;&gt;&gt; print(n5[-10:])[20 21 22 23 24 25 26 27 28 29]# 取前11-20个数，左闭右开&gt;&gt;&gt; print(n5[10:20])[10 11 12 13 14 15 16 17 18 19]# 前十个数中，每2个数取一个&gt;&gt;&gt; print(n5[:10:2])[0 2 4 6 8]# 第6-15个数中，每3个数取一个&gt;&gt;&gt; print(n5[5:15:3])[ 5 8 11 14]# 所有的数中，每10个数取一个&gt;&gt;&gt; print(n5[::10])[ 0 10 20]# 什么都不写，可以原样复制一个数组&gt;&gt;&gt; print(n5[:])[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]############### 多维数组索引与切片 ###############&gt;&gt;&gt; n6 = n5.reshape(5, 6)&gt;&gt;&gt; print(&#x27;n6:&#x27;, n6)n6: [[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23] [24 25 26 27 28 29]]# 索引第二行第三列的元素&gt;&gt;&gt; print(&#x27;n6[1, 2]:&#x27;, n6[1, 2])n6[1, 2]: 8# 在第一维取前两行，第二维每+2取一个元素&gt;&gt;&gt; print(&#x27;n6[:2, ::2]:\\n&#x27;, n6[:2, ::2])n6[:2, ::2]: [[ 0 2 4] [ 6 8 10]]# 取第一列&gt;&gt;&gt; print(&#x27;n6[:, 0]]:\\n&#x27;, n6[:, 0])n6[:, 0]]: [ 0 6 12 18 24]# 取第2、3列&gt;&gt;&gt; print(&#x27;n6[:, 3:5]]:\\n&#x27;, n6[:, 3:5])n6[:, 3:5]]: [[ 3 4] [ 9 10] [15 16] [21 22] [27 28]] 拼接、分割1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&gt;&gt;&gt; A=np.arange(10,20).reshape(5,2)&gt;&gt;&gt; print(A)[[10 11] [12 13] [14 15] [16 17] [18 19]]&gt;&gt;&gt; B=np.arange(20,30).reshape(5,2)&gt;&gt;&gt; print(B)[[20 21] [22 23] [24 25] [26 27] [28 29]]# 垂直拼接&gt;&gt;&gt; C=np.vstack([A,B]) #C=np.r_[A,B]&gt;&gt;&gt; print(C)[[10 11] [12 13] [14 15] [16 17] [18 19] [20 21] [22 23] [24 25] [26 27] [28 29]]# 自我堆叠&gt;&gt;&gt; v=np.asarray([1,2])&gt;&gt;&gt; a=np.vstack([v]*2)&gt;&gt;&gt; print(a)[[1 2] [1 2]]# 横向堆叠两次，纵向堆叠一次&gt;&gt;&gt; b=np.tile(v,(2,1))&gt;&gt;&gt; print(b)[[1 2] [1 2]]# 水平拼接&gt;&gt;&gt; C=np.hstack([A,B]) #C=np.c_[A,B]&gt;&gt;&gt; print(C)[[10 11 20 21] [12 13 22 23] [14 15 24 25] [16 17 26 27] [18 19 28 29]]#列组合column_stack([A,B]) 一维数组：按列方向组合 二维数组：同hstack一样&gt;&gt;&gt; A=np.arange(10,20)&gt;&gt;&gt; print(A)[10 11 12 13 14 15 16 17 18 19]#行组合row_stack([A,B]) 一维数组：按行方向组合 二维数组：同vstack一样 基础运算123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&gt;&gt;&gt; n7 = np.asarray([10, 20, 30,40])&gt;&gt;&gt; n8 = np.arange(4)&gt;&gt;&gt; print(n7)[10 20 30 40]&gt;&gt;&gt; print(n8)[0 1 2 3]# 计算立方&gt;&gt;&gt; print(n7**3)[ 1000 8000 27000 64000]# 三角函数&gt;&gt;&gt; print(np.sin(n7))[-0.54402111 0.91294525 -0.98803162 0.74511316]# 指定轴最大/小值&gt;&gt;&gt; print(np.amax(n7, axis=0))40&gt;&gt;&gt; print(np.amin(n7, axis=0))10# 平均值&gt;&gt;&gt; print(np.mean(n7, axis=0))25.0# 中位数&gt;&gt;&gt; print(np.median(n7))25.0# 方差&gt;&gt;&gt; print(n7.var())125.0# 标准差&gt;&gt;&gt; print(np.std(n7, axis=0))11.180339887498949# 差值&gt;&gt;&gt; print(&quot;n7-n8:&quot;,n7-n8)n7-n8: [10 19 28 37]# 逐个相乘非矩阵乘法&gt;&gt;&gt; n9 = np.asarray([[1,1],[0,1]])&gt;&gt;&gt; print(n9)[[1 1] [0 1]]&gt;&gt;&gt; n10=np.arange(4).reshape((2,2))&gt;&gt;&gt; print(n10)[[0 1] [2 3]]&gt;&gt;&gt; print(n9 * n10) [[0 1] [0 3]] 矩阵计算12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# Ax=B 求解x&gt;&gt;&gt; A = np.array([[2, 1, -2], [3, 0, 1], [1, 1, -1]])&gt;&gt;&gt; B = np.transpose(np.array([[-3, 5, -2]]))&gt;&gt;&gt; x = np.linalg.solve(A, B)&gt;&gt;&gt; print(&#x27;x:\\n&#x27;, x)x: [[ 1.] [-1.] [ 2.]]# 矩阵相乘 C=AB 求解C&gt;&gt;&gt; A = np.array([[3, 2, -2], [3, 1, 4], [3, 1, -2]])&gt;&gt;&gt; B = np.arange(9).reshape((3,3))&gt;&gt;&gt; C = np.dot(A, B)&gt;&gt;&gt; print(&#x27;C:\\n&#x27;,C)C: [[-6 -3 0] [27 35 43] [-9 -7 -5]]# 矩阵乘向量&gt;&gt;&gt; v=np.asarray([1,2])&gt;&gt;&gt; print(v)[1 2]&gt;&gt;&gt; A=np.arange(1,5).reshape(2,2)&gt;&gt;&gt; print(A)[[1 2] [3 4]]&gt;&gt;&gt; D=v.dot(A)&gt;&gt;&gt; print(D)[ 7 10]# 自动将v转换为列向量，结果有自动转化为行向量&gt;&gt;&gt; C=A.dot(v)&gt;&gt;&gt; print(C)[ 5 11]# 矩阵的逆&gt;&gt;&gt; A=np.arange(1,5).reshape(2,2)&gt;&gt;&gt; print(A)[[1 2] [3 4]]&gt;&gt;&gt; invA=np.linalg.inv(A)&gt;&gt;&gt; print(invA)[[-2. 1. ] [ 1.5 -0.5]]# 矩阵乘以矩阵的逆等于单位矩阵对角线都为1,其他为0，这里有浮点误差&gt;&gt;&gt; print(A.dot(invA))[[1.00000000e+00 1.11022302e-16] [0.00000000e+00 1.00000000e+00]]# 对于非方阵求伪逆矩阵&gt;&gt;&gt; A=np.arange(1,11).reshape(2,5)&gt;&gt;&gt; print(A)[[ 1 2 3 4 5] [ 6 7 8 9 10]]&gt;&gt;&gt; pinvA=np.linalg.pinv(A)&gt;&gt;&gt; print(pinvA)[[-0.36 0.16] [-0.2 0.1 ] [-0.04 0.04] [ 0.12 -0.02] [ 0.28 -0.08]]&gt;&gt;&gt; print(A.dot(pinvA))[[ 1.00000000e+00 -1.11022302e-16] [-8.88178420e-16 1.00000000e+00]] 排序与arg12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; x=np.asarray([2,6,7,1,4,5,8,3,10,9])&gt;&gt;&gt; print(x)[ 2 6 7 1 4 5 8 3 10 9]# argxxx 索引函数&gt;&gt;&gt; print(np.argmax(x)) #最大数的索引为88&gt;&gt;&gt; print(np.argmin(x))3# 排序&gt;&gt;&gt; print(np.sort(x))[ 1 2 3 4 5 6 7 8 9 10]# x原地排序&gt;&gt;&gt; x.sort()&gt;&gt;&gt; print(x)[ 1 2 3 4 5 6 7 8 9 10]#打乱顺序&gt;&gt;&gt; np.random.shuffle(x)&gt;&gt;&gt; print(x)[ 9 3 5 6 2 1 8 4 7 10]# 返回排序索引&gt;&gt;&gt; print(np.argsort(x))[5 4 1 7 2 3 8 6 0 9]# 划分大于3和小于3 （快排子过程）&gt;&gt;&gt; print(np.partition(x,3))[ 2 1 3 4 5 6 8 7 9 10] FancyIndexing与np比较123456789101112131415161718192021222324252627282930313233343536373839&gt;&gt;&gt; x=np.arange(16)&gt;&gt;&gt; print(x)[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]# FancyIndexing&gt;&gt;&gt; a=[3,5,8]&gt;&gt;&gt; print(x[a])[3 5 8]# np数组比较&gt;&gt;&gt; x&lt;3array([ True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False])&gt;&gt;&gt; 2*x==24-4*xarray([False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False])# 小于等于3的元素Ture/False序列&gt;&gt;&gt; i=(x&lt;=3)&gt;&gt;&gt; print(i)[ True True True True False False False False False False False False False False False False]&gt;&gt;&gt; print(x[i])[0 1 2 3]&gt;&gt;&gt; np.sum(i) # 对值Ture累加记数4# 是否含有零元素&gt;&gt;&gt; np.any(x==0)True# 是否都等于零&gt;&gt;&gt; np.all(x==0)False# 判断两个数组是否相等&gt;&gt;&gt; np.all(x==x)True 读取数据1234567891011121314&gt;&gt;&gt; np.genfromtxt(&quot;http://aima.cs.berkeley.edu/data/iris.csv&quot;, delimiter=&quot;,&quot;,skip_header=0,dtype=&quot;f8,f8,f,i4,|S8&quot;)array([(5.1, 3.5, 1.4, 0, b&#x27;setosa&#x27;), (4.9, 3. , 1.4, 0, b&#x27;setosa&#x27;), (4.7, 3.2, 1.3, 0, b&#x27;setosa&#x27;), (4.6, 3.1, 1.5, 0, b&#x27;setosa&#x27;), (5. , 3.6, 1.4, 0, b&#x27;setosa&#x27;), (5.4, 3.9, 1.7, 0, b&#x27;setosa&#x27;), (4.6, 3.4, 1.4, 0, b&#x27;setosa&#x27;), (5. , 3.4, 1.5, 0, b&#x27;setosa&#x27;), (4.4, 2.9, 1.4, 0, b&#x27;setosa&#x27;), (4.9, 3.1, 1.5, 0, b&#x27;setosa&#x27;), (5.4, 3.7, 1.5, 0, b&#x27;setosa&#x27;), (4.8, 3.4, 1.6, 0, b&#x27;setosa&#x27;), ...... ...... (5.8, 2.7, 5.1, 1, b&#x27;virginic&#x27;), (6.8, 3.2, 5.9, 2, b&#x27;virginic&#x27;), (6.7, 3.3, 5.7, 2, b&#x27;virginic&#x27;), (6.7, 3. , 5.2, 2, b&#x27;virginic&#x27;), (6.3, 2.5, 5. , 1, b&#x27;virginic&#x27;), (6.5, 3. , 5.2, 2, b&#x27;virginic&#x27;), (6.2, 3.4, 5.4, 2, b&#x27;virginic&#x27;), (5.9, 3. , 5.1, 1, b&#x27;virginic&#x27;)], dtype=[(&#x27;f0&#x27;, &#x27;&lt;f8&#x27;), (&#x27;f1&#x27;, &#x27;&lt;f8&#x27;), (&#x27;f2&#x27;, &#x27;&lt;f8&#x27;), (&#x27;f3&#x27;, &#x27;&lt;i4&#x27;), (&#x27;f4&#x27;, &#x27;S8&#x27;)])","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://coderzc.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"深度学习概述","slug":"机器学习/20181204-深度学习概述","date":"2018-12-03T16:00:00.000Z","updated":"2021-08-18T02:58:32.970Z","comments":true,"path":"2018/12/04/机器学习/20181204-深度学习概述/","link":"","permalink":"https://coderzc.github.io/2018/12/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/20181204-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/","excerpt":"提出的原因，以前的机器学习方法不能很好解决图像识别的问题 难点：视角、光照、形变、遮挡、 背景混淆、 区分度小、 图片尺寸 学习多层特征？ * ImageNet 分类图库 * LSVRC 比赛","text":"提出的原因，以前的机器学习方法不能很好解决图像识别的问题难点：视角、光照、形变、遮挡、 背景混淆、 区分度小、 图片尺寸 学习多层特征？ ImageNet 分类图库 LSVRC 比赛","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://coderzc.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"神经网络","slug":"机器学习/20181204-神经网络","date":"2018-12-03T16:00:00.000Z","updated":"2021-08-18T02:58:32.970Z","comments":true,"path":"2018/12/04/机器学习/20181204-神经网络/","link":"","permalink":"https://coderzc.github.io/2018/12/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/20181204-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://coderzc.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"ubList()-的坑","slug":"杂记/20181116subList()-的坑","date":"2018-11-15T16:00:00.000Z","updated":"2021-08-18T02:58:32.971Z","comments":true,"path":"2018/11/16/杂记/20181116subList()-的坑/","link":"","permalink":"https://coderzc.github.io/2018/11/16/%E6%9D%82%E8%AE%B0/20181116subList()-%E7%9A%84%E5%9D%91/","excerpt":"1.subList(l,r) 是左闭右开 例如：subList(1,3) 截取的是下标为1和2两个元素 2.subList() 返回对象是RandomAccessSubList不可序列化的实例化 https://stackoverflow.com/questions/26568205/resolve-a-java-util-arraylistsublist-notserializable-exception 3.切记不要这么写：list = (LinkedList) list.subList(0, 2); 否则程序会这样报复你：java.util.SubList cannot be cas","text":"1.subList(l,r) 是左闭右开 例如：subList(1,3) 截取的是下标为1和2两个元素2.subList() 返回对象是RandomAccessSubList不可序列化的实例化https://stackoverflow.com/questions/26568205/resolve-a-java-util-arraylistsublist-notserializable-exception 3.切记不要这么写：list = (LinkedList) list.subList(0, 2);否则程序会这样报复你：java.util.SubList cannot be cast to java.util.LinkedList向下转型的前提是你要先是那个类 总结：list = new ArrayList(list.subList(0, 2)); 这么写总没错啦～","categories":[{"name":"杂记","slug":"杂记","permalink":"https://coderzc.github.io/categories/%E6%9D%82%E8%AE%B0/"}],"tags":[]},{"title":"常用Linux命令","slug":"Linux/20181113-常用Linux命令","date":"2018-11-12T16:00:00.000Z","updated":"2021-08-18T02:58:32.965Z","comments":true,"path":"2018/11/13/Linux/20181113-常用Linux命令/","link":"","permalink":"https://coderzc.github.io/2018/11/13/Linux/20181113-%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4/","excerpt":"服务器常用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ##### 统计含有空指针异常文件数 find ~/ -name \"*.log\" | xargs grep \"NullPointerException\" -l | wc -l ##### 统计含有空指针异常行数 find ~/ -name \"*.log\" | xargs grep \"NullPointerException\" | wc -l ##","text":"服务器常用1234567891011121314151617181920212223242526272829303132333435363738394041##### 统计含有空指针异常文件数find ~/ -name &quot;*.log&quot; | xargs grep &quot;NullPointerException&quot; -l | wc -l##### 统计含有空指针异常行数find ~/ -name &quot;*.log&quot; | xargs grep &quot;NullPointerException&quot; | wc -l##### 搜索含有空指针异常的行，并高亮find ~/ -name &quot;*.log&quot; | xargs grep &quot;NullPointerException&quot; --color##### 不递归子目录搜索（深度为 1）find ~/ -maxdepth 1 -name &quot;.*&quot;##### 查看进程相应信息ps -ef | grep kafka##### 查看端口对应的进程号lsof -i:9092##### 查看最后1000行数据tail -n 1000 error.log##### 直接讲GBK换为utf-8并输出cat error.log | iconv -f GBK -t UTF-8##### 统计8080端口实时连接并发数netstat -na|grep ESTAB|grep 8080 | wc -l##### 通过端口号查出进程，在通过awk，拿到进程号，最后通过jmap查询对应的堆信息jmap -heap $(lsof -i:8080 | awk &#x27;&#123;if(NR==2) print $2&#125;&#x27;)##### 逐行读取1.txt内容 转化为：File--&gt;&#123;系统时间&#125;--&gt;&#123;原值+1&#125; 写入2.txt中##### &gt; 会重写文件，如果文件里面有内容会覆盖；&gt;&gt; 追加内容到文件,不会覆盖##### 双引号会识别$等保留字，单引号则不会；echo &gt; 1.txt 意思是清空 1.txt内容for i in $(cat 1.txt)doecho &quot;File--&gt;`date`--&gt;$(($i+1))&quot; &gt;&gt; 2.txtdone##### grep 使用正则提取网址grep -ohr -E &quot;https?://[a-zA-Z0-9\\.\\/_&amp;=@$%~?#-]*&quot; 1.txt 文件合并、去重、拆分12345678910111213141516171819202122232425262728293031323334353637383940414243##### 1. 两个文件合并 (一个文件在上，一个文件在下) cat file1 file2 &gt; file3##### 2. 两个文件合并 (一个文件在左，一个文件在右) paste file1 file2 &gt; file3##### 3. 归并连续出现的重复行uniq file3 &gt; file4##### 4. 输出仅连续出现一次的行列。uniq -u file3 &gt; file4##### 5. 对文本按ASCII 码 正序列排列(-r 逆序 -u相当于sort file3|uniq)sort -ru file3 &gt; file4##### 6.保留原有顺序去重(awk 大法好)awk &#x27;!a[$0]++&#x27; file3 &gt; file4##### 7.合并file1和file2 在去重后写入file3cat file1 file2| awk &#x27;!a[$0]++&#x27; &gt; file3##### 8.以2行为单位分割file文件 生成的子文件前缀为split1_split -2 file split1_##### 9.以10个字节为单位分割file文件 生成的子文件前缀为split2_split -b 10 file split2_##### 10.以10个字节为单位分割file文件 生成的子文件前缀为split3_ ,但-C参数会尽量保持每行的完整性，##### 举例：一行有13个字节，那么会切割成两个文件，一个10字节，一个3字节，而-b参数会将8字节累计到下一行凑足十字节再切split -C 10 file split3_##### 去掉空行cat 1.txt |tr -s &#x27;\\n&#x27;##### 并集sort 1.txt 2.txt | uniq##### 交集grep -F -f 1.txt 2.txt | sort | uniq##### 差集grep -F -v -f 2.txt 1.txt | sort | uniq 12# 查看java版本/usr/libexec/java_home -V 解释下 awk ‘!a[$0]++’ file：https://blog.csdn.net/zhang_red/article/details/8585457 12# 使用curl 分析请求时间curl -o /dev/null --connect-timeout 5 --max-time 10 -s -w &quot;http_code=%&#123;http_code&#125;,time_namelookup=%&#123;time_namelookup&#125;,time_connect=%&#123;time_connect&#125;,time_appconnect=%&#123;time_appconnect&#125;,time_redirect=%&#123;time_redirect&#125;,num_redirects=%&#123;num_redirects&#125;,time_pretransfer=%&#123;time_pretransfer&#125;,time_starttransfer=%&#123;time_starttransfer&#125;,time_total=%&#123;time_total&#125;,size_download=%&#123;size_download&#125;,speed_download=%&#123;speed_download&#125;\\n&quot; &#x27;https://www.baidu.com&#x27; 12# 根据每个文件第一行第一列 给文件排序find ./ -maxdepth 1 -name &quot;*.txt&quot; -exec awk -v filename=&#123;&#125; &#x27;NR==1&#123;print filename,$1&#125;&#x27; &#123;&#125; \\; | sort -k2 -n 12# 统计第二列（以空格为分隔符）含有 &quot;B&quot; 的行数cut -f 2 -d &quot; &quot; ./1.txt | grep &quot;B&quot; | wc -l","categories":[{"name":"Linux","slug":"Linux","permalink":"https://coderzc.github.io/categories/Linux/"}],"tags":[]},{"title":"Python-数据类型","slug":"杂记/20181113-Python-数据类型","date":"2018-11-12T16:00:00.000Z","updated":"2021-08-18T02:58:32.971Z","comments":true,"path":"2018/11/13/杂记/20181113-Python-数据类型/","link":"","permalink":"https://coderzc.github.io/2018/11/13/%E6%9D%82%E8%AE%B0/20181113-Python-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","excerpt":"1.数字类型(number) * int * float 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 >>> type(1) int >>> type(1.0) float >>> type(1+0.1) float >>> type(1+1.0) float >>> type(1/2) float >>>","text":"1.数字类型(number) int float 1234567891011121314151617&gt;&gt;&gt; type(1)int&gt;&gt;&gt; type(1.0)float&gt;&gt;&gt; type(1+0.1)float&gt;&gt;&gt; type(1+1.0)float&gt;&gt;&gt; type(1/2)float&gt;&gt;&gt; type(1//2)int / 是浮点运算// 是取整运算 进制转换 123456789101112131415161718192021222324252627#二进制&gt;&gt;&gt; 0b10 2#八进制&gt;&gt;&gt; 0o108#十六进制&gt;&gt;&gt; 0x1016#转化为二进制&gt;&gt;&gt; bin(10)&#x27;0b1010&#x27;#转化为十进制&gt;&gt;&gt; int(0b111)7#转化为八进制&gt;&gt;&gt; oct(0b111)&#x27;0o7&#x27;#转化为十六进制&gt;&gt;&gt; hex(0o7777)&#x27;0xfff&#x27; bool 布尔类型True False 注意大小写 1234567891011121314151617181920212223242526272829303132333435&gt;&gt;&gt; type(True)bool&gt;&gt;&gt; int(True)1&gt;&gt;&gt; int(False)0&gt;&gt;&gt; bool(1)True&gt;&gt;&gt; bool(0)False&gt;&gt;&gt; bool(2.2) # 非零就是TrueTrue&gt;&gt;&gt; bool(-1.1)True&gt;&gt;&gt; bool(0) False&gt;&gt;&gt; bool(&#x27;&#x27;)False&gt;&gt;&gt; bool([1,2,3])True&gt;&gt;&gt; bool([])False&gt;&gt;&gt; bool(&#123;1,2,3&#125;)True&gt;&gt;&gt; bool(&#123;&#125;)False&gt;&gt;&gt; bool(None)False 复数36j 2.字符串类型(string)123456789101112131415161718192021&gt;&gt;&gt; type(&#x27;1&#x27;)str&gt;&gt;&gt;&quot;let&#x27;s go&quot;&quot;let&#x27;s go&quot;# 多行文本&gt;&gt;&gt; &#x27;&#x27;&#x27;... swdwd... dwd... dwd... &#x27;&#x27;&#x27;&#x27;\\nswdwd\\ndwd\\ndwd\\n&#x27;&gt;&gt;&gt;&#x27;helo\\world&#x27;&#x27;helloworld&#x27;#原始字符串，不解析转移字符串&gt;&gt;&gt; print(r&#x27;c:\\northwind\\northwest&#x27;)c:\\northwind\\northwest 2.2字符串运算123456789101112131415161718192021&gt;&gt;&gt; &#x27;hello&#x27;+&#x27;world&#x27;&#x27;helloworld&#x27;&gt;&gt;&gt; &quot;hello&quot;*3&#x27;hellohellohello&#x27;&gt;&gt;&gt; &#x27;hello world&#x27;[0]&#x27;h&#x27;#倒数选取&gt;&gt;&gt; &#x27;hello world&#x27;[-1]&#x27;d&#x27;&gt;&gt;&gt; &#x27;hello world&#x27;[0:5]&#x27;hello&#x27;&gt;&gt;&gt; &#x27;hello world&#x27;[6:-1]&#x27;worl&#x27;&gt;&gt;&gt; &#x27;hello world&#x27;[6:]&#x27;world&#x27;","categories":[{"name":"杂记","slug":"杂记","permalink":"https://coderzc.github.io/categories/%E6%9D%82%E8%AE%B0/"}],"tags":[]},{"title":"Autowired-与@Resource","slug":"杂记/20181016-@Autowired-与@Resource","date":"2018-10-15T16:00:00.000Z","updated":"2021-08-18T02:58:32.971Z","comments":true,"path":"2018/10/16/杂记/20181016-@Autowired-与@Resource/","link":"","permalink":"https://coderzc.github.io/2018/10/16/%E6%9D%82%E8%AE%B0/20181016-@Autowired-%E4%B8%8E@Resource/","excerpt":"@Autowired 与@Resource的区别 * @Autowired与@Resource都可以用来装配bean. 都可以写在字段上,或写在setter方法上。 * @Autowired默认按类型装配（这个注解是属业spring的），默认情况下必须要求依赖对象必须存在，如果要允许null值，可以设置它的required属性为false，如：@Autowired(required=false) ，如果我们想使用名称装配可以结合@Qualifier注解进行使用，如下：1 2 3 @Autowired @Qualifier(\"myServiceImp","text":"@Autowired 与@Resource的区别 @Autowired与@Resource都可以用来装配bean. 都可以写在字段上,或写在setter方法上。 @Autowired默认按类型装配（这个注解是属业spring的），默认情况下必须要求依赖对象必须存在，如果要允许null值，可以设置它的required属性为false，如：@Autowired(required=false) ，如果我们想使用名称装配可以结合@Qualifier注解进行使用，如下：123@Autowired@Qualifier(&quot;myServiceImpl&quot;)private MyService myservice; @Resource（这个注解属于J2EE的），默认按照名称进行装配，名称可以通过name属性进行指定，如果没有指定name属性，当注解写在字段上时，默认取字段名进行安装名称查找，如果注解写在setter方法上默认取属性名进行装配。当找不到与名称匹配的bean时才按照类型进行装配。但是需要注意的是，如果name属性一旦指定，就只会按照名称进行装配。 @Autowired是根据类型进行自动装配的。如果当Spring上下文中存在不止一个MyService类型的bean时，就会抛出BeanCreationException异常;如果Spring上下文中不存在MyService类型的bean，也会抛出BeanCreationException异常。我们可以使用@Qualifier配合@Autowired来解决这些问题。 新补充：原本以为之前的理解已经完整了，直到我发现有一种特殊情况@Autowired也能正常注入，就是有一个实现类指定id为”myService”。所以应该是：@Autowired也是先按照name 装配，其实和@Resource一样，只不过@Autowired只能取字段名进行装配，不能单独指定要装配的name而已 Spring对Bean的name默认生成规则Spring对注解形式的bean的名字的默认处理就是将类名首字母小写，再拼接后面的字符，还有另外的一个特殊处理：当类的名字是以两个或以上的大写字母开头的话，bean的名字会与类名保持一致例如：BKYInfoServcie.java——&gt;@Service(“BKYInfoServcie”)","categories":[{"name":"杂记","slug":"杂记","permalink":"https://coderzc.github.io/categories/%E6%9D%82%E8%AE%B0/"}],"tags":[]},{"title":"Java并发编程-CPU缓存","slug":"Java并发编程/20180830-Java并发编程-CPU缓存","date":"2018-08-29T16:00:00.000Z","updated":"2021-08-18T02:58:32.964Z","comments":true,"path":"2018/08/30/Java并发编程/20180830-Java并发编程-CPU缓存/","link":"","permalink":"https://coderzc.github.io/2018/08/30/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/20180830-Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-CPU%E7%BC%93%E5%AD%98/","excerpt":"一. 什么是CPU的缓存 CPU与高速缓存通过快速通道直接相连，而高速缓存和主存通过数据总线相连 CPU cache出现的原因：CPU的频率太快了。快到主存跟不上，这样在处理器时钟周期内。CPU常常需要等待主存，浪费资源。所以cache的出现，是为了缓解CPU和主存之间速度不匹配的问题 （结钩：cpu → cache → memory） CPU cache远小于主存还有什么意义： 1. 时间局部性：如果某个数据被访问，那么在不久的将来很可能再次被访问 2. 空间局部 性：如果某个数据被访问。那么与它相邻的数据很快也会被访问 二. 缓存一致性协议（MESI） 用于保证多个CPU c","text":"一. 什么是CPU的缓存CPU与高速缓存通过快速通道直接相连，而高速缓存和主存通过数据总线相连 CPU cache出现的原因：CPU的频率太快了。快到主存跟不上，这样在处理器时钟周期内。CPU常常需要等待主存，浪费资源。所以cache的出现，是为了缓解CPU和主存之间速度不匹配的问题（结钩：cpu → cache → memory） CPU cache远小于主存还有什么意义： 时间局部性：如果某个数据被访问，那么在不久的将来很可能再次被访问 空间局部 性：如果某个数据被访问。那么与它相邻的数据很快也会被访问 二. 缓存一致性协议（MESI）用于保证多个CPU cache 之间缓存共享数据的一致 三.CPU乱序执行优化（指令重排）","categories":[{"name":"Java并发编程","slug":"Java并发编程","permalink":"https://coderzc.github.io/categories/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[]},{"title":"Java并发编程-什么是高并发","slug":"Java并发编程/20180830-Java并发编程-什么是高并发","date":"2018-08-29T16:00:00.000Z","updated":"2021-08-18T02:58:32.964Z","comments":true,"path":"2018/08/30/Java并发编程/20180830-Java并发编程-什么是高并发/","link":"","permalink":"https://coderzc.github.io/2018/08/30/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/20180830-Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E4%BB%80%E4%B9%88%E6%98%AF%E9%AB%98%E5%B9%B6%E5%8F%91/","excerpt":"最近想把并发编程系统的学一下，于是参考慕课网视频边学边写博客，记录一下。 基本概念： * 并发：同时拥有两个或多个现场，如果出现在单核处理器上运行,多个线程将交替地换入或换出内存，这些线程是同时“存在”的，每个线程都处于执行过程中的某个状态。如果在多核处理器上，程序中的每个线程都将分配到一个处理器核心上，此时则成为 并行 * 高并发：高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑因素之一，它通常指，通过设计保证系统能够同时并行处理很多请求","text":"最近想把并发编程系统的学一下，于是参考慕课网视频边学边写博客，记录一下。 基本概念： 并发：同时拥有两个或多个现场，如果出现在单核处理器上运行,多个线程将交替地换入或换出内存，这些线程是同时“存在”的，每个线程都处于执行过程中的某个状态。如果在多核处理器上，程序中的每个线程都将分配到一个处理器核心上，此时则成为 并行 高并发：高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑因素之一，它通常指，通过设计保证系统能够同时并行处理很多请求","categories":[{"name":"Java并发编程","slug":"Java并发编程","permalink":"https://coderzc.github.io/categories/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[]},{"title":"Java并发编程-线程安全性","slug":"Java并发编程/20180830-Java并发编程-线程安全性","date":"2018-08-29T16:00:00.000Z","updated":"2021-08-18T02:58:32.965Z","comments":true,"path":"2018/08/30/Java并发编程/20180830-Java并发编程-线程安全性/","link":"","permalink":"https://coderzc.github.io/2018/08/30/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/20180830-Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E6%80%A7/","excerpt":"什么是线程安全性 线程安全性：当多个线程访问某个类时，不管运行时采用何种调度方式或者这些线程将被如何交替执行，并且在主调代码中不需要任何额外的同步或协同，这个类都能表现出正确的行为，那么就称这个类是线程安全的。 并发中三个特性（解决线程安全问题的主要关注点） * 原子性：提供了互斥操作，同一时刻只允许一个线程对共享资源进行操作 * 可见性：当一个线程修改了共享变量的值，其他线程可以立即得知这个修改 * 有序性：一个线程观察其他线程指令的执行顺序，由于指令重排序的存在，该观察结果一般是无序的 原子性 JDK提供了Atomic包来实现原子性（CAS） CAS（compareAndSwap","text":"什么是线程安全性线程安全性：当多个线程访问某个类时，不管运行时采用何种调度方式或者这些线程将被如何交替执行，并且在主调代码中不需要任何额外的同步或协同，这个类都能表现出正确的行为，那么就称这个类是线程安全的。 并发中三个特性（解决线程安全问题的主要关注点） 原子性：提供了互斥操作，同一时刻只允许一个线程对共享资源进行操作 可见性：当一个线程修改了共享变量的值，其他线程可以立即得知这个修改 有序性：一个线程观察其他线程指令的执行顺序，由于指令重排序的存在，该观察结果一般是无序的 原子性JDK提供了Atomic包来实现原子性（CAS） CAS（compareAndSwap）：一个原子操作有三个操作数，V为变量的内存位置，A为期望的旧职，B为要跟新的新值。CAS执行时，当且仅当v取出主内存中变量的当前值与A相等时，处理器才会用新值B去更新V的值，否在不执行更新。 CAS存在的问题：CAS虽然很高效的解决原子操作，但是CAS仍然存在三大问题。ABA问题，循环时间长开销大和只能保证一个共享变量的原子操作 ABA问题。因为CAS需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么A－B－A 就会变成1A-2B－3A。关于ABA问题参考文档: http://blog.hesey.net/2011/09/resolve-aba-by-atomicstampedreference.html 循环时间长开销大。自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 只能保证一个共享变量的原子操作。当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如有两个共享变量i＝2,j=a，合并一下ij=2a，然后用CAS来操作ij。从Java1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行CAS操作。 AtomicXXX: CAS、Unsafe.compareAndSwapInt、AtomicReference（操作对象）、AtomicIntegerFieldUpdater、LongAdder、AtomicStampedReference LongAdder在AtomicLong的基础上将单点的更新压力分散到各个节点，在低并发的时候通过对base的直接更新可以很好的保障和AtomicLong的性能基本保持一致，而在高并发的时候通过分散提高了性能。缺点是LongAdder在统计的时候如果有并发更新，可能导致统计的数据有误差。 AtomicStampedReference 解决ABA问题。这个类的compareAndSet方法作用是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 synchronized（依赖jvm）一种同步锁，修饰对象有四种 修饰this：同步范围大括号括起来的代码，作用于调用对象 修饰方法：同步范围整个方法，作用于调用对象 修饰静态方法：同步范围整个静态方法，作用于所有对象 修饰类class：同步范围大括号括起来的代码，作用于所有对象 synchronized 不会被继承 Lock （依赖特殊CPU指令，代码实现，ReentrantLock）原子性总结：synchronized: 不可中断的锁，适合竞争不激烈，代码可读性好Lock: 可中断锁，竞争激烈时能维持常态Atomic: 竞争激烈时刻维持常态，比Lock性能好；但只能同步一个值 可见性导致共享变量在线程间不可见的原因： 线程交叉执行 指令重排序结合线程交叉执行 共享变量更新后的值没有在工作内存与主存间及时更新 JMM关于synchronized 两条规则： 线程解锁前，必须把共享变量的最新值刷新到主内存 线程解锁时，将清空工作内存中共享变量的值，从而使用共享变量时需要从主内存中重新读取最新的值（注意加速解锁是同一把锁） JMM关于volatitle两条规则： 对volatile变量写操作时，会在写操作后加入一条store指令，将本地内存中的共享变量的值刷新到主内存 对volatile变量读操作时，会在读操作前加入一条load指令，从主内存中读取共享变量 有序性禁止指令重排序","categories":[{"name":"Java并发编程","slug":"Java并发编程","permalink":"https://coderzc.github.io/categories/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[]},{"title":"算法集-LeetCode-167-两数之和II---输入有序数组","slug":"算法集/20180818-算法集-LeetCode-167-两数之和II---输入有序数组","date":"2018-08-17T16:00:00.000Z","updated":"2021-08-18T02:58:32.971Z","comments":true,"path":"2018/08/18/算法集/20180818-算法集-LeetCode-167-两数之和II---输入有序数组/","link":"","permalink":"https://coderzc.github.io/2018/08/18/%E7%AE%97%E6%B3%95%E9%9B%86/20180818-%E7%AE%97%E6%B3%95%E9%9B%86-LeetCode-167-%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8CII---%E8%BE%93%E5%85%A5%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84/","excerpt":"题目描述 给定一个已按照升序排列 的有序数组，找到两个数使得它们相加之和等于目标数。 函数应该返回这两个下标值 index1 和 index2，其中 index1 必须小于 index2。 说明: 返回的下标值（index1 和 index2）不是从零开始的。 你可以假设每个输入只对应唯一的答案，而且你不可以重复使用相同的元素。 示例: 输入: numbers = [2, 7, 11, 15], target = 9 输出: [1,2] 解释: 2 与 7 之和等于目标数 9 。因此 index1 = 1, index2 = 2 。 分析 有序的查找想到二分查找，运用对撞指针的思想，前后各","text":"题目描述 给定一个已按照升序排列 的有序数组，找到两个数使得它们相加之和等于目标数。函数应该返回这两个下标值 index1 和 index2，其中 index1 必须小于 index2。说明:返回的下标值（index1 和 index2）不是从零开始的。你可以假设每个输入只对应唯一的答案，而且你不可以重复使用相同的元素。 示例:输入: numbers = [2, 7, 11, 15], target = 9输出: [1,2]解释: 2 与 7 之和等于目标数 9 。因此 index1 = 1, index2 = 2 。 分析 有序的查找想到二分查找，运用对撞指针的思想，前后各设置一个指针分别向中间移动 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 public static int binarySearch(int[] arr, int l, int k) &#123; // 在[l...r]中查找k int r = arr.length - 1; while (l &lt;= r) &#123; int middle = l + ((r - l) &gt;&gt; 1); if (k == arr[middle]) return middle; else if (k &lt; arr[middle]) r = middle - 1; else l = middle + 1; &#125; return -1;&#125;public int[] twoSum(int[] numbers, int target) &#123; int[] ret = new int[2]; for (int i = 0; i &lt; numbers.length; i++) &#123; int j = binarySearch(numbers, i + 1, target - numbers[i]); if ( j != -1) &#123; ret[0]=i+1; ret[1]=j+1; &#125; &#125; return ret;&#125;//O(n) 指针碰撞public int[] twoSum2(int[] numbers, int target) &#123; int[] ret=new int[2]; int l=0; int r=numbers.length-1; while (l&lt;r)&#123; if(numbers[l]+numbers[r]==target) &#123; ret[0]=l+1; ret[1]=r+1; return ret; &#125;else if(numbers[l]+numbers[r]&lt;target)&#123; l++; &#125;else &#123; r--; &#125; &#125; return ret;&#125;","categories":[{"name":"算法集","slug":"算法集","permalink":"https://coderzc.github.io/categories/%E7%AE%97%E6%B3%95%E9%9B%86/"}],"tags":[]},{"title":"算法集-LeetCode-215-寻找数组中第k大元素","slug":"算法集/20180818-算法集-LeetCode-215-寻找数组中第k大元素","date":"2018-08-17T16:00:00.000Z","updated":"2021-08-18T02:58:32.971Z","comments":true,"path":"2018/08/18/算法集/20180818-算法集-LeetCode-215-寻找数组中第k大元素/","link":"","permalink":"https://coderzc.github.io/2018/08/18/%E7%AE%97%E6%B3%95%E9%9B%86/20180818-%E7%AE%97%E6%B3%95%E9%9B%86-LeetCode-215-%E5%AF%BB%E6%89%BE%E6%95%B0%E7%BB%84%E4%B8%AD%E7%AC%ACk%E5%A4%A7%E5%85%83%E7%B4%A0/","excerpt":"####题目描述 Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Example 1: Input: [3,2,1,5,6,4] and k = 2 Output: 5 Example 2: Input: [3,2,3,1,2,4,5,5,6] and k = 4 Output: 4 Note: You may assume k is always vali","text":"####题目描述 Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Example 1:Input: [3,2,1,5,6,4] and k = 2Output: 5Example 2:Input: [3,2,3,1,2,4,5,5,6] and k = 4Output: 4Note:You may assume k is always valid, 1 ≤ k ≤ array’s length. ####分析 不用java的sort类库，用快排思想，Quick Select，划分为左面比p大右比p小，然后判断j与k的关系判断应该去那个子集继续查找 ####代码 123456789101112131415161718192021222324252627282930313233private static void swap(int arr[], int x, int y) &#123; int temp = arr[x]; arr[x] = arr[y]; arr[y] = temp;&#125;public int findKthLargest(int[] nums, int k) &#123; if(nums==null||nums.length&lt;k) return -1; int l=0; int r=nums.length-1; while (l&lt;=r)&#123; int p=nums[l]; int i=l+1; int j=r; while (true)&#123; while (i&lt;=r&amp;&amp;nums[i]&gt;p) i++; while (j&gt;=l+1&amp;&amp;nums[j]&lt;p) j--; if(i&gt;=j) break; else swap(nums,i++,j--); &#125; swap(nums,j,l); if(k==j+1) return p; else if(k&lt;j+1) &#123;r=j-1;&#125; else &#123;l=j+1;&#125; &#125; return -1;&#125;","categories":[{"name":"算法集","slug":"算法集","permalink":"https://coderzc.github.io/categories/%E7%AE%97%E6%B3%95%E9%9B%86/"}],"tags":[]},{"title":"算法集-leetcode-两数之和(Two-Sum)","slug":"算法集/20180818-算法集-leetcode-两数之和(Two-Sum)","date":"2018-08-17T16:00:00.000Z","updated":"2021-08-18T02:58:32.972Z","comments":true,"path":"2018/08/18/算法集/20180818-算法集-leetcode-两数之和(Two-Sum)/","link":"","permalink":"https://coderzc.github.io/2018/08/18/%E7%AE%97%E6%B3%95%E9%9B%86/20180818-%E7%AE%97%E6%B3%95%E9%9B%86-leetcode-%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C(Two-Sum)/","excerpt":"####题目描述 给定一个整数数组和一个目标值，找出数组中和为目标值的两个数。 你可以假设每个输入只对应一种答案，且同样的元素不能被重复利用。 示例: 给定 nums = [2, 7, 11, 15], target = 9 因为 nums[0] + nums[1] = 2 + 7 = 9 所以返回 [0, 1] ####分析： 这是第一题很简单，就是两层for循环然后一个一个去试，结果代码倒是通过了，可是一看时间分析比大佬的代码慢好多，于是又看了第一名的代码，果然列害，通过HashMap以值为key如果发现与存在key正好匹配的数则返回，只遍历了一遍即可。 ####代码： 1 2","text":"####题目描述 给定一个整数数组和一个目标值，找出数组中和为目标值的两个数。你可以假设每个输入只对应一种答案，且同样的元素不能被重复利用。 示例:给定 nums = [2, 7, 11, 15], target = 9因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] ####分析：这是第一题很简单，就是两层for循环然后一个一个去试，结果代码倒是通过了，可是一看时间分析比大佬的代码慢好多，于是又看了第一名的代码，果然列害，通过HashMap以值为key如果发现与存在key正好匹配的数则返回，只遍历了一遍即可。 ####代码： 123456789101112131415161718192021222324252627282930313233343536373839/** * 我的代码 */public int[] twoSum(int[] nums, int target) &#123; int[] ret = new int[2]; for (int i = 0; i &lt; nums.length; i++) &#123; for (int j = 0; j &lt; nums.length; j++) &#123; if (j == i) continue; if (nums[i] + nums[j] == target) &#123; ret[0] = i; ret[1] = j; return ret; &#125; &#125; &#125; return ret;&#125;/** * 最优算法 */public int[] twoSum2(int[] numbers, int target) &#123; int[] res = new int[2]; if (numbers == null || numbers.length &lt; 2) return res; HashMap&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); for (int i = 0; i &lt; numbers.length; i++) &#123; if (!map.containsKey(target - numbers[i])) &#123; map.put(numbers[i], i); &#125; else &#123; res[0] = map.get(target - numbers[i]); res[1] = i; break; &#125; &#125; return res;&#125;","categories":[{"name":"算法集","slug":"算法集","permalink":"https://coderzc.github.io/categories/%E7%AE%97%E6%B3%95%E9%9B%86/"}],"tags":[]},{"title":"算法集-leetcode-两数相加(Add-Two-Numbers)","slug":"算法集/20180818-算法集-leetcode-两数相加(Add-Two-Numbers)","date":"2018-08-17T16:00:00.000Z","updated":"2021-08-18T02:58:32.972Z","comments":true,"path":"2018/08/18/算法集/20180818-算法集-leetcode-两数相加(Add-Two-Numbers)/","link":"","permalink":"https://coderzc.github.io/2018/08/18/%E7%AE%97%E6%B3%95%E9%9B%86/20180818-%E7%AE%97%E6%B3%95%E9%9B%86-leetcode-%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0(Add-Two-Numbers)/","excerpt":"####题目描述： 给定两个非空链表来表示两个非负整数。位数按照逆序方式存储，它们的每个节点只存储单个数字。将两数相加返回一个新的链表。 你可以假设除了数字 0 之外，这两个数字都不会以零开头。 示例： 输入：(2 -> 4 -> 3) + (5 -> 6 -> 4) 输出：7 -> 0 -> 8 原因：342 + 465 = 807 ####分析： 这道题就是模拟加法手算过程，主要是对边界值的判断，尤其是两组不等长最后一位的处理，像999+1这种要一直进位。 ####代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21","text":"####题目描述： 给定两个非空链表来表示两个非负整数。位数按照逆序方式存储，它们的每个节点只存储单个数字。将两数相加返回一个新的链表。你可以假设除了数字 0 之外，这两个数字都不会以零开头。 示例：输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)输出：7 -&gt; 0 -&gt; 8原因：342 + 465 = 807 ####分析：这道题就是模拟加法手算过程，主要是对边界值的判断，尤其是两组不等长最后一位的处理，像999+1这种要一直进位。 ####代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode pre=new ListNode(0); ListNode head=pre; while (l1 != null &amp;&amp; l2 != null) &#123; int sum = l1.val + l2.val; int a = -1; //进位 if (sum &gt;= 10) &#123; a = 1; pre.next=new ListNode(sum % 10); pre=pre.next; &#125; else &#123; pre.next=new ListNode(sum); pre=pre.next; &#125; l1 = l1.next; l2 = l2.next; if(a==-1) continue; if (l2 == null &amp; l1 == null) &#123; pre.next=new ListNode(a); pre=pre.next; &#125; else if (l2 == null &amp;&amp; l1 != null) &#123; l1.val += a; &#125; else &#123; l2.val += a; &#125; &#125; if (l1 == null &amp;&amp; l2 != null) &#123; while (l2 != null) &#123; if(l2.val&gt;=10)&#123; int a=l2.val%10; l2.val=a; if(l2.next!=null)&#123; l2.next.val+=1; &#125;else &#123; l2.next=new ListNode(1); &#125; &#125; pre.next=new ListNode(l2.val); pre=pre.next; l2 = l2.next; &#125; &#125; if (l1 != null &amp;&amp; l2 == null) &#123; while (l1 != null) &#123; if(l1.val&gt;=10)&#123; int a=l1.val%10; l1.val=a; if(l1.next!=null)&#123; l1.next.val+=1; &#125;else &#123; l1.next=new ListNode(1); &#125; &#125; pre.next=new ListNode(l1.val); pre=pre.next; l1 = l1.next; &#125; &#125; return head.next;&#125;","categories":[{"name":"算法集","slug":"算法集","permalink":"https://coderzc.github.io/categories/%E7%AE%97%E6%B3%95%E9%9B%86/"}],"tags":[]},{"title":"算法集-初篇","slug":"算法集/20180818-算法集-初篇","date":"2018-08-17T16:00:00.000Z","updated":"2021-08-18T02:58:32.972Z","comments":true,"path":"2018/08/18/算法集/20180818-算法集-初篇/","link":"","permalink":"https://coderzc.github.io/2018/08/18/%E7%AE%97%E6%B3%95%E9%9B%86/20180818-%E7%AE%97%E6%B3%95%E9%9B%86-%E5%88%9D%E7%AF%87/","excerpt":"从今天开始我要训练算法了，用博客记录一下 对应代码的github链接：https://github.com/coderzc/ACode 题目预计来源：牛客网，剑指offer，leetcode 等等（还会不断再加入别的） 代码暂时只准备写Java实现的，以后有时间也许可能会写C++或python版，哈哈 算法集：https://www.jianshu.com/nb/28378096 加油吧，少年！！！","text":"从今天开始我要训练算法了，用博客记录一下对应代码的github链接：https://github.com/coderzc/ACode题目预计来源：牛客网，剑指offer，leetcode 等等（还会不断再加入别的）代码暂时只准备写Java实现的，以后有时间也许可能会写C++或python版，哈哈算法集：https://www.jianshu.com/nb/28378096 加油吧，少年！！！","categories":[{"name":"算法集","slug":"算法集","permalink":"https://coderzc.github.io/categories/%E7%AE%97%E6%B3%95%E9%9B%86/"}],"tags":[]},{"title":"算法集-找出缺失的数字","slug":"算法集/20180818-算法集-找出缺失的数字","date":"2018-08-17T16:00:00.000Z","updated":"2021-08-18T02:58:32.972Z","comments":true,"path":"2018/08/18/算法集/20180818-算法集-找出缺失的数字/","link":"","permalink":"https://coderzc.github.io/2018/08/18/%E7%AE%97%E6%B3%95%E9%9B%86/20180818-%E7%AE%97%E6%B3%95%E9%9B%86-%E6%89%BE%E5%87%BA%E7%BC%BA%E5%A4%B1%E7%9A%84%E6%95%B0%E5%AD%97/","excerpt":"####题目描述： 找出1~n的数字中缺失的两个数 ####代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public class FindMissNumber { public static int[] findMissNumber(int[] arr) { if (null == arr || arr.length","text":"####题目描述： 找出1~n的数字中缺失的两个数 ####代码： 1234567891011121314151617181920212223242526272829303132public class FindMissNumber &#123; public static int[] findMissNumber(int[] arr) &#123; if (null == arr || arr.length &lt;= 1) &#123; return new int[]&#123;-1, -1&#125;; &#125; boolean[] b = new boolean[arr.length + 2]; int[] rets = new int[2]; for (int i = 0; i &lt; arr.length; i++) &#123; b[arr[i] - 1] = true; &#125; int j = 0; for (int i = 0; i &lt; b.length; i++) &#123; if (!b[i]) &#123; rets[j++] = i + 1; &#125; &#125; return rets; &#125; public static void main(String[] args) &#123; int[] arr = &#123;2, 4, 3, 9, 5, 6, 1&#125;; int[] missNumber = findMissNumber(arr); for (int i = 0; i &lt; missNumber.length; i++) &#123; System.out.println(missNumber[i]); &#125; &#125;&#125;","categories":[{"name":"算法集","slug":"算法集","permalink":"https://coderzc.github.io/categories/%E7%AE%97%E6%B3%95%E9%9B%86/"}],"tags":[]},{"title":"数据结构-排序","slug":"计算机基础/20180814-数据结构-排序","date":"2018-08-13T16:00:00.000Z","updated":"2021-08-18T02:58:32.972Z","comments":true,"path":"2018/08/14/计算机基础/20180814-数据结构-排序/","link":"","permalink":"https://coderzc.github.io/2018/08/14/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/20180814-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%8E%92%E5%BA%8F/","excerpt":"排序算法总览 冒泡排序（Bubble Sort） 从前往后两两比较相邻元素的值，若为逆序则交换他们，直到（n-1+i），若某次遍历未发现有逆序情况证明已经有序则应该直接返回。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 /** * 1.冒泡排序 * 每次在0～(n-1-i)中将最大的一步一步冒泡到最后 * * @param arr */ private static void bubbleSort(int arr[]) { int n = arr.length; boolean flag = false;","text":"排序算法总览 冒泡排序（Bubble Sort） 从前往后两两比较相邻元素的值，若为逆序则交换他们，直到（n-1+i），若某次遍历未发现有逆序情况证明已经有序则应该直接返回。 12345678910111213141516171819/** * 1.冒泡排序 * 每次在0～(n-1-i)中将最大的一步一步冒泡到最后 * * @param arr */private static void bubbleSort(int arr[]) &#123; int n = arr.length; boolean flag = false;//是否已经正序 for (int i = 0; !flag; i++) &#123; flag = true; for (int j = 0; j &lt; n - 1 - i; j++) &#123; if (arr[j] &gt; arr[j + 1]) &#123; swap(arr, j, j + 1); flag = false; &#125; &#125; &#125;&#125; 空间复杂度:O(1)时间复杂度：O(n^2)稳定性：由于对于相等元素不会交换所以 =&gt; 稳定 选择排序 （Selection Sort） 每次找出i~(n-1) 最小值 与第i位交换 123456789101112131415161718/** * 2.选择排序 * 每次从i～(n-1)找出最小元素放在最前面 * * @param arr */private static void selectSort(int arr[]) &#123; int n = arr.length; int minIndex = 0; for (int i = 0; i &lt; n; i++) &#123; minIndex = i;//假定最小元素是第i个 for (int j = i + 1; j &lt; n; j++) &#123; if (arr[j] &lt; arr[minIndex]) minIndex = j; &#125; swap(arr, minIndex, i);//把i～n中最小元素和第i位交换 &#125;&#125; 空间复杂度:O(1)时间复杂度：O(n^2)稳定性：由于最小值元素会直接与i位元素互换所以可能造成相同元素次序改变 =&gt; 不稳定同样适用于链式存储结构 直接插入排序（Insertion Sort） 从第二个元素开始每次从向前 0～(i-1) 有序序列倒叙依次尝试自己合适的位置，直到比前一个元素大 123456789101112131415161718/** * 3.直接插入排序 * 从第二个元素开始每次从向前 0～(i-1) 有序序列倒叙依次尝试自己合适的位置，直到比前一个元素大 * * @param arr */private static void insertionSort(int arr[]) &#123; int n = arr.length; int temp; for (int i = 1; i &lt; n; i++) &#123; temp = arr[i];//待插入的元素 int j = i; for (; j &gt; 0 &amp;&amp; temp &lt; arr[j - 1]; j--) &#123; arr[j] = arr[j - 1]; &#125; arr[j] = temp; &#125;&#125; 空间复杂度:O(1)时间复杂度：O(n^2)稳定性：由于都是向前先比较在插入小于等于当前元素不会移动 =&gt; 稳定 希尔排序（Shell Sort） 希尔排序，又称缩小增量排序，是插入排序变形，基于插入排序适用于基本有序和数据量小的基本思想：按照增量序列函数每次按增量提取出相差h的元素组成待排序列进行直接插入排序，然后缩小增量后在排序知道增量变为1完成最后一次排序 1234567891011121314151617181920private static void shellSort2(int arr[]) &#123; int n = arr.length; int h = n / 2;//初始增序起始 while (h &gt;= 1) &#123;//获得增序序列并不断缩小h int temp; for (int i = h; i &lt; n; i++) &#123;//从第一个h位元素开始，每次和所有前面与它相差h的元素序列做插入排序 temp = arr[i];//待插入的元素 int j = i; for (; j &gt;= h &amp;&amp; temp &lt; arr[j - h]; j -= h) &#123;//这里的h是 最大值而不是准确数字但该序列第二位一定大于等于h arr[j] = arr[j - h]; &#125; arr[j] = temp; &#125; h = h / 2; &#125;&#125; 空间复杂度:O(1)时间复杂度：O(n^2)稳定性：由于每次划分子序列再排序可能改变相同元素的相对次序 =&gt; 不稳定 归并排序（Merge Sort） 基本思想：将多个有序序列组合成一个新的有序表，自底向上归并应该是先2个元素合并，然后2个长度为2的元素合并，然后2个长度为4的元素合并直到直到子序列长度&gt;=n 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394package basicAlgorithm.sort;/** * 归并排序 */public class MergeSort &#123; private static int[] aux;//辅助函数 /** * 将[l...r-1]与[r..rEnd]归并 * @param arr * @param l * @param r * @param rEnd */ private static void merge(int arr[], int l, int r, int rEnd) &#123; int temp = l; int lEnd = r - 1; //先将待排序两组子序列复制到辅助数组中 for (int i = l; i &lt;= rEnd; i++) &#123; aux[i] = arr[i]; &#125; while (l &lt;= lEnd &amp;&amp; r &lt;= rEnd) &#123; if (aux[l] &lt; aux[r]) &#123; arr[temp++] = aux[l++]; &#125; else &#123; arr[temp++] = aux[r++]; &#125; &#125; //将另一边剩下的复制到原数组 while (l &lt;= lEnd) &#123; arr[temp++] = aux[l++]; &#125; // (右边的不用复制吧,本来就在原数组) while (r&lt;=rEnd)&#123; arr[temp++]=aux[r++]; &#125; &#125; /** * 自底向上的归并排序(递归写法) * @param arr * @param L * @param R */ private static void mergeSort(int arr[], int L, int R) &#123; if (L &gt;= R) &#123; return; &#125; int mid = (L + R) / 2; mergeSort(arr, L, mid); mergeSort(arr, mid + 1, R); if (arr[mid] &gt; arr[mid + 1]) &#123;//如果左右已经有序则不归并 merge(arr, L, mid + 1, R); &#125; &#125; /** * 自底向上的归并排序(迭代写法) * @param arr */ private static void mergeSortBU(int arr[]) &#123; int n = arr.length; for (int size = 1; size &lt; n; size = 2*size) &#123; //size 表示每次将2个size大小的序列归并成一个序列 for (int i = 0; i + size &lt; n; i = i + (size * 2)) &#123; merge(arr, i, i + size, Math.min(i + (size * 2) - 1, n - 1)); &#125; &#125; &#125; public static void main(String[] args) &#123; int array[] = &#123;9, 8, 19, 6, 5, 3, 4, 2, 1&#125;; //初始化辅助数组 aux = new int[array.length];// mergeSort(array, 0, array.length - 1); mergeSortBU(array); for (int a : array) &#123; System.out.println(a); &#125; &#125;&#125; 空间复杂度:O(n) 辅助数组时间复杂度：O(nlogn)稳定性：因为依此合并没有交换操作所以 =&gt; 稳定 快速排序（Quick Sort） 号称21世纪最优化的算法来了，快排也是分治思想的应用,基本思想就是先设一个基准点然后将序列小于基准点的部分放到基准点左面小于基准点的部分放到基准点右面，然后在递归的处理左面和右面。快排有很多版本我下面代码的是最常见双路快排版 123456789101112131415161718192021222324252627282930313233//双路快排private static void quickSort2(int[] array, int l, int r) &#123; if (l &gt;= r) &#123; return; &#125; swap(array, l, ((int) (Math.random() * (r - l)) + l)); int pivote = array[l];//基点放到最左边 int i = l + 1;//左边比基点小的标志位 int j = r;//右边比基点大的标志位 while (true) &#123; while (i &lt;= r &amp;&amp; array[i] &lt; pivote) &#123; i++; &#125; while (j &gt;= l + 1 &amp;&amp; array[j] &gt; pivote) &#123; j--; &#125; if (i &gt;= j) break; else swap(array, i++, j--); &#125; swap(array, l, j); quickSort2(array, l, j - 1); quickSort2(array, j + 1, r);&#125;private static void quickSort(int[] array) &#123; quickSort2(array, 0, array.length - 1);&#125; 空间复杂度:O(logn) 由于快排要借助递归实现所以要消耗额外系统栈空间平均情况下栈深log2(n)时间复杂度:O(nlogn)稳定性：因为选取基准点的过程中可能会改变相同元素次序 =&gt; 不稳定另外序列越无序越随机快排效率越高，而现实中的序列大多是随机分布的所以快排广泛应用 堆排序（Heap Sort） 堆是一种树形结构，是一颗完全二叉树，并满足任意节点都大于他的子结点（大根堆） 12345678910111213141516171819202122232425262728private static void sink(int[] array, int n, int k) &#123;//下沉 while (2 * k + 1 &lt;= n - 1) &#123; int j = 2 * k + 1;//左孩子索引 if (j + 1 &lt;= n - 1 &amp;&amp; array[j + 1] &gt; array[j]) j++;//如果有右孩子并且右孩子比左孩子大 j 换成右孩子索引 if (array[j] &gt; array[k]) &#123; swap(array, j, k); k = j; &#125; else &#123; break; &#125; &#125;&#125;private static void heapSort(int[] array) &#123; int n = array.length; //heapify 大根堆建成 for (int i = (n - 1) / 2; i &gt;= 0; i--) &#123; sink(array, n, i); &#125; //排序 for (int j = n - 1; j &gt; 0; j--) &#123; swap(array, 0, j);//把最大的和最后以为交换 sink(array, j, 0);//缩小对的 &#125;&#125; BuildHeap 时间复杂度O(nlogn)heapify 时间复杂度O(n)稳定性：=&gt; 不稳定空间复杂度：O(1) 计数排序（Counting Sort） 计数排序的核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 桶排序（Bucket Sort） 桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。为了使桶排序更加高效，我们需要做到这两点： 在额外空间充足的情况下，尽量增大桶的数量 使用的映射函数能够将输入的N个数据均匀的分配到K个桶中 基数排序（Radix Sort） 分为 MSD(最高位优先) 和 LSD(最低位优先) 基数排序 vs 计数排序 vs 桶排序 其中, d 表示位数， k 在基数排序中表示 k 进制，在桶排序中表示桶的个数， maxV 和 minV 表示元素最大值和最小值。 首先，基数排序和计数排序都可以看作是桶排序。 计数排序本质上是一种特殊的桶排序，当桶的个数取最大( maxV-minV+1 )的时候，就变成了计数排序。 基数排序也是一种桶排序。桶排序是按值区间划分桶，基数排序是按数位来划分；基数排序可以看做是多轮桶排序，每个数位上都进行一轮桶排序。 当用最大值作为基数时，基数排序就退化成了计数排序。 当使用2进制时， k=2 最小，位数 d 最大，时间复杂度 O(nd) 会变大，空间复杂度 O(n+k) 会变小。当用最大值作为基数时， k=maxV 最大， d=1 最小，此时时间复杂度 O(nd) 变小，但是空间复杂度 O(n+k) 会急剧增大，此时基数排序退化成了计数排序。 总结","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://coderzc.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"数据结构-线性表","slug":"计算机基础/20180814-数据结构-线性表","date":"2018-08-13T16:00:00.000Z","updated":"2021-08-18T02:58:32.973Z","comments":true,"path":"2018/08/14/计算机基础/20180814-数据结构-线性表/","link":"","permalink":"https://coderzc.github.io/2018/08/14/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/20180814-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%BA%BF%E6%80%A7%E8%A1%A8/","excerpt":"线性表简介： 一种逻辑结构，相同数据类型的n个数据元素的有限序列，除第一个元素外，每个元素有且仅有一个直接前驱，除最后一个元素外，每个元素有且仅有一个直接后继。 线性表的特点： （1）元素个数有限 （2）逻辑上元素有先后次序 （3）数据类型相同 （4）仅讨论元素间的逻辑关系 注：线性表是逻辑结构，顺序表和链表是存储结构。 区别： 存储类别顺序表单链表存储分配方式用一段连续的存储单元依次存储线性表的数据元素采用链式存储结构，用一组任意的存储单元存放线性表的元素时间性能查找O（1）、插入和删除O（n）查找O（n）、插入和删除O（1）空间性能需要预分配存储空间，分大了浪费，小了容易发生上溢不","text":"线性表简介： 一种逻辑结构，相同数据类型的n个数据元素的有限序列，除第一个元素外，每个元素有且仅有一个直接前驱，除最后一个元素外，每个元素有且仅有一个直接后继。 线性表的特点：（1）元素个数有限（2）逻辑上元素有先后次序（3）数据类型相同（4）仅讨论元素间的逻辑关系 注：线性表是逻辑结构，顺序表和链表是存储结构。 区别： 存储类别 顺序表 单链表 存储分配方式 用一段连续的存储单元依次存储线性表的数据元素 采用链式存储结构，用一组任意的存储单元存放线性表的元素 时间性能 查找O（1）、插入和删除O（n） 查找O（n）、插入和删除O（1） 空间性能 需要预分配存储空间，分大了浪费，小了容易发生上溢 不需要分配存储空间，只要有就可以分配，元素个数不受限制 链表1. 单链表：只有一个next指针域 12345/*单链表定义*/typedef struct LNode&#123;ElemType data;struct LNode* next;&#125;LNode,*LinkList 结构体相关知识：https://www.cnblogs.com/qyaizs/articles/2039101.html 头插法建立单链表： 尾插法建立单链表：需要增加一个尾指针r 12345678910111213LinkList&lt;DataType&gt;::LinkList(DataType a[], int n)&#123; first = new Node&lt;DataType&gt;; Node&lt;DataType&gt; *r = first; for (int i = 0; i &lt; n; i++) &#123; Node&lt;DataType&gt; *s = new Node&lt;DataType&gt;; s-&gt;data = a[i]; r-&gt;next = s; r = s; &#125; r-&gt;next = NULL;&#125; 插入节点 删除节点 2. 双向链表：单链表节点的缺点是只有一个后继节点，访问前驱节点只能从头遍历（如插入、删除），时间复杂度为O(n)。双链表，即添加一个指向前驱的节点，节点类型如下： 1234typedef struct DNode&#123;ElemType data;struct DNdoe *prior,*next;&#125;DNode,*DLinklist; 3.循环链表3.1循环单链表和单链表区别在于，表中最后一个结点的指针不是指向NULL而是指向头结点，所以判空条件不是头节点是否为空，而是它是否等于头指针。 3.1循环双链表","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://coderzc.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"数据结构-基本概念","slug":"计算机基础/20180813-数据结构-基本概念","date":"2018-08-12T16:00:00.000Z","updated":"2021-08-18T02:58:32.972Z","comments":true,"path":"2018/08/13/计算机基础/20180813-数据结构-基本概念/","link":"","permalink":"https://coderzc.github.io/2018/08/13/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/20180813-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","excerpt":"一. 数据结构三要素 二.算法基本概念 算法 是对特定问题求解步骤的一种描述，它是指令的有序序列，其中每一条指令表示一个或多个操作。此外还具有下列5个重要特性。 1. 有穷性 2. 确定性 3. 可行性 4. 输入 5. 输出 三.算法效率的量度 算法效率的量度是通过时间复杂度和空间复杂度来描述的。 3.1 时间复制度 T(n)=O(f(n)) n表示数据规模 O(f(n)) 表示运行这个算法所需要执行的指令数，和f(n)成正比 我们平时说的时间复杂度是指平均时间复杂度，即在所以可能输入等概率情况下 3.2 空间复杂度 多开一个辅助数组：O(n) 多开","text":"一. 数据结构三要素 二.算法基本概念算法 是对特定问题求解步骤的一种描述，它是指令的有序序列，其中每一条指令表示一个或多个操作。此外还具有下列5个重要特性。 有穷性 确定性 可行性 输入 输出 三.算法效率的量度算法效率的量度是通过时间复杂度和空间复杂度来描述的。 3.1 时间复制度 T(n)=O(f(n)) n表示数据规模 O(f(n)) 表示运行这个算法所需要执行的指令数，和f(n)成正比 我们平时说的时间复杂度是指平均时间复杂度，即在所以可能输入等概率情况下 3.2 空间复杂度多开一个辅助数组：O(n)多开一个辅助二维数组：O(n^2)多开辅助常数：O(1) 递归调用是有空间代价的：递归深度如果为n 额外空间复杂度则为O(n)","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://coderzc.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"Dubbo+Zookeeper-使用-SpringBoot-(dubbo-spring-boot-starter)-快速搭建RPC分布式","slug":"微服务/20180803-Dubbo+Zookeeper-使用-SpringBoot-(dubbo-spring-boot-starter)-快速搭建RPC分布式","date":"2018-08-02T16:00:00.000Z","updated":"2021-08-18T02:58:32.968Z","comments":true,"path":"2018/08/03/微服务/20180803-Dubbo+Zookeeper-使用-SpringBoot-(dubbo-spring-boot-starter)-快速搭建RPC分布式/","link":"","permalink":"https://coderzc.github.io/2018/08/03/%E5%BE%AE%E6%9C%8D%E5%8A%A1/20180803-Dubbo+Zookeeper-%E4%BD%BF%E7%94%A8-SpringBoot-(dubbo-spring-boot-starter)-%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BARPC%E5%88%86%E5%B8%83%E5%BC%8F/","excerpt":"一.安装并启动Zookeeper 看看这篇博文吧，我就不赘述了 https://blog.csdn.net/lisongjia123/article/details/78639242 二.创建公共接口 1. 新建一个maven项目 2. 创建接口： 1 2 3 4 5 6 package com.zc.dubbo.service; public interface DemoService { String sayHello(String name); } 3. 把项目打包成jar包 三.创建服","text":"一.安装并启动Zookeeper看看这篇博文吧，我就不赘述了 https://blog.csdn.net/lisongjia123/article/details/78639242 二.创建公共接口 新建一个maven项目 创建接口： 123456package com.zc.dubbo.service;public interface DemoService &#123; String sayHello(String name);&#125; 把项目打包成jar包 三.创建服务提供者 先创建一个空的springboot项目,加入dubbo-spring-boot-starter依赖12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; 导入之前生成的公共接口jar包（不会导入自行百度） 3.实现公共接口，并添加dubbo的@service（注意不是spring的service注解，看导包）实现类： 1234567891011121314package com.zc.dubboprovider.service.impl;import com.alibaba.dubbo.config.annotation.Service;import com.zc.dubbo.service.DemoService;@Servicepublic class DemoServiceImpl implements DemoService &#123; @Override public String sayHello(String name) &#123; return &quot;Hello, &quot; + name + &quot; (from Spring Boot)&quot;; &#125;&#125; 配置文件先把properties文件修改为yml文件（习惯使用yml文件，也可不改）12345678910111213#dubbo协议dubbo: protocol: id: dubbo name: dubbo port: 12345 status: server#注册中心地址 registry: address: zookeeper://localhost:2181#dubbo应用名 application: name: dubbo-provider-demo 在启动类上开启dubbo服务并扫描服务类1234567891011121314package com.zc.dubboprovider;import com.alibaba.dubbo.config.spring.context.annotation.EnableDubbo;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplication@EnableDubbo(scanBasePackages = &quot;com.zc.dubboprovider.service.impl&quot;)public class DubboProviderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DubboProviderApplication.class, args); &#125;&#125; 启动生产者 四.创建服务消费者 同样新建springboot项目，加入dubbo-spring-boot-starter依赖，消费者要测试访问所以加入web依赖： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt;&lt;/dependency&gt; 导入公共接口的依赖 配置文件 12345678910111213server: port: 8080dubbo: protocol: id: dubbo name: dubbo registry: address: zookeeper://localhost:2181 application: name: dubbo-consumer-demo 编写controller类，并使用dubbo的 @Reference 注入公共接口，dubbo会自动去注册中心找相应服务的生产者，并通过dubbo协议调用相应方法 123456789101112131415161718package com.zc.dubboconsumer.controller;import com.alibaba.dubbo.config.annotation.Reference;import com.zc.dubbo.service.DemoService;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;@RestControllerpublic class DemoController &#123; @Reference private DemoService demoService; @RequestMapping(&quot;/sayHello&quot;) public String sayHello(@RequestParam String name) &#123; return demoService.sayHello(name); &#125;&#125; 在启动类上开启dubbo并扫描controller类（不加也可以，因为加了@RestController Spring会自动扫描相应的类,但有时不会加载 @Reference，最好写上） 12345678910111213package com.zc.dubboconsumer;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplication@EnableDubbo(scanBasePackages = &quot;com.zc.dubboconsumer.controller&quot;)public class DubboConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DubboConsumerApplication.class, args); &#125;&#125; 启动消费者，并访问 http://localhost:8080/sayHello?name=dubbo 成功调用到生产者的服务 五. 使用dubbo-admin监控服务请查看这篇博文，就是个管理界面，搭起来然后把服务注册中心地址换成自己的就行了https://www.jianshu.com/p/3d619740883c 这是最终效果，生产者和消费者都能看到了","categories":[{"name":"微服务","slug":"微服务","permalink":"https://coderzc.github.io/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}],"tags":[]},{"title":"SpringCloud-服务消费者（rest+ribbon-Feign）","slug":"微服务/20180803-SpringCloud-服务消费者（rest+ribbon-Feign）","date":"2018-08-02T16:00:00.000Z","updated":"2021-08-18T02:58:32.968Z","comments":true,"path":"2018/08/03/微服务/20180803-SpringCloud-服务消费者（rest+ribbon-Feign）/","link":"","permalink":"https://coderzc.github.io/2018/08/03/%E5%BE%AE%E6%9C%8D%E5%8A%A1/20180803-SpringCloud-%E6%9C%8D%E5%8A%A1%E6%B6%88%E8%B4%B9%E8%80%85%EF%BC%88rest+ribbon-Feign%EF%BC%89/","excerpt":"在上一篇文章，讲了服务的注册和发现。在微服务架构中，业务都会被拆分成一个独立的服务，服务与服务的通讯是基于http restful的。Spring cloud有两种服务调用方式，一种是ribbon+restTemplate，另一种是feign。 一、ribbon简介 Ribbon is a client side load balancer which gives you a lot of control over the behaviour of HTTP and TCP clients. Feign already uses Ribbon, so if you are using @Fe","text":"在上一篇文章，讲了服务的注册和发现。在微服务架构中，业务都会被拆分成一个独立的服务，服务与服务的通讯是基于http restful的。Spring cloud有两种服务调用方式，一种是ribbon+restTemplate，另一种是feign。 一、ribbon简介 Ribbon is a client side load balancer which gives you a lot of control over the behaviour of HTTP and TCP clients. Feign already uses Ribbon, so if you are using @FeignClient then this section also applies.—–摘自官网 eureka是一个客户端发现的注册中心，所以需要客户端具备负载均衡的能力，而ribbon就是一个负载均衡客户端，可以很好的控制htt和tcp的一些行为。Feign默认集成了ribbon。 ribbon 已经默认实现了这些配置bean： IClientConfig ribbonClientConfig: DefaultClientConfigImpl IRule ribbonRule: ZoneAvoidanceRule IPing ribbonPing: NoOpPing ServerList ribbonServerList: ConfigurationBasedServerList ServerListFilter ribbonServerListFilter: ZonePreferenceServerListFilter ILoadBalancer ribbonLoadBalancer: ZoneAwareLoadBalancer 二、建一个服务消费者端2.1 这一篇文章基于上一篇文章的工程，启动eureka-server 工程；启动service-hi工程，它的端口为8081；将service-hi的配置文件的端口改为8082,并启动，这时你会发现：service-hi在eureka-server注册了2个实例，这就相当于一个小的集群。 2.2 重新新建一个spring-boot工程，取名为：eureka-consumer;在它的pom.xml和上一个差不多，多引一个spring-cloud-starter-netflix-ribbon库即可 2.3同样的在启动类添加@EnableDiscoveryClient注解表明自己是个eureka客户端。 并且向程序的ioc注入一个bean: restTemplate;并通过@LoadBalanced注解表明这个restRemplate开启负载均衡的功能。 12345678910111213141516@SpringBootApplication@EnableDiscoveryClient@EnableFeignClientspublic class ServiceConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ServiceConsumerApplication.class, args); &#125; @Bean @LoadBalanced RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 配置文件如下： 123456789eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/,http://localhost:8762/eureka/,http://localhost:8763/eureka/server: port: 8083spring: application: name: service-ribbon 2.4 写一个服务类HelloService，通过之前注入ioc容器的restTemplate来消费service-hi服务的“/hi”接口，在这里我们直接用的程序名替代了具体的url地址，在ribbon中它会根据服务名来选择具体的服务实例，根据服务实例在请求的时候会用具体的url替换掉服务名，代码如下： 1234567891011@Servicepublic class HelloService &#123; @Autowired private RestTemplate restTemplate; public String hiService(String name)&#123; return restTemplate.getForObject(&quot;http://service-hi/hi?name=&quot;+name,String.class); &#125;&#125; 2.5 最后写一个controller测试，在controller中用调用HelloService 的方法，代码如下： 1234567891011@RestControllerpublic class HelloController &#123; @Autowired private HelloService helloService; @GetMapping(&quot;/hi_ribbon&quot;) public String hi_ribbon(String name)&#123; return helloService.hiService(name); &#125;&#125; 在浏览器上多次访问http://localhost: 8083/hi_ribbon?name= springcloud，浏览器交替显示： hi springcloud,i am from port:8081 hi springcloud,i am from port:8082 这说明当我们通过调用restTemplate.getForObject(“http://SERVICE-HI/hi?name=“+name,String.class)方法时，已经做了负载均衡，访问了不同的端口的服务实例。 三、此时架构 四、Feign简介Feign是一个声明式的伪Http客户端，它使得写Http客户端变得更简单。使用Feign，只需要创建一个接口并注解。它具有可插拔的注解特性，可使用Feign 注解和JAX-RS注解。Feign支持可插拔的编码器和解码器。Feign默认集成了Ribbon，并和Eureka结合，默认实现了负载均衡的效果。 简而言之： Feign 采用的是基于接口的注解 Feign 整合了ribbon，具有负载均衡的能力 整合了Hystrix，具有熔断的能力","categories":[{"name":"微服务","slug":"微服务","permalink":"https://coderzc.github.io/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}],"tags":[]},{"title":"SpringCloud-服务的注册与发现Eureka","slug":"微服务/20180803-SpringCloud-服务的注册与发现Eureka","date":"2018-08-02T16:00:00.000Z","updated":"2021-08-18T02:58:32.968Z","comments":true,"path":"2018/08/03/微服务/20180803-SpringCloud-服务的注册与发现Eureka/","link":"","permalink":"https://coderzc.github.io/2018/08/03/%E5%BE%AE%E6%9C%8D%E5%8A%A1/20180803-SpringCloud-%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0Eureka/","excerpt":"本SpringCloud系列文章参考《深入理解Spring Cloud与微服务构建》 这本书的作者的博客编写，感谢这位作者。 方志朋的博客 原文地址：https://blog.csdn.net/forezp/article/details/70148833/ 一、创建服务注册中心 1.1 首先创建一个maven主工程。 首先创建一个主Maven工程，在其pom文件引入依赖，spring Boot版本为2.0.4.RELEASE，Spring Cloud版本为Finchley.SR1。 除了SpringBoot主要引入的maven库spring-cloud-starter-netflix-eu","text":"本SpringCloud系列文章参考《深入理解Spring Cloud与微服务构建》 这本书的作者的博客编写，感谢这位作者。 方志朋的博客原文地址：https://blog.csdn.net/forezp/article/details/70148833/ 一、创建服务注册中心1.1 首先创建一个maven主工程。首先创建一个主Maven工程，在其pom文件引入依赖，spring Boot版本为2.0.4.RELEASE，Spring Cloud版本为Finchley.SR1。除了SpringBoot主要引入的maven库spring-cloud-starter-netflix-eureka-server，这是Eureka服务端。pom文件如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zc&lt;/groupId&gt; &lt;artifactId&gt;eurekaserver&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;eurekaserver&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;spring-cloud.version&gt;Finchley.SR1&lt;/spring-cloud.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 1.2 启动这个服务注册中心，只需在SpringBoot启动类添加@EnableEurekaServer注解。 1234567@SpringBootApplication@EnableEurekaServerpublic class EurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaserverApplication.class, args); &#125;&#125; 1.3 完成配置文件 eureka是一个高可用的组件，它没有后端缓存，每一个实例注册之后需要向注册中心发送心跳（因此可以在内存中完成），在默认情况下erureka server也是一个eureka client ,必须要指定一个 server。eureka server的配置文件appication.yml： 12345678910111213server: port: 8761eureka: client: service-url: defaultZone: http://localhost:8762/eureka/,http://localhost:8763/eureka/ register-with-eureka: false fetch-registry: false server: enable-self-preservation: falsespring: application: name: eureka 这里通过eureka.client. register-with-eureka：false和fetch-registry：false来表明自己是一个eureka server. 通过eureka.server .enable-self-preservation=false 关闭自我保护当服务长时间连接不到则将他从注册中心移除（生产环境不建议关闭） 2.5 eureka server 监控界面，启动工程,打开浏览器访问： http://localhost:8761 ,界面如下： 二、创建服务提供者当client向server注册时，它会提供一些元数据，例如主机和端口，URL，主页等。Eureka server 从每个client实例接收心跳消息。 如果心跳超时，则通常将该实例从注册server中删除。创建过程同server类似,创建完pom.xml如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zc&lt;/groupId&gt; &lt;artifactId&gt;eurekaclient&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;eurekaclient&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;spring-cloud.version&gt;Finchley.SR1&lt;/spring-cloud.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 客户端记得要添加spring-boot-starter-web这个库，否则跑不起来 然后也是通过@EnableDiscoveryClient注解表明自己是个eurekaclient 1234567891011121314151617181920@SpringBootApplication@EnableDiscoveryClient@RestControllerpublic class EurekaclientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaclientApplication.class, args); &#125; @Value(&quot;$&#123;server.port&#125;&quot;) String port; @RequestMapping(&quot;/hi&quot;) public String home(@RequestParam(value = &quot;name&quot;, defaultValue = &quot;forezp&quot;) String name) &#123; return &quot;hi &quot; + name + &quot; ,i am from port:&quot; + port; &#125;&#125; 最后完成配置，将自己注册到eureka服务注册中心，application.yml配置文件如下： 123456789eureka: client: service-url: defaultZone: http://localhost:8761/eureka/,http://localhost:8762/eureka/,http://localhost:8763/eureka/spring: application: name: service-hiserver: port: 8081 需要指明spring.application.name,这个很重要，这在以后的服务与服务之间相互调用一般都是根据这个name ,然后你能看到注册中心我写了三个这是因为，我开了三个eurekaserver它们之间还要相互注册启动工程，打开http://localhost:8761 ，即eureka server 的网址：你会发现一个服务已经注册在服务中了，服务名为SERVICE-HI ,端口为8081 这时打开 http://localhost:8081/hi?name=springcloud ，你会在浏览器上看到 : hi springcloud ,i am from port:8081 三、服务发现这还没很好理解，暂时引用大佬的博文https://blog.csdn.net/mr_seaturtle_/article/details/77618403","categories":[{"name":"微服务","slug":"微服务","permalink":"https://coderzc.github.io/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}],"tags":[]},{"title":"什么是微服务","slug":"微服务/20180803-什么是微服务","date":"2018-08-02T16:00:00.000Z","updated":"2021-08-18T02:58:32.969Z","comments":true,"path":"2018/08/03/微服务/20180803-什么是微服务/","link":"","permalink":"https://coderzc.github.io/2018/08/03/%E5%BE%AE%E6%9C%8D%E5%8A%A1/20180803-%E4%BB%80%E4%B9%88%E6%98%AF%E5%BE%AE%E6%9C%8D%E5%8A%A1/","excerpt":"微服务一名出自 James Lewis & Martin Fowler 2014年3月25日写《Microservices》 原文地址：https://martinfowler.com/articles/microservices.html ####微服务几个特点 * 一系列微小的服务共同组成 * 跑在自己的进程里 * 每个服务为独立的业务开发 * 独立部署 * 分布式管理 微服务是一种架构风格，没有强制性和绝度标准答案 架构演进历史： #####单体架构缺点： * 开发效率低 * 代码维护难 * 部署不灵活 * 稳定性不高 * 扩展性不够","text":"微服务一名出自 James Lewis &amp; Martin Fowler 2014年3月25日写《Microservices》 原文地址：https://martinfowler.com/articles/microservices.html ####微服务几个特点 一系列微小的服务共同组成 跑在自己的进程里 每个服务为独立的业务开发 独立部署 分布式管理 微服务是一种架构风格，没有强制性和绝度标准答案 架构演进历史： #####单体架构缺点： 开发效率低 代码维护难 部署不灵活 稳定性不高 扩展性不够 微服务必定是分布式的，而分布式系统不一定是微服务 ####微服务架构的基础框架/组件 服务注册发现中心 服务网关（Service Gateway） 后端通用服务（也称中间层服务Middle Tier Service） 前端服务（也称边缘服务Edge Service）负责服务的聚合与裁剪 ####微服务两大实现手段 Dubbo Spring Cloud 服务注册中心 Zookeeper Spring Cloud Netflix Eureka 服务调用方式 RPC REST API 服务网关 无 Spring Cloud Netflix Zuul 服务跟踪 无 Spring Cloud Sleuth 断路器 不完善 Spring Cloud Netflix Hystrix 分布式配置 无 Spring Cloud Config 消息总线 无 Spring Cloud Bus 数据流 无 Spring Cloud Stream 批量任务 无 Spring Cloud Task 最后上一张Spring Cloud 微服务架构图：","categories":[{"name":"微服务","slug":"微服务","permalink":"https://coderzc.github.io/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}],"tags":[]}],"categories":[{"name":"分布式","slug":"分布式","permalink":"https://coderzc.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"容器化","slug":"容器化","permalink":"https://coderzc.github.io/categories/%E5%AE%B9%E5%99%A8%E5%8C%96/"},{"name":"NIO与网络编程","slug":"NIO与网络编程","permalink":"https://coderzc.github.io/categories/NIO%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"计算机基础","slug":"计算机基础","permalink":"https://coderzc.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"机器学习","slug":"机器学习","permalink":"https://coderzc.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"杂记","slug":"杂记","permalink":"https://coderzc.github.io/categories/%E6%9D%82%E8%AE%B0/"},{"name":"Linux","slug":"Linux","permalink":"https://coderzc.github.io/categories/Linux/"},{"name":"Java并发编程","slug":"Java并发编程","permalink":"https://coderzc.github.io/categories/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"算法集","slug":"算法集","permalink":"https://coderzc.github.io/categories/%E7%AE%97%E6%B3%95%E9%9B%86/"},{"name":"微服务","slug":"微服务","permalink":"https://coderzc.github.io/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}],"tags":[]}