>  论文原文：*[The Google File System](https://link.zhihu.com/?target=https%3A//pdos.csail.mit.edu/6.824/papers/gfs.pdf)*

为什么设计一个分布式存储系统会如此之难？

- **出发点是提高性能**，当单机数据量太大时，需要在多台服务器上分片(Sharding)数据；
- 由于多台服务器，系统可能会出现更多的故障。如果你有数千台服务器，也许每天都有机器故障，所以我们需要系统能够自动容错；
- 为了提高容错，需要复制(replication)数据到多台服务器上，一般 2-3 个数据副本；
- 数据的复制会导致数据潜在的不一致；
- 为了提高一致性往往会导致更低的性能，**这与我们的初衷恰恰相反！**

这个循环突出了分布式系统的挑战。GFS 讨论了上述的这些主题：并行性能、容错、复制、一致性，并给出了 Google 在生产环境进行的权衡。

## 约定

- cluster集群中的很多机器**容易坏** ⇒ 需要副本(replica) （GFS的设计中用户可以指定不同的命名空间需要的replica数不同，1个文件夹名称可以理解为1个命名空间）
- 大多数都是**大文件** ⇒ blocksize提高，设置成64MB（缺点是对小文件的频繁访问会导致有的chunkserver负担重，针对这种情况GFS通过增加副本数或者减缓并发请求频率进行缓解）
- 大多数文件是**追加内容**而不是覆写 ⇒ 优化append操作，支持无锁的多client并发对一个文件进行原子追加；
- 对于写操作的一致性的要求不需要太高 ⇒ 对于写操作，不需要每次读取后都知道所有写的数据内容，只需要各client读取任何的replica后读取到的数据一致；

## 设计

### 架构

![](https://gitee.com/coderzc/blogimage/raw/master/20210823194032.jpg)

如图所示，GFS 集群包括：一个 master 和多个 chunkserver，并且若干 client 会与之交互。

主要架构特性：

- **chunk**：存储在 GFS 中的文件分为多个 chunk，chunk 大小为 64M，每个 chunk 在创建时 master 会分配一个不可变、全局唯一的 64 位标识符(`chunk handle`)；默认情况下，一个 chunk 有 3 个副本，分别在不同的 chunkserver 上；
- **master**：维护文件系统的 metadata，它知道文件被分割为哪些 chunk、以及这些 chunk 的存储位置；它还负责 chunk 的迁移、重新平衡(rebalancing)和垃圾回收；此外，master 通过心跳与 chunkserver 通信，向其传递指令，并收集状态；
- **client**：首先向 master 询问文件 metadata，然后根据 metadata 中的位置信息去对应的 chunkserver 获取数据；
- **chunkserver**：存储 chunk，**client 和 chunkserver 不会缓存 chunk 数据，防止数据出现不一致**；

### master

为了简化设计，GFS 只有一个 master 进行全局管理。

master 存储三种主要类型的元数据:

- namespace(即：目录层次结构)和文件名；
- 文件名 -> `array of chunk handles` 的映射；
- `chunk handles` -> 版本号、list of chunkservers、primary、租约

所有元数据都保存在主机的内存中。前两种类型(名称—步数和文件到块的映射)也通过将更改记录到存储在master的本地磁盘并在远程机器上复制的操作日志来保持持久。使用日志允许我们简单、可靠地更新主状态，并且在主系统崩溃的情况下不会有不一致的风险。

### 读文件

![img](https://gitee.com/coderzc/blogimage/raw/master/20210824213452.jpg)

- client 将 文件名+offset 转为文件名+ `chunk index`，向 master 发起请求；
- master 在内存中的 metadata 查询对应 chunk 所在的 `chunk handle` + `chunk locations` 并返回给 client；
- client 将 master 返回给它的信息缓存起来，用文件名 + `chunk index` 作为 key；(**注意：client 只缓存 metadata，不缓存 chunk 数据**)
- client 会选择网络上最近的 chunkserver 通信(Google 的数据中心中，IP 地址是连续的，所以可以从 IP 地址差异判断网络位置的远近)，并通过 `chunk handle` + `chunk locations` 来读取数据；

> 学生提问：如果读取的数据超过了一个 chunk 怎么办？
> Robert教授：我不知道详细的细节。我的印象是，如果应用程序想要读取超过 64MB 的数据，或者就是 2 个字节，但是却跨越了 chunk 的边界，应用程序会通过一个库来向 GFS 发送 RPC，而这个库会注意到这次读请求会跨越 chunk 边界，因此会将一个读请求拆分成两个读请求再发送到 master 节点。所以，这里可能是向 master 节点发送两次读请求，得到了两个结果，之后再向两个不同的 chunk 服务器读取数据。

### 租约

如果每次写文件都请求 master，那么 master 则会成为性能瓶颈，master 找到拥有该 chunk 的 chunkserver，并给其中一个 chunkserver 授予**租约**，拥有租约的 chunkserver 称为 `Primary`，其他叫做 `Secondary`，之后：

- master 会增加版本号，并将版本号写入磁盘，然后 master 会向 `Primary` 和`Secondary` 副本对应的服务器发送消息并告诉它们，谁是 `Primary`，谁是 `Secondary`，最新的版本号是什么；
- 在租约有效期内，对该 chunk 的写操作都由 `Primary` 负责；
- 租约的有效期一般为 60 秒，租约到期后 master 可以自由地授予租约；
- master 可能会在租约到期前撤销租约(例如：重命名文件时)；
- 在写 chunk 时，`Primary` 也可以请求延长租约有效期，直至整个写完 chunk；

#### 写文件

![img](https://gitee.com/coderzc/blogimage/raw/master/20210825112936.jpg)

如图，写文件可分为 7 步：

1. client 向 master 询问 `Primary` 和 `Secondary`。如果没有 chunkserver 持有租约，master 选择一个授予租约；
2. master 返回 `Primary` 和 `Secondary` 的信息，client 缓存这些信息，只有当 `Primary` 不可达或者**租约过期**才再次联系 master；
3. client 将追加的记录发送到**每一个 chunkserver(不仅仅是****`Primary`)**，chunkserver 先将数据写到 LRU 缓存中(不是硬盘！)；
4. 一旦 client 确认每个 chunkserver 都收到数据，client 向 `Primary` 发送写请求，`Primary` 可能会收到多个连续的写请求，会先将这些操作的顺序写入本地；
5. `Primary` 做完写请求后，将写请求和顺序转发给所有的 `Secondary`，让他们以同样的顺序写数据；
6. `Secondary` 完成后应答 `Primary`；
7. `Primary` 应答 client 成功或失败。如果出现失败，client 会重试，但在重试整个写之前，会先重复步骤 3-7；

